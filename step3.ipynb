{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64b0416-4648-429c-b4db-599e242a0ebc",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "\n",
    "Converting sequences of words into their phoneme counterparts, and also including some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7878ab50-8ccc-4cee-9944-4298d6586294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e2c607-de48-4085-9cd1-f1f96253384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = os.getcwd() + \"/data/gutenberg_np_60k.txt\"\n",
    "version = \"s3.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041c1ccb-d8ee-4800-9cfe-44be39f94471",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1 - SentencePiece embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50ffa9f1-c9d5-442b-8b74-8b88cc2231e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0075437f-f1ec-427e-a4d5-542263e1d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 9000\n",
    "vocab_size = 19099\n",
    "model_type = \"unigram\"\n",
    "SP_MODEL_NAME = f\"models/{model_type}_vs{vocab_size}_{version}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36140c1-c17f-40d6-83d8-7149f4afd432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`\n",
    "# `m.vocab` is just a reference. not used in the segmentation.\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={input_file} \" \\\n",
    "    f\"--model_type={model_type} \" \\\n",
    "    f\"--model_prefix={SP_MODEL_NAME} \" \\\n",
    "    f\"--vocab_size={vocab_size} \" \\\n",
    "    f\"--max_sentence_length={max_sentence_length} \" \\\n",
    "    f\"--train_extremely_large_corpus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d682060b-b195-4adb-867b-7ee826493882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'ap', 'ple']\n",
      "[222, 1668, 473, 1816, 103, 238]\n",
      "[222, 1668, 473]\n",
      "[222, 8585]\n"
     ]
    }
   ],
   "source": [
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f\"{SP_MODEL_NAME}.model\")\n",
    "\n",
    "# encode: text => id\n",
    "print(sp.EncodeAsPieces('apple'))\n",
    "print(sp.encode_as_ids('boyhood'))\n",
    "print(sp.encode_as_ids('boy'))\n",
    "print(sp.encode_as_ids('man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bebfd3-a719-4ca1-ba3b-584fe0a22537",
   "metadata": {},
   "source": [
    "## Step 2 - Word2Vec encodings\n",
    "\n",
    "We use the SentencePiece encoder on our phoneme dataset to create a new dataset with it's tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24b51dd4-b188-4482-b7f4-319e277804cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Replace with entire dataset, if possible\n",
    "with open(input_file) as corpus_file:\n",
    "    corpus = corpus_file.readlines()\n",
    "\n",
    "sentences = [sp.EncodeAsPieces(sentence) for sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e2cde-5671-434b-addd-c48713965d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_from = 920\n",
    "\n",
    "sentences[0][select_from:select_from + 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba10b7d-db05-43ac-bcd8-e58a6302204a",
   "metadata": {},
   "source": [
    "Now, training Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c4867f4-8e11-42a5-acaf-132f62b71329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15ebd89c-497c-4da9-a9d2-72e4a9b4cd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5\n",
    "vector_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c348c1f-d001-4600-a5b3-38ca25877e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_MODEL_PATH = f\"models/w2v_vs{vector_size}_w{window}_{version}.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52b092da-95af-4c7f-b9de-7aee601e93a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, size=vector_size, window=window, min_count=0, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a38bcb22-2192-4ca3-b00c-9b8f18697a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(W2V_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0acfbed-7fbe-405f-a13d-895b4a51546d",
   "metadata": {},
   "source": [
    "## Step 3 - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d6f5e1c-eecf-43d3-8ded-427ff7255102",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(W2V_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b853104-b2d8-4f41-89c2-33d2612cce73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PʌL', 0.978600025177002),\n",
       " ('Pʌl', 0.9757851362228394),\n",
       " ('Pɐl', 0.9756471514701843),\n",
       " ('pɐL', 0.9684648513793945),\n",
       " ('pʌl', 0.967623233795166),\n",
       " ('pɐl', 0.9671772122383118),\n",
       " ('pʌL', 0.953802764415741),\n",
       " ('pɐ', 0.4324934482574463),\n",
       " ('Pɐ', 0.42574286460876465),\n",
       " ('Dɨf', 0.4195268154144287)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"PɐL\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8774b90-c60a-4135-9637-5271a0510f86",
   "metadata": {},
   "source": [
    "## Step 4 - Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca340b5a-f501-4dd4-9816-5772104c6943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "119fa8b3-8990-4387-9a0c-b8caee74b18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19078/19078 [00:02<00:00, 6556.95it/s] \n"
     ]
    }
   ],
   "source": [
    "words = list(model.wv.vocab.keys())\n",
    "\n",
    "word_to_cluster = dict()  # Stores map from word to cluster\n",
    "cluster_to_words = dict()  # Stores map from cluster to words\n",
    "cluster_idx = 0  # Counter\n",
    "\n",
    "for word in tqdm(words):\n",
    "    # Check if word has already been clustered\n",
    "    if word not in word_to_cluster.keys():\n",
    "        # Create new cluster\n",
    "        cluster_idx += 1\n",
    "        cluster_key = chr(0x0020 + cluster_idx)\n",
    "\n",
    "        # Add new word to cluster\n",
    "        cluster_to_words[cluster_key] = [word]\n",
    "        word_to_cluster[word] = cluster_key\n",
    "        \n",
    "        # Add all similar words\n",
    "        for similar_word, score in model.wv.most_similar(word, topn=200):\n",
    "            if score > 0.88:\n",
    "                cluster_to_words[cluster_key].append(similar_word)\n",
    "                word_to_cluster[similar_word] = cluster_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3031142b-ad32-4cf6-be48-8f1141d691b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2539"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cluster_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d76e543d-7bd6-4193-9c76-978c6360db4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ਂ', 'ਃ', '\\u0a04', 'ਅ', 'ਆ', 'ਇ', 'ਈ', 'ਉ', 'ਊ', '\\u0a0b']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cluster_to_words.keys())[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f778c293-6bb2-4819-a8dd-ccb3a05a83a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs/s3.1_clusters.txt\", \"w+\", encoding=\"utf-8\") as cluster_fp:\n",
    "    for cluster in cluster_to_words.keys():\n",
    "        cluster_fp.write(cluster +\"\\t\" +\"\\t\".join(sorted(cluster_to_words[cluster])) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79c4a893-dc90-46f2-a876-07ad6440d798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['post',\n",
       " 'pOst',\n",
       " 'poSt',\n",
       " 'POSt',\n",
       " 'POST',\n",
       " 'PosT',\n",
       " 'Post',\n",
       " 'POsT',\n",
       " 'PoST',\n",
       " 'posT',\n",
       " 'PoSt',\n",
       " 'poST',\n",
       " 'pOsT',\n",
       " 'pOSt',\n",
       " 'POst',\n",
       " 'pOST']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_to_words[list(cluster_to_words.keys())[201]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f223240-0f87-4d00-b620-6155396fb631",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_output_filepath = \"data/gutenberg_np_clustered.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1512cc00-b233-4ff9-80cc-fb3f3e0e7e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(clustered_output_filepath, \"w+\", encoding=\"utf-8\") as clustered_output_file:\n",
    "    for sentence in sentences:\n",
    "        clustered_output_file.write(\"\".join([word_to_cluster[word] for word in sentence]) +\"\\n\")\n",
    "\n",
    "# clustered_sentences = [\n",
    "#     [word_to_cluster[word] for word in sentence]\n",
    "#     for sentence in sentences\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc472e-f438-4490-8f14-c8af4f061211",
   "metadata": {},
   "source": [
    "## Step 5 - Splitting clustered data into units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d691c9b3-8c57-468e-9f12-53f6421508c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_input_file = clustered_output_filepath\n",
    "clustered_version = \"s3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62d717ca-5d0a-4743-a35e-a0341370f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 8000\n",
    "vocab_size = 19099\n",
    "model_type = \"unigram\"\n",
    "SP_MODEL_NAME_CLUSTERS = f\"models/{model_type}_vs{vocab_size}_{clustered_version}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6fc743-f5f7-4602-a42e-8f941a968675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`\n",
    "# `m.vocab` is just a reference. not used in the segmentation.\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={cluster_input_file} \" \\\n",
    "    f\"--model_type={model_type} \" \\\n",
    "    f\"--model_prefix={SP_MODEL_NAME_CLUSTERS} \" \\\n",
    "    f\"--vocab_size={vocab_size} \" \\\n",
    "    f\"--max_sentence_length={max_sentence_length} \" \\\n",
    "    f\"--train_extremely_large_corpus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8485bcfa-4200-4394-ac39-cbb974271088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# makes segmenter instance and loads the model file (m.model)\n",
    "spc = spm.SentencePieceProcessor()\n",
    "spc.load(f\"{SP_MODEL_NAME_CLUSTERS}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a519521f-8c07-46de-8d67-915e3a2d5c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cluster_input_file) as corpus_file:\n",
    "    corpus = corpus_file.readlines()\n",
    "\n",
    "clustered_sentences = [spc.EncodeAsPieces(sentence) for sentence in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08abeac7-cb60-44e0-9afe-89e83468919f",
   "metadata": {},
   "source": [
    "## Step 6 - Word2Vec from clustered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cdca100-f282-4c67-908b-9ad6f9150e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_c = 5\n",
    "vector_size_c = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe7bf6e9-f960-4320-a3d8-1a9ba88c39dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_MODEL_PATH_CLUSTERS = f\"models/w2v_vs{vector_size_c}_w{window_c}_{clustered_version}.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8968b0e7-40ca-494e-97fb-182af1762e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c = Word2Vec(clustered_sentences, size=vector_size_c, window=window_c, min_count=0, workers=4, iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8597f6de-31fe-4c6a-aae4-6108b6362a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c.save(W2V_MODEL_PATH_CLUSTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cda0884-df1c-410d-90f4-13aac482169e",
   "metadata": {},
   "source": [
    "## Step 7 - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed9356ff-6e5f-44b8-ac4f-f22597e48d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c = Word2Vec.load(W2V_MODEL_PATH_CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95d7dcd2-7290-428a-b766-a6ba9433272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_clusters(test_word, topn=5):\n",
    "    most_similar = list()\n",
    "    for cluster, score in model_c.wv.most_similar(word_to_cluster[test_word], topn=topn):\n",
    "        most_similar.append(([cluster_to_words[char][:3] for char in cluster], score))\n",
    "    \n",
    "    return most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc1f1ccc-6211-49a6-8979-db151acfaff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([['hänD', 'hÄND', 'HÄnɖ'], ['S', 's'], ['Änɖ', 'äNɖ', 'ÄNɖ']],\n",
       "  0.8297650218009949),\n",
       " ([['śOlD', 'ʃoLD', 'śoLD'], ['ɜz', 'ɝz', 'ɝZ']], 0.8046799302101135),\n",
       " ([['RÆT', 'ʁÆt', 'RæT'], ['āRm', 'ARm', 'ārm']], 0.7949904799461365),\n",
       " ([['LɨPs', 'LipS', 'lɨPS']], 0.7779038548469543),\n",
       " ([['hänD', 'hÄND', 'HÄnɖ']], 0.7738319039344788),\n",
       " ([['hänD', 'hÄND', 'HÄnɖ'], ['S', 's']], 0.7736732959747314),\n",
       " ([['Mąd', 'mĄð', 'Mąð']], 0.7693848609924316),\n",
       " ([['āRm', 'ARm', 'ārm'], ['Zʌnɖ', 'zɐnD', 'zɐND']], 0.7676824331283569),\n",
       " ([['pɨ', 'Pɨ', 'pi'], ['LO', 'Lo', 'lo']], 0.7655820846557617),\n",
       " ([['RÆT', 'ʁÆt', 'RæT'], ['hänD', 'hÄND', 'HÄnɖ']], 0.7582882642745972),\n",
       " ([['śOlD', 'ʃoLD', 'śoLD'], ['ɜz', 'ɝz', 'ɝZ'], ['ʌNɖ', 'ɐND', 'ɐNɖ']],\n",
       "  0.7423087954521179),\n",
       " ([['hänD', 'hÄND', 'HÄnɖ'], ['ʌNɖ', 'ɐND', 'ɐNɖ']], 0.731292724609375),\n",
       " ([['śOlD', 'ʃoLD', 'śoLD'], ['ɝ', 'ɜ', 'ɚ']], 0.7303489446640015),\n",
       " ([['tı', 'Tı', 'tI'], ['ʈ']], 0.7164862155914307),\n",
       " ([['čI', 'Čı', 'ĆI'], ['kS', 'KS', 'Ks']], 0.7162569165229797)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word = \"HÄNDZ\"\n",
    "# sp.EncodeAsPieces(test_word)\n",
    "\n",
    "find_most_similar_clusters(test_word, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0b66a09-c8c8-43cf-b893-7ecdda75fe93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "к\n"
     ]
    }
   ],
   "source": [
    "test_cluster = word_to_cluster[test_word]\n",
    "print(test_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d84eb994-c885-4a35-98f3-3c548463cd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ј', 0.6868019104003906), ('ÈT', 0.569015622138977), ('Rɒ', 0.5664949417114258), ('Ԫ', 0.5378398895263672), ('ɢt', 0.5141922831535339), ('ɕ', 0.5025638341903687), ('/±', 0.49946022033691406), ('iɣ', 0.49369749426841736), ('7', 0.48990678787231445), ('ȧA', 0.46953094005584717)]\n"
     ]
    }
   ],
   "source": [
    "most_similar = model_c.wv.most_similar(test_cluster)\n",
    "print(most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c955c8c-5943-4d6c-8ff0-9f90aae644e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ńg', 'ńɠ', 'ŃG', 'Ńɠ', 'Ńg', 'ńG']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_to_words[most_similar[0][0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b96d06-7b92-4326-a6c9-9f9b47a53a14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
