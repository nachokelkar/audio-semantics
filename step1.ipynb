{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Thesis - Step 1\n",
        "\n",
        "Using pure text data to get best similarity scores."
      ],
      "metadata": {},
      "id": "2c355c92-8b8c-4161-b033-3becc7bdd33a"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1680194273976
        }
      },
      "id": "57f10954-bf97-4afc-afc5-ec3ab2f55994"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 1 - SentencePiece\n",
        "\n",
        "This approach uses SentencePiece on text data with only the letters to try and find words."
      ],
      "metadata": {},
      "id": "ad2c9adf-f4ec-4b4a-873b-2c226205bd77"
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import sentencepiece as spm"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1680194274974
        }
      },
      "id": "e7d6f33c-b3ad-4f9e-8a71-d131915b1b64"
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = os.getcwd() + \"/data/gutenberg_no_spaces.txt\"\n",
        "max_sentence_length = 500000\n",
        "vocab_size = 8000\n",
        "model_type = \"bpe\"\n",
        "SP_MODEL_NAME = f\"./models/{model_type}_{vocab_size}_v2\""
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1680031548880
        }
      },
      "id": "3ec298bb-7639-4c4d-983b-75ed45c38643"
    },
    {
      "cell_type": "code",
      "source": [
        "# train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`\n",
        "# `m.vocab` is just a reference. not used in the segmentation.\n",
        "spm.SentencePieceTrainer.train(\n",
        "    f\"--input={input_file} \" \\\n",
        "    f\"--model_type={model_type} \" \\\n",
        "    f\"--model_prefix={SP_MODEL_NAME} \" \\\n",
        "    f\"--vocab_size={vocab_size} \" \\\n",
        "    f\"--max_sentence_length={max_sentence_length} \" \\\n",
        "    f\"--train_extremely_large_corpus\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1680031587088
        }
      },
      "id": "dd64cda8-e4db-4816-934a-3bd87dcf0f5e"
    },
    {
      "cell_type": "code",
      "source": [
        "SP_MODEL_NAME = \"unigram_8k_v2\""
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680194771086
        }
      },
      "id": "9d5458ab-230c-4494-b81d-1a03f0e974e1"
    },
    {
      "cell_type": "code",
      "source": [
        "# makes segmenter instance and loads the model file (m.model)\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(f\"models/{SP_MODEL_NAME}.model\")\n",
        "\n",
        "# encode: text => id\n",
        "print(sp.EncodeAsPieces('apple'))\n",
        "print(sp.encode_as_ids('boyhood'))\n",
        "print(sp.encode_as_ids('boy'))\n",
        "print(sp.encode_as_ids('man'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['‚ñÅ', 'apple']\n[0, 6647]\n[0, 201, 24]\n[0, 58]\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1680194772113
        }
      },
      "id": "0c205055-8cb3-4391-b73d-acfaa9c1ef53"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the vocabulary created by SentencePiece"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "e72c9453-50a4-4e22-8ebd-7020fefe4609"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}\n",
        "with open(f\"models/{SP_MODEL_NAME}.vocab\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        word, freq = line.strip().split('\\t')\n",
        "        vocab[word] = np.exp(float(freq))\n"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1680194775066
        }
      },
      "id": "9979009c-5718-4dfd-9e9b-2ccbc3bc00db"
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data/gutenberg_no_spaces.txt\") as corpus_file:\n",
        "    corpus = corpus_file.readlines()\n",
        "\n",
        "sentences = [[' '.join(sp.EncodeAsPieces(sentence)) for sentence in corpus]]\n",
        "# sentences = [[' '.join(sentence) for sentence in corpus]]\n",
        "# sentences = [list(sentence) for sentence in corpus]"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680194951102
        }
      },
      "id": "10091f5e-499d-4301-a19f-3ba88bf86e86"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Approach 1.1 - Using FastText"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "5ef3c7f6-c1cc-4467-afa9-bb3ffc922005"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "d80c9150-90f6-47b4-90c8-81ef2a22fd92"
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastText(size=300, window=3, min_count=0)\n",
        "model.build_vocab_from_freq(vocab)\n",
        "model.train(sentences, total_examples=len(sentences), epochs=20)"
      ],
      "outputs": [],
      "execution_count": 39,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680195221234
        }
      },
      "id": "6b780228-35b0-40f9-b8e7-3abc5260839f"
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"models/fasttext_300_u8kv2\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[b'_\\xdf\\xbc\\x91\\xcfMT`\\xf2#\\xa8n[\\xa4\\xbbEc|\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00', b'\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00']\nBad pipe message: %s [b'\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n']\nBad pipe message: %s [b\"Y\\x98&\\x92\\xe8m\\xfa'\\x96\\x81@\\x88|\\xe1\\xd0\\xda<\\xd4\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\r\\x00 \\x00\\x1e\\x06\\x01\\x06\\x02\\x06\\x03\\x05\\x01\\x05\\x02\\x05\\x03\\x04\\x01\\x04\\x02\\x04\\x03\\x03\\x01\\x03\\x02\\x03\\x03\\x02\"]\nBad pipe message: %s [b'']\nBad pipe message: %s [b'\\x03']\nBad pipe message: %s [b\"\\xdb\\xadJ[\\x98\\x1c\\xa9\\x88\\xc1\\xd2\\xa6\\xc1\\xd98Sg\\xbf\\xa8\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\"]\nBad pipe message: %s [b'\\x06\\x00\\x17\\x00\\x03\\xc0\\x10']\nBad pipe message: %s [b\"\\xec\\xeav\\x84U\\xec\\xf5\\x87\\xa0\\xf3\\xc3\\xa3k\\x8dB\\xcaor \\x1a\\x16\\x98\\xbcg\\xb0\\xa2mB&'8\\xb1\\xb3\"]\nBad pipe message: %s [b'\\x18\\xce%\\x1bV}\\xa9\\x13\\x07+\\xf9\\xbd\\x8e\\x8dD\\xaf\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\xf1\\xc7\\xeb\\xd3\\xa1\\xdaz']\nBad pipe message: %s [b\"\\x1d)\\xbe\\xd6\\xf5\\x0e\\xb5\\x87IH\\xf9\\x97\\xe7\\xa5qE\\x9b\\xac '\\xaePc\\xc4\\xcf8\\xf7\\xf6\\xdc\\x9ed\\x15r\\x87k\\x01\\xb4\\xa9\\x8b\\xb0\\xab\\x18\\x17\\x1e\\xe2\\xb4\\x9e\\x9cH\\r\\x8d\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\", b'\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08']\nBad pipe message: %s [b'\\t\\x08\\n\\x08\\x0b\\x08\\x04']\nBad pipe message: %s [b'\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'']\nBad pipe message: %s [b'\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 S\\x15\\xbd\\x0f~\\xae\\xbcnm\\x19cm\\xbbP%\\xd6E\\x8e\\x82\\x06\\xed\\xc3']\nBad pipe message: %s [b\"$A\\x0e\\xf1\\xaf\\x1e\\x97\\xd5\\x16<\\xb3\\x13\\xd8\\x1eJ\\xc2^a\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\", b'\\x03\\x06', b'\\x07\\x08']\nBad pipe message: %s [b'\\t\\x08\\n\\x08\\x0b\\x08\\x04']\nBad pipe message: %s [b'\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'', b'\\x03\\x03']\nBad pipe message: %s [b'']\nBad pipe message: %s [b'', b'\\x02']\nBad pipe message: %s [b'\\x05\\x02\\x06']\nBad pipe message: %s [b'~\\xeaB\\x86\\xc6J\\x83\\x93\\x1d\\xfe\\xdc\\xf3C\\xa0\\xfd\\xa1\\xb1?\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0']\nBad pipe message: %s [b'\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01']\nBad pipe message: %s [b'_i6wn\\xef\\xd9{\\xd6\\xfe\\x86z\\x90\\xbd\\x906\\rr\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x00', b\"\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\"]\nBad pipe message: %s [b'{A\\x95UV\\x14\\xc6\\xd9\\x9a\\xf8\\xab\\x83\\xf4z\\xe2\\xae\\xe2\\xa0 \\xec\\xb1=C\\xf2\\xf8R\\x07\\x82\\x9e\\xc4\\xef\\x96\\xd5\\xe5\\xb8/\\xb1\\x98\\x0c\\x00\\xd4\"\\x93\\xca.\\x83C\\xd4:\\x99\\x96\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0', b'.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04']\nBad pipe message: %s [b'\\x03\\x06', b'\\x07\\x08']\nBad pipe message: %s [b'\\t\\x08\\n\\x08\\x0b\\x08\\x04']\nBad pipe message: %s [b'\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'']\nBad pipe message: %s [b'\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 _\\x99\\xa0\\xf9~t0\\xb2c\\xb4C\\x17g\\xd5\\x10\\xde\\xe7\\xa7\\xdb?\\x04\\x06']\nBad pipe message: %s [b'-\\xe2\\xcc\\xb5\\xec\\x8b!\\xe0\\xc7B\\xe7\\x85\\x93iX\\xd9Un \\xd89\\x8f\\xdb\\x97\\xef\\xe7k\\xf4\\x8fF\\x1f\\xe6c\\xde\\xfawE\\x93g']\nBad pipe message: %s [b'\\xbf\\xb1\\xa2\\x0e\\x19j\\xe1\\xf3\\xa3\\xd9\\xff\\x89\\x85\\xc6w\\x87\\x94I\\x00\\x00|\\xc0,\\xc00\\x00']\nBad pipe message: %s [b\"\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\"]\nBad pipe message: %s [b'#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03']\nBad pipe message: %s [b'\\x08\\x08\\x08\\t\\x08\\n\\x08', b'\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06']\nBad pipe message: %s [b'', b'\\x03\\x03']\nBad pipe message: %s [b'']\nBad pipe message: %s [b'', b'\\x02']\nBad pipe message: %s "
        }
      ],
      "execution_count": 46,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680195403033
        }
      },
      "id": "9c153352-9f44-4dfd-85e4-9e5eb4b433d7"
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.wv.similarity(\"banana\", \"fruit\"))\n",
        "print(model.wv.similarity(\"banana\", \"apple\"))\n",
        "print(model.wv.similarity(\"banana\", \"man\"))\n",
        "print(model.wv.similarity(\"human\", \"man\"))\n",
        "print(model.wv.similarity(\"human\", \"banana\"))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "-0.0001073372\n0.08859801\n-0.053881098\n0.24784493\n0.04947821\n"
        }
      ],
      "execution_count": 48,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680195594057
        }
      },
      "id": "ee528ea8-f8a7-4eaf-a9f6-73f62a77cfd9"
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(\"science\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 50,
          "data": {
            "text/plain": "[('conscience', 0.6583830118179321),\n ('patience', 0.47274458408355713),\n ('thescienceof', 0.4711171090602875),\n ('audience', 0.47084999084472656),\n ('obedience', 0.45668476819992065),\n ('experience', 0.4282777011394501),\n ('impatience', 0.41176164150238037),\n ('scientist', 0.4040490388870239),\n ('convenience', 0.35665076971054077),\n ('lence', 0.3357565999031067)]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 50,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680195612104
        }
      },
      "id": "8068fbb5-d0f8-411e-b4ec-4ac08bcdf534"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Approach 1.2 - Word2Vec"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "9ab30628-7d4a-48ec-9e38-5027e7a06a59"
    },
    {
      "cell_type": "code",
      "source": [
        "W2V_MODEL_PATH = \"models/w2v_100_v1.model\""
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680004547121
        }
      },
      "id": "de00c3bf-0be8-4256-9ada-b25b580268e5"
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences, window=5, min_count=0, workers=4)\n",
        "# model.build_vocab()"
      ],
      "outputs": [],
      "execution_count": 50,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680005530991
        }
      },
      "id": "bdaf5159-36c3-4638-bb3e-8deb8d8ea6d4"
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(W2V_MODEL_PATH)"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680004691811
        }
      },
      "id": "cba718e9-7e56-4962-8f55-2896b45c274a"
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Word2Vec.load(W2V_MODEL_PATH)"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680004328602
        }
      },
      "id": "d0f2a252-1d31-477d-8f7d-85e9e582dce5"
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(\"cat\")"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"word 'cat' not in vocabulary\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/gensim/models/keyedvectors.py:553\u001b[0m, in \u001b[0;36mWordEmbeddingsKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m     mean\u001b[38;5;241m.\u001b[39mappend(weight \u001b[38;5;241m*\u001b[39m word)\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 553\u001b[0m     mean\u001b[38;5;241m.\u001b[39mappend(weight \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab:\n\u001b[1;32m    555\u001b[0m         all_words\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[word]\u001b[38;5;241m.\u001b[39mindex)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/gensim/models/keyedvectors.py:468\u001b[0m, in \u001b[0;36mWordEmbeddingsKeyedVectors.word_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m word)\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'cat' not in vocabulary\""
          ]
        }
      ],
      "execution_count": 51,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680012211655
        }
      },
      "id": "e3817dab-eb26-49ed-a3e8-75ff44975f0a"
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"vocab.txt\", \"w+\") as fp:\r\n",
        "    fp.write(str(model.wv.vocab.keys()))"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680003791787
        }
      },
      "id": "e6de1388-6357-44fc-aae0-62695567bb70"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 2 - WordPiece tokenizer"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "30ad6866-adfc-4bef-a920-43f40bf70441"
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.pre_tokenizers import PreTokenizer\r\n",
        "\r\n",
        "class FixedLengthPreTokenizer(PreTokenizer):\r\n",
        "    def __init__(self, n=3):\r\n",
        "        self.n = n\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "    def pre_tokenize(self, text):\r\n",
        "        return [(i, i+self.n) for i in range(0, len(text), self.n)]\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "cc7b81a1-8d47-41ae-a07b-eab52ff02f6b"
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import trainers\r\n",
        "from tokenizers.models import WordPiece\r\n",
        "from tokenizers import Tokenizer\r\n",
        "\r\n",
        "tokenizer = Tokenizer(WordPiece())\r\n",
        "\r\n",
        "trainer = trainers.WordPieceTrainer(\r\n",
        "    vocab_size=10000, \r\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\r\n",
        "    initial_alphabet= list(\r\n",
        "        \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\r\n",
        "    ),\r\n",
        "    continuing_subword_prefix=\"##\",\r\n",
        "    show_progress=True,\r\n",
        "    min_frequency=1\r\n",
        ")\r\n",
        "\r\n",
        "# tokenizer.pre_tokenizer = FixedLengthPreTokenizer()  # or another value for n\r\n",
        "\r\n",
        "tokenizer.train(\r\n",
        "    files=[\"data/gutenberg_no_spaces.txt\"], \r\n",
        "    trainer=trainer\r\n",
        ")\r\n",
        "\r\n",
        "tokenizer.save(\"models/tokenizer.json\")\r\n"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679686719830
        }
      },
      "id": "696fe546-61db-4ddd-96d8-bb63c9954d5f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 3 - Character CNN"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "b206bacb-ebb0-46d5-a865-b1e4d8ebc799"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "import sys\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680029630371
        }
      },
      "id": "29babca4-750a-41b2-b69e-38535364ebc8"
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\""
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680029630479
        }
      },
      "id": "b89c4832-4fc5-486d-aa02-c75f2acd20d0"
    },
    {
      "cell_type": "code",
      "source": [
        "# To prevent recomputing alphabet each time\r\n",
        "vocab_path = 'data/gutenberg_vocabulary.txt'\r\n",
        "\r\n",
        "vocabulary = sorted(set(open(vocab_path).read().split()))\r\n",
        "print(vocabulary)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '√†', '√°', '√¢', '√£', '√§', '√¶', '√ß', '√®', '√©', '√™', '√´', '√¨', '√≠', '√Æ', '√Ø', '√±', '√≤', '√≥', '√¥', '√∂', '√π', '√ª', '√º', 'ƒÅ', '≈ì', 'Œ±', 'Œ≤', 'Œ¥', 'Œµ', 'Œ∑', 'Œ∏', 'Œπ', 'Œ∫', 'Œª', 'Œº', 'ŒΩ', 'Œø', 'œÄ', 'œÅ', 'œÇ', 'œÉ', 'œÑ', 'œÖ', 'œÜ', 'œâ']\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680030688818
        }
      },
      "id": "3810f35b-5b5a-4a13-9549-4dbdb4b6111b"
    },
    {
      "cell_type": "code",
      "source": [
        "class CharDataset(Dataset):\r\n",
        "    def __init__(self, corpus_path, seq_length):\r\n",
        "        self.corpus_path = corpus_path\r\n",
        "        self.seq_length = seq_length\r\n",
        "        self.vocab = vocabulary # Load the unique characters in the corpus\r\n",
        "        self.char_to_index = {c: i for i, c in enumerate(self.vocab)} # Map each character to an index\r\n",
        "        self.index_to_char = {i: c for i, c in enumerate(self.vocab)} # Map each index to a character\r\n",
        "        self.corpus_size = os.path.getsize(corpus_path)\r\n",
        "        # self.num_chunks = int(self.corpus_size / (1024 * 1024)) # Split the corpus into 1MB chunks\r\n",
        "        # self.chunk_size = int(self.corpus_size / self.num_chunks)\r\n",
        "    \r\n",
        "    def __len__(self):\r\n",
        "        return self.num_chunks\r\n",
        "    \r\n",
        "    def __getitem__(self, idx):\r\n",
        "        start_pos = idx * self.chunk_size\r\n",
        "        # end_pos = (idx + 1) * self.chunk_size\r\n",
        "        with open(self.corpus_path) as f:\r\n",
        "            f.seek(start_pos)\r\n",
        "            chunk = f.read(self.chunk_size).replace('\\n', '')\r\n",
        "        input_seq = chunk[:-1]\r\n",
        "        target = chunk[1:]\r\n",
        "        input_seq = [self.char_to_index[c] for c in input_seq]\r\n",
        "        target = [self.char_to_index[c] for c in target]\r\n",
        "        input_seq = torch.LongTensor(input_seq)\r\n",
        "        target = torch.LongTensor(target)\r\n",
        "        return input_seq, target\r\n"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680030689866
        }
      },
      "id": "45a9e8f0-a90f-4059-8ae2-0a7aa2fdaf9b"
    },
    {
      "cell_type": "code",
      "source": [
        "class CharacterCNN(nn.Module):\r\n",
        "    def __init__(self, input_size, embedding_size, filter_sizes, num_filters):\r\n",
        "        super(CharacterCNN, self).__init__()\r\n",
        "        self.embedding_size = embedding_size\r\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\r\n",
        "        self.conv_layers = nn.ModuleList([\r\n",
        "            nn.Conv1d(in_channels=embedding_size, out_channels=num_filters, kernel_size=fs, padding=0)\r\n",
        "            for fs in filter_sizes\r\n",
        "        ])\r\n",
        "        self.fc = nn.Linear(len(filter_sizes) * num_filters, input_size)\r\n",
        "        \r\n",
        "    def forward(self, input):\r\n",
        "        # input shape: (seq_len, batch_size)\r\n",
        "        embedded = self.embedding(input) # shape: (seq_len, batch_size, embedding_size)\r\n",
        "        embedded = embedded.permute(1, 2, 0) # shape: (batch_size, embedding_size, seq_len)\r\n",
        "        conv_outputs = []\r\n",
        "        for conv in self.conv_layers:\r\n",
        "            conv_outputs.append(torch.relu(conv(embedded)))\r\n",
        "        pooled_outputs = [torch.max(conv_output, dim=-1)[0] for conv_output in conv_outputs]\r\n",
        "        fc_input = torch.cat(pooled_outputs, dim=-1)\r\n",
        "        output = self.fc(fc_input)\r\n",
        "        # print(output.shape)\r\n",
        "        return output\r\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680029633154
        }
      },
      "id": "7d2ebfd2-4549-4926-bcb8-67994ab9ebc4"
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_path = 'data/gutenberg.txt'\r\n",
        "seq_length = 20\r\n",
        "vocab_size = len(vocabulary)\r\n",
        "embedding_size = 128\r\n",
        "# output_size = 100 # size of the word embeddings\r\n",
        "filter_sizes = [3, 4] # filter sizes for convolutional layers\r\n",
        "num_filters = 64 # number of filters for each convolutional layer\r\n",
        "batch_size = 4\r\n",
        "learning_rate = 0.001\r\n",
        "num_epochs = 10\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680029633849
        }
      },
      "id": "56d18853-0fff-494d-912e-f331522f1bc1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset and dataloader\r\n",
        "dataset = CharDataset(corpus_path, seq_length)\r\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680029634784
        }
      },
      "id": "650455bf-809a-493a-ad42-ced2485acef0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model and optimizer\r\n",
        "model = CharacterCNN(vocab_size, embedding_size, filter_sizes, num_filters)\r\n",
        "model.to(device)\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680029637461
        }
      },
      "id": "b7574c23-ee63-4030-99aa-2b2c62d7bc97"
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\r\n",
        "for epoch in range(num_epochs):\r\n",
        "    total_loss = 0\r\n",
        "    for i, (input_seq, target) in enumerate(dataloader):\r\n",
        "        # print(sys.getsizeof(input_seq), sys.getsizeof(target), input_seq.shape)\r\n",
        "        print(input_seq.shape, target.shape)\r\n",
        "        input_seq = input_seq.to(device)\r\n",
        "        print(target.shape)\r\n",
        "        target = target.to(device)\r\n",
        "        optimizer.zero_grad()\r\n",
        "        output = model(input_seq)\r\n",
        "        loss = nn.CrossEntropyLoss()(output, target)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        total_loss += loss.item()\r\n",
        "        if (i + 1) % 100 == 0:\r\n",
        "            print(f'Epoch {epoch + 1}/{num_epochs}, Batch {i + 1}/{len(dataloader)}, Loss: {total_loss / (i + 1)}')\r\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "torch.Size([4, 1048739]) torch.Size([4, 1048739])\ntorch.Size([4, 1048739])\ntorch.Size([1048739, 71])\n"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected input batch_size (1048739) to match target batch_size (4).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m output \u001b[38;5;241m=\u001b[39m model(input_seq)\n\u001b[0;32m---> 12\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1048739) to match target batch_size (4)."
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1680029720070
        }
      },
      "id": "c447f1c1-ec3d-48b4-a4ce-a6759a5c7a52"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "4dd755e2-773d-4165-8054-e27349f7caa7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}