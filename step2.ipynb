{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64b0416-4648-429c-b4db-599e242a0ebc",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "\n",
    "Converting sequences of words into their phoneme counterparts, and then using it to create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7878ab50-8ccc-4cee-9944-4298d6586294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e2c607-de48-4085-9cd1-f1f96253384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = os.getcwd() + \"/data/gtbrg_phonemes_8m.txt\"\n",
    "version = \"s2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041c1ccb-d8ee-4800-9cfe-44be39f94471",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1 - SentencePiece embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50ffa9f1-c9d5-442b-8b74-8b88cc2231e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0075437f-f1ec-427e-a4d5-542263e1d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 6000\n",
    "vocab_size = 19099\n",
    "model_type = \"unigram\"\n",
    "SP_MODEL_NAME = f\"models/{model_type}_vs{vocab_size}_{version}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36140c1-c17f-40d6-83d8-7149f4afd432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`\n",
    "# `m.vocab` is just a reference. not used in the segmentation.\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={input_file} \" \\\n",
    "    f\"--model_type={model_type} \" \\\n",
    "    f\"--model_prefix={SP_MODEL_NAME} \" \\\n",
    "    f\"--vocab_size={vocab_size} \" \\\n",
    "    f\"--max_sentence_length={max_sentence_length} \" \\\n",
    "    f\"--train_extremely_large_corpus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d682060b-b195-4adb-867b-7ee826493882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'a', 'p', 'pl', 'e']\n",
      "[4, 554, 390, 19096, 19096, 22]\n",
      "[4, 554]\n",
      "[4, 121, 19097, 104]\n"
     ]
    }
   ],
   "source": [
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f\"{SP_MODEL_NAME}.model\")\n",
    "\n",
    "# encode: text => id\n",
    "print(sp.EncodeAsPieces('apple'))\n",
    "print(sp.encode_as_ids('boyhood'))\n",
    "print(sp.encode_as_ids('boy'))\n",
    "print(sp.encode_as_ids('man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bebfd3-a719-4ca1-ba3b-584fe0a22537",
   "metadata": {},
   "source": [
    "## Step 2 - Word2Vec encodings\n",
    "\n",
    "We use the SentencePiece encoder on our phoneme dataset to create a new dataset with it's tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b51dd4-b188-4482-b7f4-319e277804cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file) as corpus_file:\n",
    "    corpus = corpus_file.readlines()\n",
    "\n",
    "sentences = [sp.EncodeAsPieces(sentence) for sentence in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba10b7d-db05-43ac-bcd8-e58a6302204a",
   "metadata": {},
   "source": [
    "Now, training Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c4867f4-8e11-42a5-acaf-132f62b71329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57c7053e-d6c9-4d5c-bc39-6448a990d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5\n",
    "vector_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c348c1f-d001-4600-a5b3-38ca25877e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_MODEL_PATH = f\"models/w2v_vs{vector_size}_w{5}_{version}.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b092da-95af-4c7f-b9de-7aee601e93a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, size=vector_size, window=window, min_count=0, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38bcb22-2192-4ca3-b00c-9b8f18697a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(W2V_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0acfbed-7fbe-405f-a13d-895b4a51546d",
   "metadata": {},
   "source": [
    "## Step 3 - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97c990dd-bef9-4adb-88c1-db15a5aa4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from g2p_en import G2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d6f5e1c-eecf-43d3-8ded-427ff7255102",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(W2V_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f55d9494-2342-436a-ae2d-d0ac51f34be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2p = G2p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c79b7a07-a245-4db8-972a-4b6aaaaac3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_phonemes(word):\n",
    "    parsed_text = \"\".join(g2p(word))\n",
    "    final_text = \"\"\n",
    "\n",
    "    for char in parsed_text:\n",
    "        if char.isalpha():\n",
    "            final_text += char.lower()\n",
    "\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2807f57a-5e02-40f1-881f-c87fd1473ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_pwords(test_word):\n",
    "    parsed_text = \"\".join(g2p(test_word))\n",
    "    final_text = \"\"\n",
    "\n",
    "    for char in parsed_text:\n",
    "        if char.isalpha():\n",
    "            final_text += char.lower()\n",
    "\n",
    "    return model.wv.most_similar(final_text, topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee732d7d-4722-4e6e-a90b-f35095cdf4ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'hhyuwmahn' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_496/2409042606.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmost_similar_pwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_496/1670204293.py\u001b[0m in \u001b[0;36mmost_similar_pwords\u001b[0;34m(test_word)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mfinal_text\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'hhyuwmahn' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "most_similar_pwords(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cdcda82-46ab-46ed-b69c-25a45ceec0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fruwts', 0.7007562518119812),\n",
       " ('dhahfruwt', 0.6919422149658203),\n",
       " ('vehjhtahbahlz', 0.6699405908584595),\n",
       " ('fuwd', 0.6603603363037109),\n",
       " ('brehd', 0.6302711963653564)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_pwords(\"fruit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f590583-8585-475a-8fce-823fb84f1b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kwiyn', 0.6846692562103271),\n",
       " ('daoter', 0.6765732765197754),\n",
       " ('prihnsehs', 0.6557174921035767),\n",
       " ('kawntahs', 0.6210659742355347),\n",
       " ('sihster', 0.6079518795013428)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"kihng\", \"wuhmahn\"], negative=[\"maen\"], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8774b90-c60a-4135-9637-5271a0510f86",
   "metadata": {},
   "source": [
    "## Step 4 - Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2656ce7c-0437-4adc-b41c-9c03f6235e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f27e0fc9-3d83-4abd-a7db-a98c0ce6a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsim_scores = []\n",
    "\n",
    "with open(\"data/wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\") as wordsim_fp:\n",
    "    for line in wordsim_fp.readlines():\n",
    "        scores = line.split(\"\\t\")\n",
    "        w1, w2 = scores[0], scores[1]\n",
    "        gold_score = float(scores[2])\n",
    "        wordsim_scores.append([w1, w2, gold_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96adc148-117d-42ba-9d8f-255feae1a05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'q' in list(model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43052322-80e3-45fa-a674-09391ce286e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['k', 'i', 'h', 'n', 'g']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([i.lower() for i in \"\".join(g2p(w1)) if i.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9ec0229-b664-4462-afb9-ddc274b342c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4872273878508886 , tested 121/203 pairs\n",
      "0.35459617632606777 , including OOV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/scipy/stats/_stats_py.py:118: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  a = np.asarray(a)\n"
     ]
    }
   ],
   "source": [
    "gold_vocab = []\n",
    "gold_all = []\n",
    "preds_vocab = []\n",
    "preds_all = []\n",
    "tested = 0\n",
    "oov = 0\n",
    "\n",
    "for pairs in wordsim_scores:\n",
    "    w1, w2 = pairs[0].lower(), pairs[1].lower()\n",
    "    \n",
    "    w1 = \"\".join([i.lower() for i in \"\".join(g2p(w1)) if i.isalpha()])\n",
    "    w2 = \"\".join([i.lower() for i in \"\".join(g2p(w2)) if i.isalpha()])\n",
    "    \n",
    "    try:\n",
    "        pred = model.wv.similarity(w1, w2)\n",
    "        preds_vocab.append(pred)\n",
    "        gold_vocab.append(pairs[2])\n",
    "        tested += 1\n",
    "    \n",
    "    except KeyError:\n",
    "        if w1 not in model.wv.vocab.keys():\n",
    "            w1_units = sp.EncodeAsPieces(w1)[1:]\n",
    "            w1_vectors = np.array([model.wv[unit] for unit in w1_units])\n",
    "            w1_vector = w1_vectors.sum(axis=0)\n",
    "        else:\n",
    "            w1_vector = model.wv[w1]\n",
    "        if w2 not in model.wv.vocab.keys():\n",
    "            w2_units = sp.EncodeAsPieces(w2)[1:]\n",
    "            w2_vectors = np.array([model.wv[unit] for unit in w2_units])\n",
    "            w2_vector = w2_vectors.sum(axis=0)\n",
    "        else:\n",
    "            w2_vector = model.wv[w2]\n",
    "\n",
    "        pred = cosine_similarity(w1_vector.reshape(1, -1), w2_vector.reshape(1, -1))\n",
    "        oov += 1\n",
    "    \n",
    "    preds_all.append(pred)\n",
    "    gold_all.append(pairs[2])\n",
    "        \n",
    "\n",
    "print(spearmanr(preds_vocab, gold_vocab)[0], f\", tested {tested}/{len(wordsim_scores)} pairs\")\n",
    "print(spearmanr(preds_all, gold_all)[0], f\", including OOV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0417f71-3b2f-4f5a-ba8a-005b49f9a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simlex_scores = []\n",
    "\n",
    "with open(\"data/SimLex-999/SimLex-999.txt\") as simlex_fp:\n",
    "    for line in simlex_fp.readlines()[1:]:\n",
    "        scores = line.split(\"\\t\")\n",
    "        w1, w2 = scores[0], scores[1]\n",
    "        gold_score = float(scores[3])\n",
    "        simlex_scores.append([w1, w2, gold_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11f9b10f-6f7b-4a77-91e5-f6d03c9c742e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24245591386571747 , tested 775/999 pairs\n",
      "0.14236843257028958 , including OOV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/scipy/stats/_stats_py.py:118: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  a = np.asarray(a)\n"
     ]
    }
   ],
   "source": [
    "gold_vocab = []\n",
    "gold_all = []\n",
    "preds_vocab = []\n",
    "preds_all = []\n",
    "tested = 0\n",
    "oov = 0\n",
    "\n",
    "for pairs in simlex_scores:\n",
    "    w1, w2 = pairs[0].lower(), pairs[1].lower()\n",
    "    \n",
    "    w1 = \"\".join([i.lower() for i in \"\".join(g2p(w1)) if i.isalpha()])\n",
    "    w2 = \"\".join([i.lower() for i in \"\".join(g2p(w2)) if i.isalpha()])\n",
    "    \n",
    "    try:\n",
    "        pred = model.wv.similarity(w1, w2)\n",
    "        preds_vocab.append(pred)\n",
    "        gold_vocab.append(pairs[2])\n",
    "        tested += 1\n",
    "    \n",
    "    except KeyError:\n",
    "        if w1 not in model.wv.vocab.keys():\n",
    "            w1_units = sp.EncodeAsPieces(w1)[1:]\n",
    "            w1_vectors = np.array([model.wv[unit] for unit in w1_units])\n",
    "            w1_vector = w1_vectors.sum(axis=0)\n",
    "        else:\n",
    "            w1_vector = model.wv[w1]\n",
    "        if w2 not in model.wv.vocab.keys():\n",
    "            w2_units = sp.EncodeAsPieces(w2)[1:]\n",
    "            w2_vectors = np.array([model.wv[unit] for unit in w2_units])\n",
    "            w2_vector = w2_vectors.sum(axis=0)\n",
    "        else:\n",
    "            w2_vector = model.wv[w2]\n",
    "\n",
    "        pred = cosine_similarity(w1_vector.reshape(1, -1), w2_vector.reshape(1, -1))\n",
    "        oov += 1\n",
    "    \n",
    "    preds_all.append(pred)\n",
    "    gold_all.append(pairs[2])\n",
    "        \n",
    "\n",
    "print(spearmanr(preds_vocab, gold_vocab)[0], f\", tested {tested}/{len(simlex_scores)} pairs\")\n",
    "print(spearmanr(preds_all, gold_all)[0], f\", including OOV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe2aa3fb-8aa1-4007-ac68-ca386d0d6c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40965265"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(\"fihz\", \"kehm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e48bfa50-c8f1-466f-b2c3-703e1146f8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4911ed3-20ce-421a-8fe1-c6c1dcef6130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', 'dɐ', 'praʄektɠ', 'ó', 'tɐnbɜg']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.wv.vocab.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7bf6e9-f960-4320-a3d8-1a9ba88c39dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
