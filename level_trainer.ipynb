{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from os import makedirs\n",
    "import logging\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencePieceConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_sentence_length: int = 5000,\n",
    "        vocab_size: int = 20000,\n",
    "        model_type: str = \"unigram\",\n",
    "        model_tag: str = \"lw\"\n",
    "    ):\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model_type = model_type\n",
    "        self.model_tag = model_tag\n",
    "\n",
    "    def get_model_name(\n",
    "            self,\n",
    "            model_name: str = None,\n",
    "            model_dir: str = \"models/level_wise/levelX\"\n",
    "    ):\n",
    "        if model_name is not None:\n",
    "            return model_name\n",
    "        return f\"{model_dir.strip('/')}/\" \\\n",
    "            f\"{self.model_type}_vs{self.vocab_size}_{self.model_tag}\"\n",
    "\n",
    "class Word2VecConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_size: int = 100,\n",
    "        window: int = 5,\n",
    "        model_tag: str = \"lw\"\n",
    "    ):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.model_tag = model_tag\n",
    "\n",
    "    def get_model_path(\n",
    "            self,\n",
    "            model_name: str = None,\n",
    "            model_dir: str = \"models/level_wise/levelX\"\n",
    "    ):\n",
    "        if model_name is not None:\n",
    "            return model_name\n",
    "        return f\"{model_dir.strip('/')}/\" \\\n",
    "            f\"w2v_vs{self.vector_size}_w{self.window}_{self.model_tag}.model\"\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(\n",
    "            self,\n",
    "            sp_config: SentencePieceConfig = SentencePieceConfig(),\n",
    "            w2v_config: Word2VecConfig = Word2VecConfig(),\n",
    "            cluster_threshold: float = 0.45\n",
    "    ):\n",
    "        self.sp_config = sp_config\n",
    "        self.w2v_config = w2v_config\n",
    "        self.cluster_threshold = cluster_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "    def __init__(\n",
    "            self,\n",
    "            threshold: float = None,\n",
    "            model: str | Word2Vec = None,\n",
    "            map_file: str = None,\n",
    "            log_dir: str = None\n",
    "    ):\n",
    "        self.cluster_to_words = dict()\n",
    "        self.word_to_cluster = dict()\n",
    "\n",
    "        if model is not None and threshold is not None:\n",
    "            self.create_clusters_from_model(model, threshold)\n",
    "        elif map_file is not None:\n",
    "            self.load_mapping(map_file)\n",
    "\n",
    "        self.log_dir = log_dir\n",
    "        self.logger = logging.getLogger(\"Cluster\")\n",
    "        self.logger.setLevel(\"DEBUG\")\n",
    "        if log_dir is not None:\n",
    "            cluster_log_fh = logging.FileHandler(self.log_dir +\"clusters_log.txt\")\n",
    "            log_fh = logging.FileHandler(self.log_dir +\"log.txt\")\n",
    "            cluster_log_fh.setLevel(\"DEBUG\")\n",
    "            log_fh.setLevel(\"DEBUG\")\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            cluster_log_fh.setFormatter(formatter)\n",
    "            log_fh.setFormatter(formatter)\n",
    "            self.logger.addHandler(cluster_log_fh)\n",
    "            self.logger.addHandler(log_fh)\n",
    "\n",
    "    def create_clusters_from_model(\n",
    "            self,\n",
    "            model: Word2Vec,\n",
    "            threshold: float\n",
    "    ):\n",
    "        words = list(model.wv.keys_to_index.keys())\n",
    "\n",
    "        cluster_idx = 0  # Counter\n",
    "\n",
    "        for word in words:\n",
    "            # Check if word has already been clustered\n",
    "            if word not in self.word_to_cluster.keys():\n",
    "                # Create new cluster\n",
    "                cluster_idx += 1\n",
    "                while (not chr(0x0020 + cluster_idx).isalpha()) or len(chr(0x0020 + cluster_idx)) > 1:\n",
    "                    cluster_idx += 1\n",
    "                cluster_key = chr(0x0020 + cluster_idx)\n",
    "\n",
    "                # Add new word to cluster\n",
    "                self.cluster_to_words[cluster_key] = [word]\n",
    "                self.word_to_cluster[word] = cluster_key\n",
    "                \n",
    "                # Add all similar words\n",
    "                for similar_word, score in model.wv.most_similar(word, topn=200):\n",
    "                    if score > threshold:\n",
    "                        self.cluster_to_words[cluster_key].append(similar_word)\n",
    "                        self.word_to_cluster[similar_word] = cluster_key\n",
    "        \n",
    "        self.logger.info(f\"Created {len(self.cluster_to_words)} clusters.\")\n",
    "\n",
    "    def load_mapping(\n",
    "            self,\n",
    "            map_file: str\n",
    "    ) -> None:\n",
    "        with open(map_file, \"r\", encoding=\"utf-8\") as map_fp:\n",
    "            for line in map_fp.readlines():\n",
    "                cluster, words = line.split(\"\\t\")\n",
    "                words = words.split(\",\")\n",
    "                self.cluster_to_words[cluster] = words\n",
    "                for word in words:\n",
    "                    self.word_to_cluster[word] = cluster\n",
    "\n",
    "    def save_mapping(\n",
    "            self,\n",
    "            map_file: str = None,\n",
    "            map_dir: str = \"data/level_wise/levelX\"\n",
    "    ) -> None:\n",
    "        if map_file is None:\n",
    "            map_dir = map_dir.strip(\"/\")\n",
    "            makedirs(map_dir)\n",
    "            map_file = map_dir + \"/clusters.txt\"\n",
    "        with open(map_file, \"w+\", encoding=\"utf-8\") as map_fp:\n",
    "            for cluster in list(self.cluster_to_words.keys()):\n",
    "                map_fp.write(\n",
    "                    cluster +\"\\t\"\n",
    "                    +\",\".join([word for word in self.cluster_to_words[cluster]])\n",
    "                    +\"\\n\"\n",
    "                )\n",
    "\n",
    "\n",
    "class WordToUtteranceMapping:\n",
    "    def __init__(\n",
    "            self,\n",
    "            map_file: str = None\n",
    "    ):\n",
    "        self.utterances = {}\n",
    "        if map_file is not None:\n",
    "            self.load_mapping(map_file)\n",
    "\n",
    "    def load_mapping(\n",
    "            self,\n",
    "            map_file: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Loads mapping from a file with word and utterance separated by a tab-space\n",
    "        \"\"\"\n",
    "        with open(map_file, \"r\") as utterance_file:\n",
    "            for line in utterance_file.readlines():\n",
    "                key, seq = line.strip().split(\"\\t\")\n",
    "\n",
    "                if key not in self.utterances:\n",
    "                    self.utterances[key] = []\n",
    "\n",
    "                self.utterances[key].append(seq)\n",
    "\n",
    "    def get_vectors_from_word(\n",
    "            self,\n",
    "            word,\n",
    "            sp_model: spm.SentencePieceProcessor,\n",
    "            w2v_model: Word2Vec\n",
    "    ):\n",
    "        return np.array(\n",
    "            [self.get_vector_from_utterance(\n",
    "                utterance,\n",
    "                sp_model,\n",
    "                w2v_model\n",
    "            ) for utterance in self.utterances[word]]\n",
    "        )\n",
    "\n",
    "    def get_vector_from_utterance(\n",
    "            self,\n",
    "            utterance,\n",
    "            sp_model: spm.SentencePieceProcessor,\n",
    "            w2v_model: Word2Vec\n",
    "    ):\n",
    "        if utterance in w2v_model.wv.key_to_index.keys():\n",
    "            return w2v_model.wv[utterance].reshape(1, -1)\n",
    "        else:\n",
    "            pieces = list(\n",
    "                filter(\n",
    "                    lambda x: x != \"_\",\n",
    "                    sp_model.EncodeAsPieces(utterance)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            units = [piece.replace(\"_\", \"\") for piece in pieces]\n",
    "\n",
    "            vectors = np.array([w2v_model.wv[unit] for unit in units])\n",
    "            return vectors.mean(axis=0).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestBench:\n",
    "    def __init__(\n",
    "            self,\n",
    "            scores_file: str = None\n",
    "    ) -> None:\n",
    "        if scores_file:\n",
    "            self.load_scores(scores_file)\n",
    "\n",
    "    def load_scores(\n",
    "            self,\n",
    "            scores_file: str\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    def score(\n",
    "            self,\n",
    "            sp_model: spm.SentencePieceProcessor,\n",
    "            w2v_model: Word2Vec,\n",
    "            utterances: WordToUtteranceMapping\n",
    "    ) -> dict:\n",
    "        pass\n",
    "\n",
    "\n",
    "class LSTestBench(TestBench):\n",
    "    def __init__(self, scores_file: str = None) -> None:\n",
    "        super().__init__(scores_file)\n",
    "\n",
    "    def load_scores(\n",
    "            self,\n",
    "            scores_file: str\n",
    "    ):\n",
    "        self.sim_pairs = []\n",
    "        self.rel_pairs = []\n",
    "\n",
    "        with open(scores_file, \"r\") as pairs_file:\n",
    "            for line in pairs_file.readlines()[1:]:\n",
    "                w1, w2, sim, rel = line.strip().split(\",\")\n",
    "                if sim:\n",
    "                    self.sim_pairs.append((w1, w2, float(sim)))\n",
    "                if rel:\n",
    "                    self.rel_pairs.append((w1, w2, float(rel)))\n",
    "\n",
    "    def single_test(\n",
    "            self,\n",
    "            pairs: list,\n",
    "            sp_model: spm.SentencePieceProcessor,\n",
    "            w2v_model: Word2Vec,\n",
    "            utterances: WordToUtteranceMapping\n",
    "    ):\n",
    "        scores = {\n",
    "            test_set: {\n",
    "                method: [] for method in [\"min\", \"max\", \"avg\", \"all\"]\n",
    "            } for test_set in [\"librispeech\", \"synthetic\"]\n",
    "        }\n",
    "        gold_standard = {\n",
    "            \"librispeech\": [],\n",
    "            \"synthetic\": []\n",
    "        }\n",
    "        trials = 0\n",
    "        errors = 0\n",
    "\n",
    "        for pair in pairs:\n",
    "            try:\n",
    "                w1, w2, rel = pair\n",
    "\n",
    "                test_set = \"librispeech\" if w1.startswith(\"ls_\") else \"synthetic\"\n",
    "                w1.replace(\"ls_\", \"\").replace(\"sy_\", \"\")\n",
    "                w2.replace(\"ls_\", \"\").replace(\"sy_\", \"\")\n",
    "\n",
    "                w1_vectors = utterances.get_vectors_from_word(w1, sp_model, w2v_model)\n",
    "                w2_vectors = utterances.get_vectors_from_word(w2, sp_model, w2v_model)\n",
    "\n",
    "                similarities = [cosine_similarity(i, j) for i in w1_vectors for j in w2_vectors]\n",
    "\n",
    "                scores[test_set][\"min\"].append(np.min(similarities))\n",
    "                scores[test_set][\"avg\"].append(np.mean(similarities))\n",
    "                scores[test_set][\"max\"].append(np.max(similarities))\n",
    "\n",
    "                gold_standard[test_set].append(rel)\n",
    "            except:\n",
    "                errors += 1\n",
    "            trials += 1\n",
    "        \n",
    "        return {\n",
    "            'score' : {\n",
    "                dataset : {\n",
    "                    variant : pearsonr(scores[dataset][variant], gold_standard[dataset])[0] * 100\n",
    "                    for variant in scores[dataset].keys()\n",
    "                }\n",
    "                for dataset in scores.keys()\n",
    "            },\n",
    "            'errors' : errors,\n",
    "            'trials' : trials\n",
    "        }\n",
    "\n",
    "\n",
    "    def score(\n",
    "            self,\n",
    "            sp_model: spm.SentencePieceProcessor,\n",
    "            w2v_model: Word2Vec,\n",
    "            utterances: WordToUtteranceMapping\n",
    "    ) -> dict:\n",
    "        tests = {'sim' : self.sim_pairs, 'rel' : self.rel_pairs}\n",
    "\n",
    "        return {\n",
    "            test : self.single_test(\n",
    "                tests[test],\n",
    "                sp_model,\n",
    "                w2v_model,\n",
    "                utterances\n",
    "            ) for test in tests\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LevelwiseModels:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_dir: str = \"models/level_wise/\",\n",
    "            data_dir: str = \"data/level_wise/\",\n",
    "            log_dir: str = \"logs/level_wise/\"\n",
    "    ):\n",
    "        self.n_levels = 0\n",
    "\n",
    "        self.model_dir = model_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        self.logger = logging.getLogger(\"LevelwiseModels\")\n",
    "        self.logger.setLevel(\"DEBUG\")\n",
    "        if log_dir is not None:\n",
    "            log_fh = logging.FileHandler(self.log_dir +\"log.txt\")\n",
    "            log_fh.setLevel(\"DEBUG\")\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            log_fh.setFormatter(formatter)\n",
    "            self.logger.addHandler(log_fh)\n",
    "\n",
    "        self.sp_models = []\n",
    "        self.w2v_models = []\n",
    "        self.clusters = []\n",
    "        self.mappings = []\n",
    "\n",
    "    def _create_level_folders(\n",
    "            self,\n",
    "            level: int\n",
    "    ):\n",
    "        data_dir = f\"{self.data_dir}/level{level}\"\n",
    "        model_dir = f\"{self.model_dir}/level{level}\"\n",
    "        makedirs(data_dir)\n",
    "        makedirs(model_dir)\n",
    "        return data_dir, model_dir\n",
    "\n",
    "    def train(\n",
    "            self,\n",
    "            input_file: str,\n",
    "            n_levels: int = 1,\n",
    "            configs: list[Config] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Main training function\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "            input_file : str\n",
    "                Filepath of the input. The file must contain a sentence\n",
    "                per line, with maximum line length of 5000.\n",
    "            configs : list\n",
    "                Arguments for SentencePiece and Word2Vec at each level.\n",
    "        \"\"\"\n",
    "        for level in range(1, n_levels + 1):\n",
    "            self.logger.info(f\"STARTING LEVEL {level}\")\n",
    "\n",
    "            # Create folder for level files\n",
    "            data_dir, model_dir = self._create_level_folders(level)\n",
    "            self.logger.info(\"Created level folders.\")\n",
    "\n",
    "            # Load run config\n",
    "            config = Config()\n",
    "            if len(configs) >= level:\n",
    "                config = configs[level]\n",
    "\n",
    "            # Train SentencePiece\n",
    "            spm.SentencePieceTrainer.train(\n",
    "                f\"--input={input_file} \" \\\n",
    "                f\"--model_type={config.sp_config.model_type} \" \\\n",
    "                f\"--model_prefix={config.sp_config.get_model_name(model_dir=model_dir)} \" \\\n",
    "                f\"--vocab_size={config.sp_config.vocab_size} \" \\\n",
    "                f\"--max_sentence_length={config.sp_config.max_sentence_length} \" \\\n",
    "                f\"--train_extremely_large_corpus\"\n",
    "            )\n",
    "            self.logger.info(\"Created SentencePiece model - \"\n",
    "                             f\"{config.sp_config.get_model_name(model_dir=model_dir)}.\")\n",
    "\n",
    "            # Load SentencePiece model\n",
    "            sp_model = spm.SentencePieceProcessor()\n",
    "            sp_model.load(f\"{config.sp_config.get_model_name(model_dir=model_dir)}.model\")\n",
    "\n",
    "\n",
    "            # Convert input file to sentences\n",
    "            with open(input_file, \"r\", encoding=\"utf-8\") as corpus_file:\n",
    "                corpus = corpus_file.readlines()\n",
    "\n",
    "            sentences = []\n",
    "            for sentence in corpus:\n",
    "                pieces = list(\n",
    "                    filter(\n",
    "                        lambda x: x != \"_\",\n",
    "                        sp_model.EncodeAsPieces(sentence)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                new_pieces = [piece.replace(\"_\", \"\") for piece in pieces]\n",
    "                sentences.append(new_pieces)\n",
    "            self.logger.info(f\"Converted input to sentences (Sample = {sentences[0][:10]})\")\n",
    "\n",
    "\n",
    "            # Train Word2Vec\n",
    "            w2v_model = Word2Vec(\n",
    "                sentences,\n",
    "                window=config.w2v_config.window,\n",
    "                vector_size=config.w2v_config.vector_size,\n",
    "                min_count=0,\n",
    "                workers=4,\n",
    "                epochs=7\n",
    "            )\n",
    "            w2v_model.save(config.w2v_config.get_model_path(model_dir=model_dir))\n",
    "\n",
    "\n",
    "            # Perform clustering\n",
    "            cluster = Cluster(model=w2v_model, threshold=config.cluster_threshold)\n",
    "\n",
    "            # Save next level corpus\n",
    "            with open(f\"{data_dir}/corpus.txt\", \"w+\", encoding=\"utf-8\") as corpus_fp:\n",
    "                for line in sentences:\n",
    "                    corpus_fp.write(\n",
    "                        \"\".join(\n",
    "                            cluster.word_to_cluster[piece] for piece in line\n",
    "                        ) + \"\\n\"\n",
    "                    )\n",
    "            self.logger.info(f\"Created level {level} corpus.\")\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            # Save all files\n",
    "            cluster.save_mapping()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
