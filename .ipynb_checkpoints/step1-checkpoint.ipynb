{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c355c92-8b8c-4161-b033-3becc7bdd33a",
   "metadata": {},
   "source": [
    "# Thesis - Step 1\n",
    "\n",
    "Using pure text data to get best similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f10954-bf97-4afc-afc5-ec3ab2f55994",
   "metadata": {
    "gather": {
     "logged": 1680194273976
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c9adf-f4ec-4b4a-873b-2c226205bd77",
   "metadata": {},
   "source": [
    "## Approach 1 - SentencePiece\n",
    "\n",
    "This approach uses SentencePiece on text data with only the letters to try and find words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d6f33c-b3ad-4f9e-8a71-d131915b1b64",
   "metadata": {
    "gather": {
     "logged": 1680194274974
    }
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec298bb-7639-4c4d-983b-75ed45c38643",
   "metadata": {
    "gather": {
     "logged": 1680031548880
    }
   },
   "outputs": [],
   "source": [
    "input_file = os.getcwd() + \"/data/gtbrg_8m_lines.txt\"\n",
    "version = 8\n",
    "max_sentence_length = 4002\n",
    "vocab_size = 19099\n",
    "model_type = \"unigram\"\n",
    "SP_MODEL_NAME = f\"models/{model_type}_{vocab_size}_v{version}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd64cda8-e4db-4816-934a-3bd87dcf0f5e",
   "metadata": {
    "gather": {
     "logged": 1680031587088
    }
   },
   "outputs": [],
   "source": [
    "# train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`\n",
    "# `m.vocab` is just a reference. not used in the segmentation.\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={input_file} \" \\\n",
    "    f\"--model_type={model_type} \" \\\n",
    "    f\"--model_prefix={SP_MODEL_NAME} \" \\\n",
    "    f\"--vocab_size={vocab_size} \" \\\n",
    "    f\"--max_sentence_length={max_sentence_length} \" \\\n",
    "    f\"--train_extremely_large_corpus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef756792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/gutenberg_no_spaces.txt\") as fp:\n",
    "#     print(fp.readlines()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5458ab-230c-4494-b81d-1a03f0e974e1",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680194771086
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# SP_MODEL_NAME = \"models/unigram_8k_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c205055-8cb3-4391-b73d-acfaa9c1ef53",
   "metadata": {
    "gather": {
     "logged": 1680194772113
    }
   },
   "outputs": [],
   "source": [
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f\"{SP_MODEL_NAME}.model\")\n",
    "\n",
    "# encode: text => id\n",
    "print(sp.EncodeAsPieces('apple'))\n",
    "print(sp.encode_as_ids('boyhood'))\n",
    "print(sp.encode_as_ids('boy'))\n",
    "print(sp.encode_as_ids('man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72c9453-50a4-4e22-8ebd-7020fefe4609",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Loading the vocabulary created by SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10091f5e-499d-4301-a19f-3ba88bf86e86",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680194951102
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/gutenberg_no_spaces.txt\") as corpus_file:\n",
    "    corpus = corpus_file.readlines()\n",
    "\n",
    "sentences = [sp.EncodeAsPieces(sentence) for sentence in corpus]\n",
    "# sentences = [' '.join(sentence) for sentence in corpus]\n",
    "# sentences = [list(sentence) for sentence in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab30628-7d4a-48ec-9e38-5027e7a06a59",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Approach 1.1 - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4e1d074-fd74-4e64-9f78-e5865d3a72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00c3bf-0be8-4256-9ada-b25b580268e5",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680004547121
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "W2V_MODEL_PATH = f\"models/w2v_100_v{version}.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaf5159-36c3-4638-bb3e-8deb8d8ea6d4",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680005530991
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, window=5, min_count=0, workers=4)\n",
    "# model.build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba718e9-7e56-4962-8f55-2896b45c274a",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680004691811
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model.save(W2V_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0f2a252-1d31-477d-8f7d-85e9e582dce5",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680004328602
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(W2V_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b160f-d95f-4652-a5f9-25446e422ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b3458ab-8d5e-444f-a079-16dab88cede3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‚ñÅ', 'ban', 'ana']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.EncodeAsPieces(\"banana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ffd11ce-0b84-4ad0-9c36-ea4e23e99a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thehuman', 0.6968255043029785),\n",
       " ('external', 0.6862231492996216),\n",
       " ('spiritual', 0.6827719211578369),\n",
       " ('rational', 0.6694786548614502),\n",
       " ('physical', 0.6432474255561829),\n",
       " ('universal', 0.6383942365646362),\n",
       " ('complex', 0.6383225321769714),\n",
       " ('animate', 0.6229571104049683),\n",
       " ('vulgar', 0.6203749179840088),\n",
       " ('ofthemind', 0.6202679872512817)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3817dab-eb26-49ed-a3e8-75ff44975f0a",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680012211655
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tree', 0.6462252140045166),\n",
       " ('orange', 0.6357700824737549),\n",
       " ('cherry', 0.6354914903640747),\n",
       " ('worm', 0.6290103197097778),\n",
       " ('egg', 0.6227107048034668),\n",
       " ('apples', 0.6179237365722656),\n",
       " ('clover', 0.6081827878952026),\n",
       " ('cake', 0.6053444147109985),\n",
       " ('crab', 0.6045732498168945),\n",
       " ('salmon', 0.598516583442688)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcbdb197-a10d-4e03-89b0-8af92ae68e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7470260858535767),\n",
       " ('maiden', 0.6551803350448608),\n",
       " ('daughter', 0.6430643796920776),\n",
       " ('sister', 0.6402992010116577),\n",
       " ('prince', 0.638314962387085),\n",
       " ('countess', 0.5871137380599976),\n",
       " ('dame', 0.5857516527175903),\n",
       " ('thequeen', 0.5773256421089172),\n",
       " ('princess', 0.5710912346839905),\n",
       " ('damsel', 0.560126543045044)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02409257-9feb-4437-8750-fb8662f635b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'banana' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_264/2695128023.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"banana\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'banana' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.wv.most_similar(\"banana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad6866-adfc-4bef-a920-43f40bf70441",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "source": [
    "## Approach 2 - WordPiece tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7b81a1-8d47-41ae-a07b-eab52ff02f6b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import PreTokenizer\n",
    "\n",
    "class FixedLengthPreTokenizer(PreTokenizer):\n",
    "    def __init__(self, n=3):\n",
    "        self.n = n\n",
    "        super().__init__()\n",
    "\n",
    "    def pre_tokenize(self, text):\n",
    "        return [(i, i+self.n) for i in range(0, len(text), self.n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696fe546-61db-4ddd-96d8-bb63c9954d5f",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1679686719830
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import trainers\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(WordPiece())\n",
    "\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=10000, \n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    initial_alphabet= list(\n",
    "        \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    ),\n",
    "    continuing_subword_prefix=\"##\",\n",
    "    show_progress=True,\n",
    "    min_frequency=1\n",
    ")\n",
    "\n",
    "# tokenizer.pre_tokenizer = FixedLengthPreTokenizer()  # or another value for n\n",
    "\n",
    "tokenizer.train(\n",
    "    files=[\"data/gutenberg_no_spaces.txt\"], \n",
    "    trainer=trainer\n",
    ")\n",
    "\n",
    "tokenizer.save(\"models/tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b206bacb-ebb0-46d5-a865-b1e4d8ebc799",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Approach 3 - Character CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29babca4-750a-41b2-b69e-38535364ebc8",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680029630371
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b89c4832-4fc5-486d-aa02-c75f2acd20d0",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680029630479
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3810f35b-5b5a-4a13-9549-4dbdb4b6111b",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680030688818
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '√†', '√°', '√¢', '√£', '√§', '√¶', '√ß', '√®', '√©', '√™', '√´', '√¨', '√≠', '√Æ', '√Ø', '√±', '√≤', '√≥', '√¥', '√∂', '√π', '√ª', '√º', 'ƒÅ', '≈ì', 'Œ±', 'Œ≤', 'Œ¥', 'Œµ', 'Œ∑', 'Œ∏', 'Œπ', 'Œ∫', 'Œª', 'Œº', 'ŒΩ', 'Œø', 'œÄ', 'œÅ', 'œÇ', 'œÉ', 'œÑ', 'œÖ', 'œÜ', 'œâ']\n"
     ]
    }
   ],
   "source": [
    "# To prevent recomputing alphabet each time\n",
    "vocab_path = 'data/gutenberg_vocabulary.txt'\n",
    "\n",
    "vocabulary = sorted(set(open(vocab_path).read().split()))\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45a9e8f0-a90f-4059-8ae2-0a7aa2fdaf9b",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680030689866
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, corpus_path, seq_length):\n",
    "        self.corpus_path = corpus_path\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab = vocabulary # Load the unique characters in the corpus\n",
    "        self.char_to_index = {c: i for i, c in enumerate(self.vocab)} # Map each character to an index\n",
    "        self.index_to_char = {i: c for i, c in enumerate(self.vocab)} # Map each index to a character\n",
    "        self.corpus_size = os.path.getsize(corpus_path)\n",
    "        # self.num_chunks = int(self.corpus_size / (1024 * 1024)) # Split the corpus into 1MB chunks\n",
    "        # self.chunk_size = int(self.corpus_size / self.num_chunks)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_chunks\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_pos = idx * self.chunk_size\n",
    "        # end_pos = (idx + 1) * self.chunk_size\n",
    "        with open(self.corpus_path) as f:\n",
    "            f.seek(start_pos)\n",
    "            chunk = f.read(self.chunk_size).replace('\\n', '')\n",
    "        input_seq = chunk[:-1]\n",
    "        target = chunk[1:]\n",
    "        input_seq = [self.char_to_index[c] for c in input_seq]\n",
    "        target = [self.char_to_index[c] for c in target]\n",
    "        input_seq = torch.LongTensor(input_seq)\n",
    "        target = torch.LongTensor(target)\n",
    "        return input_seq, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2ebfd2-4549-4926-bcb8-67994ab9ebc4",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680029633154
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "class CharacterCNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, filter_sizes, num_filters):\n",
    "        super(CharacterCNN, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_size, out_channels=num_filters, kernel_size=fs, padding=0)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, input_size)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # input shape: (seq_len, batch_size)\n",
    "        embedded = self.embedding(input) # shape: (seq_len, batch_size, embedding_size)\n",
    "        embedded = embedded.permute(1, 2, 0) # shape: (batch_size, embedding_size, seq_len)\n",
    "        conv_outputs = []\n",
    "        for conv in self.conv_layers:\n",
    "            conv_outputs.append(torch.relu(conv(embedded)))\n",
    "        pooled_outputs = [torch.max(conv_output, dim=-1)[0] for conv_output in conv_outputs]\n",
    "        fc_input = torch.cat(pooled_outputs, dim=-1)\n",
    "        output = self.fc(fc_input)\n",
    "        # print(output.shape)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56d18853-0fff-494d-912e-f331522f1bc1",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680029633849
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "corpus_path = 'data/gutenberg.txt'\n",
    "seq_length = 20\n",
    "vocab_size = len(vocabulary)\n",
    "embedding_size = 128\n",
    "# output_size = 100 # size of the word embeddings\n",
    "filter_sizes = [3, 4] # filter sizes for convolutional layers\n",
    "num_filters = 64 # number of filters for each convolutional layer\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "650455bf-809a-493a-ad42-ced2485acef0",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680029634784
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset = CharDataset(corpus_path, seq_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7574c23-ee63-4030-99aa-2b2c62d7bc97",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680029637461
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create model and optimizer\n",
    "model = CharacterCNN(vocab_size, embedding_size, filter_sizes, num_filters)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c447f1c1-ec3d-48b4-a4ce-a6759a5c7a52",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680029720070
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1048739]) torch.Size([4, 1048739])\n",
      "torch.Size([4, 1048739])\n",
      "torch.Size([1048739, 71])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1048739) to match target batch_size (4).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m output \u001b[38;5;241m=\u001b[39m model(input_seq)\n\u001b[0;32m---> 12\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1048739) to match target batch_size (4)."
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (input_seq, target) in enumerate(dataloader):\n",
    "        # print(sys.getsizeof(input_seq), sys.getsizeof(target), input_seq.shape)\n",
    "        print(input_seq.shape, target.shape)\n",
    "        input_seq = input_seq.to(device)\n",
    "        print(target.shape)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_seq)\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Batch {i + 1}/{len(dataloader)}, Loss: {total_loss / (i + 1)}')\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd755e2-773d-4165-8054-e27349f7caa7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
