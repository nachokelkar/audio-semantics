{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c355c92-8b8c-4161-b033-3becc7bdd33a",
   "metadata": {},
   "source": [
    "# Thesis - Step 1\n",
    "\n",
    "Using pure text data to get best similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57f10954-bf97-4afc-afc5-ec3ab2f55994",
   "metadata": {
    "gather": {
     "logged": 1680194273976
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c9adf-f4ec-4b4a-873b-2c226205bd77",
   "metadata": {},
   "source": [
    "## Approach 1 - SentencePiece\n",
    "\n",
    "This approach uses SentencePiece on text data with only the letters to try and find words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7d6f33c-b3ad-4f9e-8a71-d131915b1b64",
   "metadata": {
    "gather": {
     "logged": 1680194274974
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ec298bb-7639-4c4d-983b-75ed45c38643",
   "metadata": {
    "gather": {
     "logged": 1680031548880
    }
   },
   "outputs": [],
   "source": [
    "input_file = os.getcwd() + \"/data/gtbrg_2m_lines.txt\"\n",
    "max_sentence_length = 5000\n",
    "vocab_size = 19099\n",
    "model_type = \"unigram\"\n",
    "SP_MODEL_NAME = f\"models/{model_type}_{vocab_size}_v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd64cda8-e4db-4816-934a-3bd87dcf0f5e",
   "metadata": {
    "gather": {
     "logged": 1680031587088
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/home/studio-lab-user/sagemaker-studiolab-notebooks/thesis/audio-semantics/data/gtbrg_500k_lines.txt --model_type=unigram --model_prefix=models/unigram_19099_v3 --vocab_size=19099 --max_sentence_length=5000 --train_extremely_large_corpus\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /home/studio-lab-user/sagemaker-studiolab-notebooks/thesis/audio-semantics/data/gtbrg_500k_lines.txt\n",
      "  input_format: \n",
      "  model_prefix: models/unigram_19099_v3\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 19099\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 5000\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 1\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /home/studio-lab-user/sagemaker-studiolab-notebooks/thesis/audio-semantics/data/gtbrg_500k_lines.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 500001 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=17562371\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=27\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 500001 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 1000000 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 500001\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 427190\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 427190 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=399418 obj=62.3764 num_tokens=2397134 num_tokens/piece=6.00157\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=231127 obj=55.1538 num_tokens=2443609 num_tokens/piece=10.5726\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=172019 obj=54.1508 num_tokens=2491345 num_tokens/piece=14.483\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=166997 obj=53.8762 num_tokens=2508072 num_tokens/piece=15.0187\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=125081 obj=54.0878 num_tokens=2570981 num_tokens/piece=20.5545\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=124936 obj=53.9138 num_tokens=2583462 num_tokens/piece=20.6783\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=93692 obj=54.3949 num_tokens=2677607 num_tokens/piece=28.5788\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=93681 obj=54.1331 num_tokens=2690281 num_tokens/piece=28.7175\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=70258 obj=54.8686 num_tokens=2813138 num_tokens/piece=40.0401\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=70255 obj=54.5197 num_tokens=2839501 num_tokens/piece=40.4171\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=52690 obj=55.4608 num_tokens=2973405 num_tokens/piece=56.4321\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=52690 obj=55.0611 num_tokens=2989904 num_tokens/piece=56.7452\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=39517 obj=56.1332 num_tokens=3123416 num_tokens/piece=79.0398\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=39516 obj=55.7492 num_tokens=3133145 num_tokens/piece=79.288\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=29636 obj=56.9732 num_tokens=3273138 num_tokens/piece=110.445\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=29636 obj=56.5816 num_tokens=3279065 num_tokens/piece=110.645\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=22227 obj=57.9507 num_tokens=3426979 num_tokens/piece=154.181\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=22227 obj=57.5435 num_tokens=3429808 num_tokens/piece=154.308\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=21008 obj=57.8036 num_tokens=3462936 num_tokens/piece=164.839\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=21008 obj=57.7185 num_tokens=3464367 num_tokens/piece=164.907\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: models/unigram_19099_v3.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: models/unigram_19099_v3.vocab\n"
     ]
    }
   ],
   "source": [
    "# train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`\n",
    "# `m.vocab` is just a reference. not used in the segmentation.\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={input_file} \" \\\n",
    "    f\"--model_type={model_type} \" \\\n",
    "    f\"--model_prefix={SP_MODEL_NAME} \" \\\n",
    "    f\"--vocab_size={vocab_size} \" \\\n",
    "    f\"--max_sentence_length={max_sentence_length} \" \\\n",
    "    f\"--train_extremely_large_corpus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef756792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/gutenberg_no_spaces.txt\") as fp:\n",
    "#     print(fp.readlines()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5458ab-230c-4494-b81d-1a03f0e974e1",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680194771086
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# SP_MODEL_NAME = \"models/unigram_8k_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c205055-8cb3-4391-b73d-acfaa9c1ef53",
   "metadata": {
    "gather": {
     "logged": 1680194772113
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'apple']\n",
      "[3, 18804]\n",
      "[3, 767]\n",
      "[3, 58]\n"
     ]
    }
   ],
   "source": [
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f\"{SP_MODEL_NAME}.model\")\n",
    "\n",
    "# encode: text => id\n",
    "print(sp.EncodeAsPieces('apple'))\n",
    "print(sp.encode_as_ids('boyhood'))\n",
    "print(sp.encode_as_ids('boy'))\n",
    "print(sp.encode_as_ids('man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72c9453-50a4-4e22-8ebd-7020fefe4609",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Loading the vocabulary created by SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10091f5e-499d-4301-a19f-3ba88bf86e86",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680194951102
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/gutenberg_no_spaces.txt\") as corpus_file:\n",
    "    corpus = corpus_file.readlines()\n",
    "\n",
    "sentences = [sp.EncodeAsPieces(sentence) for sentence in corpus]\n",
    "# sentences = [' '.join(sentence) for sentence in corpus]\n",
    "# sentences = [list(sentence) for sentence in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab30628-7d4a-48ec-9e38-5027e7a06a59",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Approach 1.1 - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de00c3bf-0be8-4256-9ada-b25b580268e5",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680004547121
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "W2V_MODEL_PATH = \"models/w2v_100_v2.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdaf5159-36c3-4638-bb3e-8deb8d8ea6d4",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680005530991
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, window=5, min_count=0, workers=4)\n",
    "# model.build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cba718e9-7e56-4962-8f55-2896b45c274a",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680004691811
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model.save(W2V_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0f2a252-1d31-477d-8f7d-85e9e582dce5",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680004328602
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# model = Word2Vec.load(W2V_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a2b160f-d95f-4652-a5f9-25446e422ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁theproject', 'gutenbergebookof', 'darkhollow', 'by', 'anna', 'katherine', 'green', 'thisebook', 'is', 'fortheuseof']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27ae9872-4a70-4e37-9612-66fdf73dace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"cat\"\n",
    "\n",
    "def find_most_similar(model, string):\n",
    "    pieces = [piece for piece in sp.EncodeAsPieces(string)[1:]]\n",
    "    print(pieces)\n",
    "    return np.mean([model.wv.most_similar(piece) for piece in sp.EncodeAsPieces(string)[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3817dab-eb26-49ed-a3e8-75ff44975f0a",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680012211655
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('sof', 0.7251863479614258),\n",
       "  ('ofgreat', 0.7209269404411316),\n",
       "  ('ofhuman', 0.7016451358795166),\n",
       "  ('ofsuch', 0.6895963549613953),\n",
       "  ('ofthegreat', 0.6712996959686279),\n",
       "  ('ofmany', 0.6693395972251892),\n",
       "  ('ofthe', 0.6684151291847229),\n",
       "  ('oftwo', 0.6615369915962219),\n",
       "  ('ofits', 0.6407619714736938),\n",
       "  ('ofthefirst', 0.6180253028869629)]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model.wv.most_similar(piece) for piece in sp.EncodeAsPieces(\"of\")[1:]]\n",
    "# model.wv.similarity(\"cat\", \"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9afd01b1-b0bf-4f85-a0e8-c5bde40ea874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['has']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[piece for piece in sp.EncodeAsPieces(\"has\")[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad6866-adfc-4bef-a920-43f40bf70441",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": []
   },
   "source": [
    "## Approach 2 - WordPiece tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7b81a1-8d47-41ae-a07b-eab52ff02f6b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import PreTokenizer\n",
    "\n",
    "class FixedLengthPreTokenizer(PreTokenizer):\n",
    "    def __init__(self, n=3):\n",
    "        self.n = n\n",
    "        super().__init__()\n",
    "\n",
    "    def pre_tokenize(self, text):\n",
    "        return [(i, i+self.n) for i in range(0, len(text), self.n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696fe546-61db-4ddd-96d8-bb63c9954d5f",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1679686719830
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import trainers\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(WordPiece())\n",
    "\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=10000, \n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    initial_alphabet= list(\n",
    "        \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    ),\n",
    "    continuing_subword_prefix=\"##\",\n",
    "    show_progress=True,\n",
    "    min_frequency=1\n",
    ")\n",
    "\n",
    "# tokenizer.pre_tokenizer = FixedLengthPreTokenizer()  # or another value for n\n",
    "\n",
    "tokenizer.train(\n",
    "    files=[\"data/gutenberg_no_spaces.txt\"], \n",
    "    trainer=trainer\n",
    ")\n",
    "\n",
    "tokenizer.save(\"models/tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b206bacb-ebb0-46d5-a865-b1e4d8ebc799",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Approach 3 - Character CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29babca4-750a-41b2-b69e-38535364ebc8",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680029630371
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b89c4832-4fc5-486d-aa02-c75f2acd20d0",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680029630479
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3810f35b-5b5a-4a13-9549-4dbdb4b6111b",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680030688818
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'á', 'â', 'ã', 'ä', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ñ', 'ò', 'ó', 'ô', 'ö', 'ù', 'û', 'ü', 'ā', 'œ', 'α', 'β', 'δ', 'ε', 'η', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ο', 'π', 'ρ', 'ς', 'σ', 'τ', 'υ', 'φ', 'ω']\n"
     ]
    }
   ],
   "source": [
    "# To prevent recomputing alphabet each time\n",
    "vocab_path = 'data/gutenberg_vocabulary.txt'\n",
    "\n",
    "vocabulary = sorted(set(open(vocab_path).read().split()))\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45a9e8f0-a90f-4059-8ae2-0a7aa2fdaf9b",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680030689866
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, corpus_path, seq_length):\n",
    "        self.corpus_path = corpus_path\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab = vocabulary # Load the unique characters in the corpus\n",
    "        self.char_to_index = {c: i for i, c in enumerate(self.vocab)} # Map each character to an index\n",
    "        self.index_to_char = {i: c for i, c in enumerate(self.vocab)} # Map each index to a character\n",
    "        self.corpus_size = os.path.getsize(corpus_path)\n",
    "        # self.num_chunks = int(self.corpus_size / (1024 * 1024)) # Split the corpus into 1MB chunks\n",
    "        # self.chunk_size = int(self.corpus_size / self.num_chunks)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_chunks\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_pos = idx * self.chunk_size\n",
    "        # end_pos = (idx + 1) * self.chunk_size\n",
    "        with open(self.corpus_path) as f:\n",
    "            f.seek(start_pos)\n",
    "            chunk = f.read(self.chunk_size).replace('\\n', '')\n",
    "        input_seq = chunk[:-1]\n",
    "        target = chunk[1:]\n",
    "        input_seq = [self.char_to_index[c] for c in input_seq]\n",
    "        target = [self.char_to_index[c] for c in target]\n",
    "        input_seq = torch.LongTensor(input_seq)\n",
    "        target = torch.LongTensor(target)\n",
    "        return input_seq, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2ebfd2-4549-4926-bcb8-67994ab9ebc4",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680029633154
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "class CharacterCNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, filter_sizes, num_filters):\n",
    "        super(CharacterCNN, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_size, out_channels=num_filters, kernel_size=fs, padding=0)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, input_size)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # input shape: (seq_len, batch_size)\n",
    "        embedded = self.embedding(input) # shape: (seq_len, batch_size, embedding_size)\n",
    "        embedded = embedded.permute(1, 2, 0) # shape: (batch_size, embedding_size, seq_len)\n",
    "        conv_outputs = []\n",
    "        for conv in self.conv_layers:\n",
    "            conv_outputs.append(torch.relu(conv(embedded)))\n",
    "        pooled_outputs = [torch.max(conv_output, dim=-1)[0] for conv_output in conv_outputs]\n",
    "        fc_input = torch.cat(pooled_outputs, dim=-1)\n",
    "        output = self.fc(fc_input)\n",
    "        # print(output.shape)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56d18853-0fff-494d-912e-f331522f1bc1",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680029633849
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "corpus_path = 'data/gutenberg.txt'\n",
    "seq_length = 20\n",
    "vocab_size = len(vocabulary)\n",
    "embedding_size = 128\n",
    "# output_size = 100 # size of the word embeddings\n",
    "filter_sizes = [3, 4] # filter sizes for convolutional layers\n",
    "num_filters = 64 # number of filters for each convolutional layer\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "650455bf-809a-493a-ad42-ced2485acef0",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680029634784
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset = CharDataset(corpus_path, seq_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7574c23-ee63-4030-99aa-2b2c62d7bc97",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680029637461
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create model and optimizer\n",
    "model = CharacterCNN(vocab_size, embedding_size, filter_sizes, num_filters)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c447f1c1-ec3d-48b4-a4ce-a6759a5c7a52",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1680029720070
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1048739]) torch.Size([4, 1048739])\n",
      "torch.Size([4, 1048739])\n",
      "torch.Size([1048739, 71])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1048739) to match target batch_size (4).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m output \u001b[38;5;241m=\u001b[39m model(input_seq)\n\u001b[0;32m---> 12\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1048739) to match target batch_size (4)."
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (input_seq, target) in enumerate(dataloader):\n",
    "        # print(sys.getsizeof(input_seq), sys.getsizeof(target), input_seq.shape)\n",
    "        print(input_seq.shape, target.shape)\n",
    "        input_seq = input_seq.to(device)\n",
    "        print(target.shape)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_seq)\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Batch {i + 1}/{len(dataloader)}, Loss: {total_loss / (i + 1)}')\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd755e2-773d-4165-8054-e27349f7caa7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
