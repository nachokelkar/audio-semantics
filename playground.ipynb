{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuBERT test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, HubertModel\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/hubert-large-ls960-ft\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = HubertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"C:\\\\Users\\\\mj115gl\\\\work_dir\\\\thesis\\\\audio-semantics\\\\data\\\\LibriSpeech\\\\dev-clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = processor(ds[\"train\"][2][\"audio\"][\"array\"], return_tensors=\"pt\").input_values  # Batch size 1\n",
    "logits = model(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.decode(predicted_ids[0])\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train\"][1][\"audio\"][\"array\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.last_hidden_state.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec, LineSentence\n",
    "# from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_fp = open(\"data/gtbrg_i.txt\", \"r\", encoding=\"utf-16\")\n",
    "sentences = LineSentence(line_fp)\n",
    "# line_fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_fp.seek(0)\n",
    "line_fp.readline()\n",
    "# line_fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 100\n",
    "window = 5\n",
    "w2v_model_tag = \"TEST\"\n",
    "W2V_MODEL_PATH = f\"models/w2v_vs{vector_size}_w{window}_{w2v_model_tag}.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    sentences,\n",
    "    window=window,\n",
    "    vector_size=vector_size,\n",
    "    min_count=0,\n",
    "    workers=4,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(W2V_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(w2v_model.wv.key_to_index.keys())[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsim_scores = []\n",
    "\n",
    "with open(\"data/wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\") as wordsim_fp:\n",
    "    for line in wordsim_fp.readlines():\n",
    "        scores = line.split(\"\\t\")\n",
    "        w1, w2 = scores[0], scores[1]\n",
    "        gold_score = float(scores[2])\n",
    "        wordsim_scores.append([w1, w2, gold_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_vocab = []\n",
    "gold_all = []\n",
    "preds_vocab = []\n",
    "preds_all = []\n",
    "tested = 0\n",
    "oov = 0\n",
    "\n",
    "for pairs in wordsim_scores:\n",
    "    w1, w2 = pairs[0].lower(), pairs[1].lower()\n",
    "    \n",
    "    try:\n",
    "        pred = w2v_model.wv.similarity(w1, w2)\n",
    "        preds_vocab.append(pred)\n",
    "        gold_vocab.append(pairs[2])\n",
    "        tested += 1\n",
    "    \n",
    "    except KeyError:\n",
    "        # if w1 not in w2v_model.wv.vocab.keys():\n",
    "        #     w1_units = sp.EncodeAsPieces(w1)[1:]\n",
    "        #     w1_vectors = np.array([w2v_model.wv[unit] for unit in w1_units])\n",
    "        #     w1_vector = w1_vectors.mean(axis=0)\n",
    "        # else:\n",
    "        #     w1_vector = w2v_model.wv[w1]\n",
    "        # if w2 not in w2v_model.wv.vocab.keys():\n",
    "        #     w2_units = sp.EncodeAsPieces(w2)[1:]\n",
    "        #     w2_vectors = np.array([w2v_model.wv[unit] for unit in w2_units])\n",
    "        #     w2_vector = w2_vectors.mean(axis=0)\n",
    "        # else:\n",
    "        #     w2_vector = w2v_model.wv[w2]\n",
    "\n",
    "        # pred = cosine_similarity(w1_vector.reshape(1, -1), w2_vector.reshape(1, -1))\n",
    "        oov += 1\n",
    "    \n",
    "    preds_all.append(pred)\n",
    "    gold_all.append(pairs[2])\n",
    "        \n",
    "\n",
    "print(spearmanr(preds_vocab, gold_vocab)[0], f\", tested {tested}/{len(wordsim_scores)} pairs\")\n",
    "print(spearmanr(preds_all, gold_all)[0], f\", including OOV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simlex_scores = []\n",
    "\n",
    "with open(\"data/SimLex-999/SimLex-999.txt\") as simlex_fp:\n",
    "    for line in simlex_fp.readlines()[1:]:\n",
    "        scores = line.split(\"\\t\")\n",
    "        w1, w2 = scores[0], scores[1]\n",
    "        gold_score = float(scores[3])\n",
    "        simlex_scores.append([w1, w2, gold_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_vocab = []\n",
    "gold_all = []\n",
    "preds_vocab = []\n",
    "preds_all = []\n",
    "tested = 0\n",
    "oov = 0\n",
    "\n",
    "for pairs in simlex_scores:\n",
    "    w1, w2 = pairs[0].lower(), pairs[1].lower()\n",
    "    \n",
    "    try:\n",
    "        pred = w2v_model.wv.similarity(w1, w2)\n",
    "        preds_vocab.append(pred)\n",
    "        gold_vocab.append(pairs[2])\n",
    "        tested += 1\n",
    "    \n",
    "    except KeyError:\n",
    "        # if w1 not in w2v_model.wv.vocab.keys():\n",
    "        #     w1_units = sp.EncodeAsPieces(w1)[1:]\n",
    "        #     w1_vectors = np.array([w2v_model.wv[unit] for unit in w1_units])\n",
    "        #     w1_vector = w1_vectors.mean(axis=0)\n",
    "        # else:\n",
    "        #     w1_vector = w2v_model.wv[w1]\n",
    "        # if w2 not in w2v_model.wv.vocab.keys():\n",
    "        #     w2_units = sp.EncodeAsPieces(w2)[1:]\n",
    "        #     w2_vectors = np.array([w2v_model.wv[unit] for unit in w2_units])\n",
    "        #     w2_vector = w2_vectors.mean(axis=0)\n",
    "        # else:\n",
    "        #     w2_vector = w2v_model.wv[w2]\n",
    "\n",
    "        # pred = cosine_similarity(w1_vector.reshape(1, -1), w2_vector.reshape(1, -1))\n",
    "        oov += 1\n",
    "    \n",
    "    preds_all.append(pred)\n",
    "    gold_all.append(pairs[2])\n",
    "        \n",
    "\n",
    "print(spearmanr(preds_vocab, gold_vocab)[0], f\", tested {tested}/{len(simlex_scores)} pairs\")\n",
    "print(spearmanr(preds_all, gold_all)[0], f\", including OOV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = []\n",
    "words = list(w2v_model.wv.key_to_index.keys())\n",
    "\n",
    "tqdm_iterator = tqdm(range(0, 99, 1))\n",
    "\n",
    "for threshold in tqdm_iterator:\n",
    "    threshold = threshold / 100\n",
    "    word_to_cluster = dict()  # Stores map from word to cluster\n",
    "    cluster_to_words = dict()  # Stores map from cluster to words\n",
    "    cluster_idx = 0  # Counter\n",
    "\n",
    "    for word in words:\n",
    "        tqdm_iterator.set_postfix({\"Word\": word})\n",
    "        # Check if word has already been clustered\n",
    "        if word not in word_to_cluster.keys():\n",
    "            # Create new cluster\n",
    "            cluster_idx += 1\n",
    "            # cluster_key = chr(0x0020 + cluster_idx)\n",
    "            cluster_key = cluster_idx\n",
    "\n",
    "            # Add new word to cluster\n",
    "            tqdm_iterator.set_postfix({\"status\" : \"adding new word to cluster\"})\n",
    "            cluster_to_words[cluster_key] = [word]\n",
    "            word_to_cluster[word] = cluster_key\n",
    "            \n",
    "            # Add all similar words\n",
    "            tqdm_iterator.set_postfix({\"status\" : \"adding all words\"})\n",
    "            for similar_word, score in w2v_model.wv.most_similar(word, topn=50):\n",
    "                tqdm_iterator.set_postfix({\"status\" : \"checked 50 words\"})\n",
    "                if score > threshold:\n",
    "                    cluster_to_words[cluster_key].append(similar_word)\n",
    "                    word_to_cluster[similar_word] = cluster_key\n",
    "    \n",
    "    n_clusters.append(len(cluster_to_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = {\n",
    "    i: key for i, key in enumerate(ascii_letters)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_key = {}\n",
    "key_to_word = {}\n",
    "\n",
    "with open(\"data/quantized/dev-gold.csv\", \"r\") as key_file:\n",
    "    for line in key_file.readlines()[1:]:\n",
    "        dataset, key, _, word = line.strip().split(\",\")\n",
    "        if word not in word_to_key:\n",
    "            word_to_key[word] = {\n",
    "                'librispeech': [],\n",
    "                'synthetic': []\n",
    "            }\n",
    "        word_to_key[word][dataset].append(key)\n",
    "        key_to_word[key] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = {}\n",
    "\n",
    "for dataset in [\"librispeech\", \"synthetic\"]:\n",
    "    with open(f\"data/quantized/semantic/dev/{dataset}/quantized_outputs.txt\", \"r\") as utterance_file:\n",
    "        for line in utterance_file.readlines():\n",
    "            key, seq = line.strip().split(\"\\t\")\n",
    "            utterance = seq.split(\",\")[1:]\n",
    "\n",
    "            key = \"ls_\" + key_to_word[key] if dataset == \"librispeech\" else \"sy_\" + key_to_word[key]\n",
    "            if key not in utterances:\n",
    "                utterances[key] = []\n",
    "\n",
    "            utterances[key].append(\n",
    "                \"\".join(\n",
    "                    [letters[int(v)] for i, v in enumerate(utterance)]\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/level_wise/level0/utterances_original.txt\", \"w+\", encoding=\"utf-8\") as ufp:\n",
    "    for word in utterances:\n",
    "        for utterance in utterances[word]:\n",
    "            ufp.write(word + \"\\t\" + utterance + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_pairs = []\n",
    "rel_pairs = []\n",
    "\n",
    "with open(\"data/quantized/dev-pairs.csv\", \"r\") as pairs_file:\n",
    "    for line in pairs_file.readlines()[1:]:\n",
    "        dataset, _, w1, w2, sim, rel = line.strip().split(\",\")\n",
    "        if sim:\n",
    "            sim_pairs.append((dataset, w1, w2, float(sim)))\n",
    "        if rel:\n",
    "            rel_pairs.append((dataset, w1, w2, float(rel)))\n",
    "\n",
    "with open(\"data/level_wise/level0/pairs.txt\", \"w+\", encoding=\"utf-8\") as pairs_fp:\n",
    "    for pair in sim_pairs:\n",
    "        dataset, w1, w2, score = pair\n",
    "        pairs_fp.write(\n",
    "            (\"ls_\" + w1 if dataset == \"librispeech\" else \"sy_\" + w1) + \",\" +\n",
    "            (\"ls_\" + w2 if dataset == \"librispeech\" else \"sy_\" + w2) + \",\" +\n",
    "            str(score) + \",\" + \"\\n\"\n",
    "        )\n",
    "    for pair in rel_pairs:\n",
    "        dataset, w1, w2, score = pair\n",
    "        pairs_fp.write(\n",
    "            (\"ls_\" + w1 if dataset == \"librispeech\" else \"sy_\" + w1) + \",\" +\n",
    "            (\"ls_\" + w2 if dataset == \"librispeech\" else \"sy_\" + w2) + \",\" +\n",
    "            \",\" + str(score) + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_model = spm.SentencePieceProcessor()\n",
    "\n",
    "sp_model.Load(\"models/original_60k_250x1/level1/unigram_vs60000_lw.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_corpus = []\n",
    "\n",
    "with open(\"data/level_wise/level0/dev_corpus_original.txt\", \"r\", encoding=\"utf-8\") as ocfp:\n",
    "    for line in ocfp.readlines():\n",
    "        original_corpus.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/original_60k_250x1/level1/dev_corpus_ft.txt\", \"w+\", encoding=\"utf-8\") as ncfp:\n",
    "    for line in original_corpus:\n",
    "        pieces = list(\n",
    "            filter(\n",
    "                lambda x: x != \"▁\",\n",
    "                sp_model.EncodeAsPieces(line)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "        \n",
    "        ncfp.write(\" \".join(units) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = fasttext.train_unsupervised(\n",
    "    \"data/original_60k_250x1/level1/corpus_ft.txt\",\n",
    "    \"cbow\",\n",
    "    dim=250,\n",
    "    thread=4,\n",
    "    epoch=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.save_model(\"models/fasttext_cbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_model = fasttext.load_model(\"models/fasttext_cbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from levelwise_model.test_bench import LSTestBench\n",
    "from levelwise_model.utterances import WordToUtteranceMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_mapping = WordToUtteranceMapping(map_file=\"data/level_wise/level0/utterances_original.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bench = LSTestBench(scores_file=\"data/level_wise/level0/pairs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bench.ft_score_and_save(ft_model=ft_model, utterances=utterance_mapping, results_file=\"results/ft_cbow\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\mj115gl\\work_dir\\thesis\\audio-semantics\\models\\db_final. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\mj115gl\\work_dir\\thesis\\audio-semantics\\models\\db_final were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('C:\\\\Users\\\\mj115gl\\\\work_dir\\\\thesis\\\\audio-semantics\\\\models\\\\db_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.Load(\"models/original_60k_250x1/level1/unigram_vs60000_lw.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from levelwise_model.utterances import WordToUtteranceMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = WordToUtteranceMapping(utterances=\"data/level_wise/level0/utterances_original.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_utterances = {}\n",
    "\n",
    "for word in utterances.utterances:\n",
    "    updated_utterances[word] = []\n",
    "    for utt in utterances.utterances[word]:\n",
    "        pieces = list(\n",
    "            filter(\n",
    "                lambda x: x != \"▁\",\n",
    "                sp_model.EncodeAsPieces(utt)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "        updated_utterances[word].append(\n",
    "            \" \".join(units)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_utterances = WordToUtteranceMapping(utterances=updated_utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from levelwise_model.test_bench import LSTestBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bench = LSTestBench(\"data/level_wise/level0/pairs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_word_vec_fn(utt):\n",
    "    pieces = list(\n",
    "        filter(\n",
    "            lambda x: x != \"▁\",\n",
    "            sp_model.EncodeAsPieces(utt)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "    return model.encode(\" \".join(units)).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'similarities': {'score': {'librispeech': 8.253180987037307,\n",
       "   'synthetic': -0.004998110381584656},\n",
       "  'errors': 0,\n",
       "  'trials': 1013},\n",
       " 'xw-abx-test': {'ABX Result': {'librispeech': 0.581140350877193,\n",
       "   'synthetic': 0.49278846153846156,\n",
       "   'combined': 0.5240683229813664}},\n",
       " 'sw-abx-test': {'Same-word ABX Result': {'librispeech': 0.66,\n",
       "   'synthetic': 0.71,\n",
       "   'mixed': 0.6457142857142857}}}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bench.run_suite(\n",
    "    utterances=updated_utterances,\n",
    "    word_vec_fn=db_word_vec_fn,\n",
    "    results_file=\"results/db-50.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test positive\n",
    "# ls_sims_pos = []\n",
    "# sy_sims_pos = []\n",
    "# tested = 0\n",
    "\n",
    "# for word in updated_utterances:\n",
    "#     if len(updated_utterances[word]) > 1:\n",
    "#         tested += 1\n",
    "#         utterances_list = [\n",
    "#             \" \".join(\n",
    "#                 [\n",
    "#                     piece.replace(\"▁\", \"\")\n",
    "#                     for piece in list(\n",
    "#                         filter(\n",
    "#                             lambda x: x != \"▁\",\n",
    "#                             sp_model.EncodeAsPieces(utterance)\n",
    "#                         )\n",
    "#                     )\n",
    "#                 ]\n",
    "#             ) for utterance in updated_utterances[word]\n",
    "#         ]\n",
    "\n",
    "#         embeddings = model.encode(utterances_list)\n",
    "#         similarities = cosine_similarity(embeddings)\n",
    "\n",
    "#         if word.startswith(\"ls_\"):\n",
    "#             ls_sims_pos.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "#         if word.startswith(\"sy_\"):\n",
    "#             sy_sims_pos.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "\n",
    "# print(f\"Avg positive score (LS) = {sum(ls_sims_pos)/len(ls_sims_pos)} ({len(ls_sims_pos)} words).\")\n",
    "# print(f\"Avg positive score (SY) = {sum(sy_sims_pos)/len(sy_sims_pos)} ({len(sy_sims_pos)} words).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_utts = []\n",
    "\n",
    "# for word in updated_utterances:\n",
    "#     n_utts.append(len(updated_utterances[word]))\n",
    "\n",
    "# print(sum(n_utts)/len(n_utts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test negative\n",
    "# ls_sims_neg = []\n",
    "# sy_sims_neg = []\n",
    "# tested = 0\n",
    "# n_negative_samples = 5\n",
    "\n",
    "# ls_word_list = set([word for word in updated_utterances.keys() if word.startswith(\"ls_\")])\n",
    "# sy_word_list = set([word for word in updated_utterances.keys() if word.startswith(\"sy_\")])\n",
    "\n",
    "# for word in updated_utterances:\n",
    "#     tested += 1\n",
    "\n",
    "#     negative_samples = []\n",
    "#     if word.startswith(\"ls_\"):\n",
    "#         use_list = ls_word_list\n",
    "#     else:\n",
    "#         use_list = sy_word_list\n",
    "#     sample = np.random.choice(list(use_list - {word}), size=(n_negative_samples))\n",
    "#     for s in sample:\n",
    "#         negative_samples.append(\n",
    "#             np.random.choice(updated_utterances[s])\n",
    "#         )\n",
    "\n",
    "#     utterances_list = [\n",
    "#         \" \".join(\n",
    "#             [\n",
    "#                 piece.replace(\"▁\", \"\")\n",
    "#                 for piece in list(\n",
    "#                     filter(\n",
    "#                         lambda x: x != \"▁\",\n",
    "#                         sp_model.EncodeAsPieces(utterance)\n",
    "#                     )\n",
    "#                 )\n",
    "#             ]\n",
    "#         ) for utterance in negative_samples\n",
    "#     ]\n",
    "    \n",
    "#     embeddings = model.encode(utterances_list)\n",
    "#     similarities = cosine_similarity(embeddings)\n",
    "\n",
    "#     if word.startswith(\"ls_\"):\n",
    "#         ls_sims_neg.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "#     if word.startswith(\"sy_\"):\n",
    "#         sy_sims_neg.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "\n",
    "# print(f\"Avg negative score (LS) = {sum(ls_sims_neg)/len(ls_sims_neg)} ({len(ls_sims_neg)} words).\")\n",
    "# print(f\"Avg negative score (SY) = {sum(sy_sims_neg)/len(sy_sims_neg)} ({len(sy_sims_neg)} words).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(sy_sims)/len(sy_sims)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABX Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(\"models/original_60k_250x1/level1/w2v_vs250_w1_lw.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wv_word_vec_fn(utt):\n",
    "    if utt in w2v_model.wv.key_to_index.keys():\n",
    "        return w2v_model.wv[utt].reshape(1, -1)\n",
    "    else:\n",
    "        pieces = list(\n",
    "            filter(\n",
    "                lambda x: x != \"▁\",\n",
    "                sp_model.EncodeAsPieces(utt)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "\n",
    "        vectors = np.array([w2v_model.wv[unit] for unit in units])\n",
    "        return vectors.mean(axis=0).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'similarities': {'score': {'librispeech': 9.01030729942617,\n",
       "   'synthetic': 1.028987880071217},\n",
       "  'errors': 0,\n",
       "  'trials': 1013},\n",
       " 'xw-abx-test': {'ABX Result': {'librispeech': 0.49780701754385964,\n",
       "   'synthetic': 0.46033653846153844,\n",
       "   'combined': 0.47360248447204967}},\n",
       " 'sw-abx-test': {'Same-word ABX Result': {'librispeech': 0.9057142857142857,\n",
       "   'synthetic': 0.9571428571428572,\n",
       "   'mixed': 0.8442857142857143}}}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bench.run_suite(\n",
    "    utterances=utterances,\n",
    "    word_vec_fn=wv_word_vec_fn,\n",
    "    results_file=\"results/w2v.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = 0\n",
    "# total = 0\n",
    "# use_noise_for_x = True\n",
    "\n",
    "# def get_correct_word(pair, word):\n",
    "#     if pair[0] == word:\n",
    "#         return pair[1]\n",
    "#     return pair[1]\n",
    "\n",
    "# for word_a in updated_utterances:\n",
    "#     # Sort words by similarity\n",
    "#     similar_words = sorted(\n",
    "#         # Get only pairs containing word A\n",
    "#         filter(\n",
    "#             lambda x: x[0] == word_a or x[1] == word_a,\n",
    "#             rel_pairs\n",
    "#         ),\n",
    "#         key=lambda x: x[2],\n",
    "#         reverse=True\n",
    "#     )\n",
    "\n",
    "#     similar_words = list(\n",
    "#         map(\n",
    "#             lambda x: get_correct_word(x, word_a),\n",
    "#             similar_words\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     for utt_a in updated_utterances[word_a]:\n",
    "#         word_b = similar_words[0]\n",
    "#         utt_b = np.random.choice(\n",
    "#             updated_utterances[word_b]\n",
    "#         )\n",
    "\n",
    "#         if not use_noise_for_x:\n",
    "#             word_x = similar_words[-1]\n",
    "#             utt_x = np.random.choice(\n",
    "#                 updated_utterances[word_x]\n",
    "#             )\n",
    "\n",
    "#         else:\n",
    "#             utt_x = np.random.choice(\n",
    "#                 list(ascii_letters),\n",
    "#                 len(utt_a),\n",
    "#                 replace=True\n",
    "#             )\n",
    "#             utt_x = \"\".join(utt_x)\n",
    "\n",
    "#         v_a = word_vec_fn(utt_a).reshape(1, -1)\n",
    "#         v_b = word_vec_fn(utt_b).reshape(1, -1)\n",
    "#         v_x = word_vec_fn(utt_x).reshape(1, -1)\n",
    "\n",
    "#         if cosine_similarity(v_a, v_b) > cosine_similarity(v_a, v_x):\n",
    "#             preds += 1\n",
    "#         total += 1\n",
    "\n",
    "# print({\"ABX Result\": preds/total})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ABX Result': 0.5062111801242236}\n"
     ]
    }
   ],
   "source": [
    "# preds = 0\n",
    "# total = 0\n",
    "# use_noise_for_x = False\n",
    "\n",
    "# def get_correct_word(pair, word):\n",
    "#     if pair[0] == word:\n",
    "#         return pair[1]\n",
    "#     return pair[1]\n",
    "\n",
    "# with open(\"results/abx_test_db.txt\", \"w+\") as abx_results_file:\n",
    "#     abx_results_file.write(\"A,B,X,sim(AB),sim(AX),chosen\\n\")\n",
    "#     for word_a in updated_utterances:\n",
    "#         # Sort words by similarity\n",
    "#         similar_words = sorted(\n",
    "#             # Get only pairs containing word A\n",
    "#             filter(\n",
    "#                 lambda x: x[0] == word_a or x[1] == word_a,\n",
    "#                 rel_pairs\n",
    "#             ),\n",
    "#             key=lambda x: x[2],\n",
    "#             reverse=True\n",
    "#         )\n",
    "\n",
    "#         similar_words = list(\n",
    "#             map(\n",
    "#                 lambda x: get_correct_word(x, word_a),\n",
    "#                 similar_words\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#         for utt_a in updated_utterances[word_a]:\n",
    "#             word_b = similar_words[0]\n",
    "#             utt_b = np.random.choice(\n",
    "#                 updated_utterances[word_b]\n",
    "#             )\n",
    "\n",
    "#             if not use_noise_for_x:\n",
    "#                 word_x = similar_words[-1]\n",
    "#                 utt_x = np.random.choice(\n",
    "#                     updated_utterances[word_x]\n",
    "#                 )\n",
    "\n",
    "#             else:\n",
    "#                 word_x = \"noise\"\n",
    "#                 utt_x = np.random.choice(\n",
    "#                     list(ascii_letters),\n",
    "#                     len(utt_a),\n",
    "#                     replace=True\n",
    "#                 )\n",
    "#                 utt_x = \"\".join(utt_x)\n",
    "\n",
    "#             if len(set([word_a, word_b, word_x])) == 3:\n",
    "#                 v_a = word_vec_fn(utt_a).reshape(1, -1)\n",
    "#                 v_b = word_vec_fn(utt_b).reshape(1, -1)\n",
    "#                 v_x = word_vec_fn(utt_x).reshape(1, -1)\n",
    "\n",
    "#                 sim_ab = cosine_similarity(v_a, v_b)\n",
    "#                 sim_ax = cosine_similarity(v_a, v_x)\n",
    "\n",
    "#                 chosen = \"X\"\n",
    "#                 if sim_ab > sim_ax:\n",
    "#                     chosen = \"B\"\n",
    "#                     preds += 1\n",
    "#                 total += 1\n",
    "\n",
    "#                 abx_results_file.write(\",\".join([word_a, word_b, word_x, str(sim_ab), str(sim_ax), chosen]) + \"\\n\")\n",
    "\n",
    "# print({\"ABX Result\": preds/total})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(set([word_a, word_b, word_x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_pairs = []\n",
    "# rel_pairs = []\n",
    "\n",
    "# with open(\"data/level_wise/level0/pairs.txt\", \"r\") as pairs_file:\n",
    "#     for line in pairs_file.readlines()[1:]:\n",
    "#         w1, w2, sim, rel = line.strip().split(\",\")\n",
    "#         if sim:\n",
    "#             sim_pairs.append((w1, w2, float(sim)))\n",
    "#         if rel:\n",
    "#             rel_pairs.append((w1, w2, float(rel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse vector baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.Load(\"models/original_60k_250x1/level1/unigram_vs60000_lw.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sv_word_vec_fn(word):\n",
    "    vector = np.zeros(60000)\n",
    "    tokens = sp_model.EncodeAsIds(word)\n",
    "    vector[tokens] = 1\n",
    "    return vector.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from levelwise_model.test_bench import LSTestBench\n",
    "from levelwise_model.utterances import WordToUtteranceMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bench = LSTestBench(\"data/base_data/pairs.txt\")\n",
    "utterances = WordToUtteranceMapping(utterances=\"data/base_data/utterances_original.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'similarities': {'score': {'librispeech': -6.103752173089786,\n",
       "   'synthetic': 0.2691688253264706},\n",
       "  'errors': 0,\n",
       "  'trials': 1013},\n",
       " 'xw-abx-test': {'ABX Result': {'librispeech': 0.40131578947368424,\n",
       "   'synthetic': 0.18389423076923078,\n",
       "   'combined': 0.2608695652173913}},\n",
       " 'sw-abx-test': {'Same-word ABX Result': {'librispeech': 0.564,\n",
       "   'synthetic': 0.454,\n",
       "   'mixed': 0.448}}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bench.run_suite(\n",
    "    utterances=utterances,\n",
    "    word_vec_fn=sv_word_vec_fn,\n",
    "    results_file=\"results/baseline_sparse_vectors.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate ABX test pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from levelwise_model.utterances import WordToUtteranceMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = WordToUtteranceMapping(map_file=\"data/level_wise/level0/utterances_original.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(a, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_utterances = []\n",
    "for word in utterances.utterances:\n",
    "    for utt in utterances.utterances[word]:\n",
    "        all_utterances.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/level_wise/level0/abx_sw_original.txt\", \"w+\", encoding=\"utf-8\") as abx_file:\n",
    "    for word_a in utterances.utterances:\n",
    "        for utt_a in utterances.utterances[word_a]:\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human ABX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from levelwise_model.utterances import WordToUtteranceMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = WordToUtteranceMapping(\"data/base_data/utterances_original.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sets = {\n",
    "    'librispeech': {\n",
    "        'preds': 0,\n",
    "        'total': 0,\n",
    "        'dataset': utterances.ls_utterances\n",
    "    },\n",
    "    'synthetic': {\n",
    "        'preds': 0,\n",
    "        'total': 0,\n",
    "        'dataset': utterances.sy_utterances\n",
    "    },\n",
    "    'mixed': {\n",
    "        'preds': 0,\n",
    "        'total': 0,\n",
    "        'dataset': utterances.mixed_utterances\n",
    "    },\n",
    "}\n",
    "\n",
    "preds = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "with open(\"test_file.txt\", \"w+\") as test_file:\n",
    "    test_file.write(\"Test set\\tIndex\\tA\\tB\\tX\\n\")\n",
    "    test_file.write(\"Test set\\tIndex\\tWord\\tUtterance\\n\")\n",
    "    with open(\"solutions.txt\", \"w+\") as solutions_file:\n",
    "        for test_set in test_sets:\n",
    "            ds = test_sets[test_set]['dataset']\n",
    "            for index in range(20):\n",
    "                # Select two random words for A and X\n",
    "                flag = True\n",
    "                while flag:\n",
    "                    word_a, word_x = np.random.choice(\n",
    "                        list(ds.keys()),\n",
    "                        size=2,\n",
    "                        replace=False\n",
    "                    )\n",
    "                    utt_a, utt_b = np.random.choice(\n",
    "                        ds[word_a],\n",
    "                        size=2,\n",
    "                        replace=False\n",
    "                    )\n",
    "\n",
    "                    utt_x = np.random.choice(\n",
    "                        ds[word_x]\n",
    "                    )\n",
    "                    if len(set([utt_a, utt_b, utt_x])) == 3:\n",
    "                        flag = False\n",
    "\n",
    "                test_file.write(test_set + \"\\t\" + str(index) + \"\\t\")\n",
    "                solutions_file.write(test_set + \"\\t\" + str(index) + \"\\t\")\n",
    "                test_file.write(utt_a + \"\\t\")\n",
    "                if np.random.random() < 0.5:\n",
    "                    test_file.write(utt_b + \"\\t\" + utt_x + \"\\n\")\n",
    "                    solutions_file.write(\"B\\t\" + utt_b + \"\\n\")\n",
    "                else:\n",
    "                    test_file.write(utt_x + \"\\t\" + utt_b + \"\\n\")\n",
    "                    solutions_file.write(\"X\\t\" + utt_x + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different W2V methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.Load(\"models/original_60k_250x1/level1/unigram_vs60000_lw.model\")\n",
    "w2v_model = Word2Vec.load(\"models/original_60k_250x1/level1/w2v_vs250_w1_lw.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_avg_word_vec_fn(word):\n",
    "    if word in w2v_model.wv.key_to_index.keys():\n",
    "        return w2v_model.wv[word].reshape(1, -1)\n",
    "    else:\n",
    "        pieces = list(\n",
    "            filter(\n",
    "                lambda x: x != \"▁\",\n",
    "                sp_model.EncodeAsPieces(word)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "        weights = [len(unit) for unit in units]\n",
    "\n",
    "        vectors = np.array([w2v_model.wv[unit] for unit in units])\n",
    "        return np.average(vectors, axis=0, weights=weights).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<levelwise_model.utterances.WordToUtteranceMapping at 0x12f1fb880>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'similarities': {'score': {'librispeech': 6.347491997101285,\n",
       "   'synthetic': -0.6719883444536395},\n",
       "  'errors': 0,\n",
       "  'trials': 1013},\n",
       " 'xw-abx-test': {'ABX Result': {'librispeech': 0.5175438596491229,\n",
       "   'synthetic': 0.47475961538461536,\n",
       "   'combined': 0.48990683229813664}},\n",
       " 'sw-abx-test': {'Same-word ABX Result': {'librispeech': 0.9442857142857143,\n",
       "   'synthetic': 0.9671428571428572,\n",
       "   'mixed': 0.8885714285714286}}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bench.run_suite(\n",
    "    utterances=utterances,\n",
    "    word_vec_fn=weighted_avg_word_vec_fn,\n",
    "    results_file=\"results/weighted_w2v.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "59580\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "with open(\"models/original_60k_250x1/level1/unigram_vs60000_lw.vocab\", \"r\") as vocab_file:\n",
    "    for line in vocab_file.readlines():\n",
    "        vocab.append(line.strip().split(\"\\t\")[0])\n",
    "print(len(vocab))\n",
    "vocab = list(set(vocab))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input='content', lowercase=False, vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "with open(\"data/base_data/corpus_ft.txt\", \"r\") as corpus_file:\n",
    "    for line in corpus_file.readlines():\n",
    "        corpus.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(lowercase=False,\n",
       "                vocabulary=[&#x27;JJJJJJJJJJxJJxxx&#x27;, &#x27;aaazO&#x27;, &#x27;TTTTTToobbbbIIII&#x27;,\n",
       "                            &#x27;KKKNNNNNNNNGGo&#x27;, &#x27;oVVVVpkP&#x27;, &#x27;HHHHHklllllTT&#x27;,\n",
       "                            &#x27;yyR&#x27;, &#x27;h&#x27;, &#x27;LLLLswww&#x27;, &#x27;mmmnnnnCCC&#x27;, &#x27;yyMMMMnn&#x27;,\n",
       "                            &#x27;LssssuugF&#x27;, &#x27;KKKKKKOF&#x27;, &#x27;VVVVyppjj&#x27;,\n",
       "                            &#x27;QQLLLssssswww&#x27;, &#x27;ooVVVyypppjj&#x27;, &#x27;ddddoVVV&#x27;,\n",
       "                            &#x27;mmmmmmmnFmmmnn&#x27;, &#x27;ffllllllT&#x27;, &#x27;obbbbbIIIy&#x27;,\n",
       "                            &#x27;eeeeeeeeeeeeenFt&#x27;, &#x27;LLLLssssugg&#x27;, &#x27;XXXXXF&#x27;,\n",
       "                            &#x27;yyyypfPPP&#x27;, &#x27;pkP&#x27;, &#x27;EEEEEoVVVy&#x27;,\n",
       "                            &#x27;nFnmnmmmmmmmmmmm&#x27;, &#x27;rKKKWWWWWW&#x27;, &#x27;obbbIIpkkQQQ&#x27;,\n",
       "                            &#x27;GGookkkk&#x27;, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(lowercase=False,\n",
       "                vocabulary=[&#x27;JJJJJJJJJJxJJxxx&#x27;, &#x27;aaazO&#x27;, &#x27;TTTTTToobbbbIIII&#x27;,\n",
       "                            &#x27;KKKNNNNNNNNGGo&#x27;, &#x27;oVVVVpkP&#x27;, &#x27;HHHHHklllllTT&#x27;,\n",
       "                            &#x27;yyR&#x27;, &#x27;h&#x27;, &#x27;LLLLswww&#x27;, &#x27;mmmnnnnCCC&#x27;, &#x27;yyMMMMnn&#x27;,\n",
       "                            &#x27;LssssuugF&#x27;, &#x27;KKKKKKOF&#x27;, &#x27;VVVVyppjj&#x27;,\n",
       "                            &#x27;QQLLLssssswww&#x27;, &#x27;ooVVVyypppjj&#x27;, &#x27;ddddoVVV&#x27;,\n",
       "                            &#x27;mmmmmmmnFmmmnn&#x27;, &#x27;ffllllllT&#x27;, &#x27;obbbbbIIIy&#x27;,\n",
       "                            &#x27;eeeeeeeeeeeeenFt&#x27;, &#x27;LLLLssssugg&#x27;, &#x27;XXXXXF&#x27;,\n",
       "                            &#x27;yyyypfPPP&#x27;, &#x27;pkP&#x27;, &#x27;EEEEEoVVVy&#x27;,\n",
       "                            &#x27;nFnmnmmmmmmmmmmm&#x27;, &#x27;rKKKWWWWWW&#x27;, &#x27;obbbIIpkkQQQ&#x27;,\n",
       "                            &#x27;GGookkkk&#x27;, ...])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(lowercase=False,\n",
       "                vocabulary=['JJJJJJJJJJxJJxxx', 'aaazO', 'TTTTTToobbbbIIII',\n",
       "                            'KKKNNNNNNNNGGo', 'oVVVVpkP', 'HHHHHklllllTT',\n",
       "                            'yyR', 'h', 'LLLLswww', 'mmmnnnnCCC', 'yyMMMMnn',\n",
       "                            'LssssuugF', 'KKKKKKOF', 'VVVVyppjj',\n",
       "                            'QQLLLssssswww', 'ooVVVyypppjj', 'ddddoVVV',\n",
       "                            'mmmmmmmnFmmmnn', 'ffllllllT', 'obbbbbIIIy',\n",
       "                            'eeeeeeeeeeeeenFt', 'LLLLssssugg', 'XXXXXF',\n",
       "                            'yyyypfPPP', 'pkP', 'EEEEEoVVVy',\n",
       "                            'nFnmnmmmmmmmmmmm', 'rKKKWWWWWW', 'obbbIIpkkQQQ',\n",
       "                            'GGookkkk', ...])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_word_vec_fn(word):\n",
    "    if word in w2v_model.wv.key_to_index.keys():\n",
    "        return w2v_model.wv[word].reshape(1, -1)\n",
    "    else:\n",
    "        pieces = list(\n",
    "            filter(\n",
    "                lambda x: x != \"▁\",\n",
    "                sp_model.EncodeAsPieces(word)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "        vectorised = vectorizer.transform([\" \".join(units)])\n",
    "        weights = vectorised.data\n",
    "        print(weights)\n",
    "        print(units)\n",
    "\n",
    "        vectors = np.array([w2v_model.wv[unit] for unit in units])\n",
    "        return np.average(vectors, axis=0, weights=weights).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51552197 0.45878498 0.499019   0.33485971 0.40323995]\n",
      "['UQQ', 'oVVVyDDDDiiii', 'SSTTTzzzzzz', 'oIIII', 'yyyppj']\n",
      "[0.51552197 0.45878498 0.499019   0.33485971 0.40323995]\n",
      "['UQQ', 'oVVVyDDDDiiii', 'SSTTTzzzzzz', 'oIIII', 'yyyppj']\n",
      "[0.41982555 0.4923799  0.22079995 0.44145863 0.35089233 0.46318978]\n",
      "['kB', 'o', 'VVVyyDDDDDiii', 'SSSTTzzzzzzz', 'IIIIyy', 'yyyyyyyypppjjjj', 'MMMMMM']\n",
      "Length of weights not compatible with specified axis.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have length at least 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_bench\u001b[39m.\u001b[39;49mrun_suite(\n\u001b[1;32m      2\u001b[0m     utterances\u001b[39m=\u001b[39;49mutterances,\n\u001b[1;32m      3\u001b[0m     word_vec_fn\u001b[39m=\u001b[39;49mtf_idf_word_vec_fn,\n\u001b[1;32m      4\u001b[0m     results_file\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mresults/tfidf_w2v.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m )\n",
      "File \u001b[0;32m~/Work/thesis/audio-semantics/levelwise_model/test_bench.py:372\u001b[0m, in \u001b[0;36mLSTestBench.run_suite\u001b[0;34m(self, utterances, word_vec_fn, results_file)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_suite\u001b[39m(\n\u001b[1;32m    351\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    352\u001b[0m         utterances: WordToUtteranceMapping,\n\u001b[1;32m    353\u001b[0m         word_vec_fn: \u001b[39mcallable\u001b[39m,\n\u001b[1;32m    354\u001b[0m         results_file: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    355\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[1;32m    356\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[39m    Runs all tests and saves to a results file.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[39m            to a file.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    371\u001b[0m     results \u001b[39m=\u001b[39m {\n\u001b[0;32m--> 372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msimilarities\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msimilarity_score_test(\n\u001b[1;32m    373\u001b[0m             utterances\u001b[39m=\u001b[39;49mutterances,\n\u001b[1;32m    374\u001b[0m             word_vec_fn\u001b[39m=\u001b[39;49mword_vec_fn\n\u001b[1;32m    375\u001b[0m         ),\n\u001b[1;32m    376\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mxw-abx-test\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcross_word_abx_test(\n\u001b[1;32m    377\u001b[0m             utterances\u001b[39m=\u001b[39mutterances,\n\u001b[1;32m    378\u001b[0m             word_vec_fn\u001b[39m=\u001b[39mword_vec_fn,\n\u001b[1;32m    379\u001b[0m             use_noise_for_x\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    380\u001b[0m         ),\n\u001b[1;32m    381\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msw-abx-test\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msame_word_abx_test(\n\u001b[1;32m    382\u001b[0m             utterances\u001b[39m=\u001b[39mutterances,\n\u001b[1;32m    383\u001b[0m             word_vec_fn\u001b[39m=\u001b[39mword_vec_fn,\n\u001b[1;32m    384\u001b[0m             use_noise_for_x\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    385\u001b[0m         )\n\u001b[1;32m    386\u001b[0m     }\n\u001b[1;32m    388\u001b[0m     \u001b[39mif\u001b[39;00m results_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    389\u001b[0m         dump(results, \u001b[39mopen\u001b[39m(results_file, \u001b[39m\"\u001b[39m\u001b[39mw+\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/Work/thesis/audio-semantics/levelwise_model/test_bench.py:134\u001b[0m, in \u001b[0;36mLSTestBench.similarity_score_test\u001b[0;34m(self, utterances, word_vec_fn)\u001b[0m\n\u001b[1;32m    130\u001b[0m     trials \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    132\u001b[0m \u001b[39m# Return the output score\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m--> 134\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m: {\n\u001b[1;32m    135\u001b[0m         test_set: pearsonr(\n\u001b[1;32m    136\u001b[0m                 scores[test_set],\n\u001b[1;32m    137\u001b[0m                 gold_standard[test_set]\n\u001b[1;32m    138\u001b[0m             )[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m    139\u001b[0m         \u001b[39mfor\u001b[39;00m test_set \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mlibrispeech\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msynthetic\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    140\u001b[0m     },\n\u001b[1;32m    141\u001b[0m     \u001b[39m'\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m'\u001b[39m: errors,\n\u001b[1;32m    142\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrials\u001b[39m\u001b[39m'\u001b[39m: trials\n\u001b[1;32m    143\u001b[0m }\n",
      "File \u001b[0;32m~/Work/thesis/audio-semantics/levelwise_model/test_bench.py:135\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    130\u001b[0m     trials \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    132\u001b[0m \u001b[39m# Return the output score\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    134\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m: {\n\u001b[0;32m--> 135\u001b[0m         test_set: pearsonr(\n\u001b[1;32m    136\u001b[0m                 scores[test_set],\n\u001b[1;32m    137\u001b[0m                 gold_standard[test_set]\n\u001b[1;32m    138\u001b[0m             )[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m    139\u001b[0m         \u001b[39mfor\u001b[39;00m test_set \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mlibrispeech\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msynthetic\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    140\u001b[0m     },\n\u001b[1;32m    141\u001b[0m     \u001b[39m'\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m'\u001b[39m: errors,\n\u001b[1;32m    142\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrials\u001b[39m\u001b[39m'\u001b[39m: trials\n\u001b[1;32m    143\u001b[0m }\n",
      "File \u001b[0;32m~/Work/thesis/audio-semantics/venv/lib/python3.9/site-packages/scipy/stats/_stats_py.py:4411\u001b[0m, in \u001b[0;36mpearsonr\u001b[0;34m(x, y, alternative)\u001b[0m\n\u001b[1;32m   4408\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mx and y must have the same length.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   4410\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m-> 4411\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mx and y must have length at least 2.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   4413\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\n\u001b[1;32m   4414\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(y)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have length at least 2."
     ]
    }
   ],
   "source": [
    "test_bench.run_suite(\n",
    "    utterances=utterances,\n",
    "    word_vec_fn=tf_idf_word_vec_fn,\n",
    "    results_file=\"results/tfidf_w2v.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁',\n",
       " 'UUUQ',\n",
       " 'obbbbbIpkk',\n",
       " 'QQ',\n",
       " 'rrAAffff',\n",
       " 'QQQ',\n",
       " 'oVVVppppjj',\n",
       " 'rrrKKKKK',\n",
       " 'OOOOOOOOOOMMMMMM']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out = sp_model.EncodeAsPieces(\"UUUQobbbbbIpkkQQrrAAffffQQQoVVVppppjjrrrKKKKKOOOOOOOOOOMMMMMM\")\n",
    "test_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = vectorizer.transform([\" \".join(test_out)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4176033 , 0.48186836, 0.41833208, 0.34615082, 0.31373919,\n",
       "       0.14351974, 0.12405802, 0.40517607])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22945"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_['UUUQ']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
