{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuBERT test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, HubertModel\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/hubert-large-ls960-ft were not used when initializing HubertModel: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/hubert-large-ls960-ft\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = HubertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d8ac12448e41bfa3a10855a228628d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset audiofolder (C:/Users/mj115gl/.cache/huggingface/datasets/audiofolder/dev-clean-6671ed00cafc447b/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a50d5cef5be4f65b32a942213430618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"C:\\\\Users\\\\mj115gl\\\\work_dir\\\\thesis\\\\audio-semantics\\\\data\\\\LibriSpeech\\\\dev-clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutput' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m input_values \u001b[39m=\u001b[39m processor(ds[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m2\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m], return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_values  \u001b[39m# Batch size 1\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m logits \u001b[39m=\u001b[39m model(input_values)\u001b[39m.\u001b[39;49mlogits\n\u001b[0;32m      3\u001b[0m predicted_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m transcription \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39mdecode(predicted_ids[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BaseModelOutput' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "input_values = processor(ds[\"train\"][2][\"audio\"][\"array\"], return_tensors=\"pt\").input_values  # Batch size 1\n",
    "logits = model(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.decode(predicted_ids[0])\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77040,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][1][\"audio\"][\"array\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 199760])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 624, 32])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HubertModel(\n",
       "  (feature_extractor): HubertFeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): HubertLayerNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (1-4): 4 x HubertLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x HubertLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): HubertFeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): HubertEncoderStableLayerNorm(\n",
       "    (pos_conv_embed): HubertPositionalConvEmbedding(\n",
       "      (conv): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "      (padding): HubertSamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x HubertEncoderLayerStableLayerNorm(\n",
       "        (attention): HubertAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): HubertFeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 624, 1024])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec, LineSentence\n",
    "# from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_fp = open(\"data/gtbrg_i.txt\", \"r\", encoding=\"utf-16\")\n",
    "sentences = LineSentence(line_fp)\n",
    "# line_fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'project gutenbergs the house on the borderland by william hope hodgson this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_fp.seek(0)\n",
    "line_fp.readline()\n",
    "# line_fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 100\n",
    "window = 5\n",
    "w2v_model_tag = \"TEST\"\n",
    "W2V_MODEL_PATH = f\"models/w2v_vs{vector_size}_w{window}_{w2v_model_tag}.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    sentences,\n",
    "    window=window,\n",
    "    vector_size=vector_size,\n",
    "    min_count=0,\n",
    "    workers=4,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(W2V_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'and',\n",
       " 'of',\n",
       " 'to',\n",
       " 'a',\n",
       " 'in',\n",
       " 'i',\n",
       " 'that',\n",
       " 'he',\n",
       " 'was',\n",
       " 'it',\n",
       " 'his',\n",
       " 'with',\n",
       " 'you',\n",
       " 'as']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(w2v_model.wv.key_to_index.keys())[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.8657098412513733),\n",
       " ('gentleman', 0.8089630603790283),\n",
       " ('fellow', 0.7922055125236511),\n",
       " ('person', 0.7875133752822876),\n",
       " ('creature', 0.7498121857643127),\n",
       " ('soldier', 0.7496156692504883),\n",
       " ('scotchman', 0.7041886448860168),\n",
       " ('girl', 0.6975987553596497),\n",
       " ('nobleman', 0.6809559464454651),\n",
       " ('chap', 0.6759620308876038)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.9092535972595215),\n",
       " ('princess', 0.8003181219100952),\n",
       " ('prince', 0.7828630805015564),\n",
       " ('sultan', 0.7463798522949219),\n",
       " ('empress', 0.7256040573120117),\n",
       " ('isabella', 0.7094663977622986),\n",
       " ('dowager', 0.6953573226928711),\n",
       " ('emperor', 0.6942340135574341),\n",
       " ('dauphin', 0.6938880085945129),\n",
       " ('duchess', 0.6818597316741943)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsim_scores = []\n",
    "\n",
    "with open(\"data/wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\") as wordsim_fp:\n",
    "    for line in wordsim_fp.readlines():\n",
    "        scores = line.split(\"\\t\")\n",
    "        w1, w2 = scores[0], scores[1]\n",
    "        gold_score = float(scores[2])\n",
    "        wordsim_scores.append([w1, w2, gold_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.621953172787471 , tested 200/203 pairs\n",
      "0.6182286374851715 , including OOV\n"
     ]
    }
   ],
   "source": [
    "gold_vocab = []\n",
    "gold_all = []\n",
    "preds_vocab = []\n",
    "preds_all = []\n",
    "tested = 0\n",
    "oov = 0\n",
    "\n",
    "for pairs in wordsim_scores:\n",
    "    w1, w2 = pairs[0].lower(), pairs[1].lower()\n",
    "    \n",
    "    try:\n",
    "        pred = w2v_model.wv.similarity(w1, w2)\n",
    "        preds_vocab.append(pred)\n",
    "        gold_vocab.append(pairs[2])\n",
    "        tested += 1\n",
    "    \n",
    "    except KeyError:\n",
    "        # if w1 not in w2v_model.wv.vocab.keys():\n",
    "        #     w1_units = sp.EncodeAsPieces(w1)[1:]\n",
    "        #     w1_vectors = np.array([w2v_model.wv[unit] for unit in w1_units])\n",
    "        #     w1_vector = w1_vectors.mean(axis=0)\n",
    "        # else:\n",
    "        #     w1_vector = w2v_model.wv[w1]\n",
    "        # if w2 not in w2v_model.wv.vocab.keys():\n",
    "        #     w2_units = sp.EncodeAsPieces(w2)[1:]\n",
    "        #     w2_vectors = np.array([w2v_model.wv[unit] for unit in w2_units])\n",
    "        #     w2_vector = w2_vectors.mean(axis=0)\n",
    "        # else:\n",
    "        #     w2_vector = w2v_model.wv[w2]\n",
    "\n",
    "        # pred = cosine_similarity(w1_vector.reshape(1, -1), w2_vector.reshape(1, -1))\n",
    "        oov += 1\n",
    "    \n",
    "    preds_all.append(pred)\n",
    "    gold_all.append(pairs[2])\n",
    "        \n",
    "\n",
    "print(spearmanr(preds_vocab, gold_vocab)[0], f\", tested {tested}/{len(wordsim_scores)} pairs\")\n",
    "print(spearmanr(preds_all, gold_all)[0], f\", including OOV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "simlex_scores = []\n",
    "\n",
    "with open(\"data/SimLex-999/SimLex-999.txt\") as simlex_fp:\n",
    "    for line in simlex_fp.readlines()[1:]:\n",
    "        scores = line.split(\"\\t\")\n",
    "        w1, w2 = scores[0], scores[1]\n",
    "        gold_score = float(scores[3])\n",
    "        simlex_scores.append([w1, w2, gold_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.339987359598551 , tested 994/999 pairs\n",
      "0.3385031092150666 , including OOV\n"
     ]
    }
   ],
   "source": [
    "gold_vocab = []\n",
    "gold_all = []\n",
    "preds_vocab = []\n",
    "preds_all = []\n",
    "tested = 0\n",
    "oov = 0\n",
    "\n",
    "for pairs in simlex_scores:\n",
    "    w1, w2 = pairs[0].lower(), pairs[1].lower()\n",
    "    \n",
    "    try:\n",
    "        pred = w2v_model.wv.similarity(w1, w2)\n",
    "        preds_vocab.append(pred)\n",
    "        gold_vocab.append(pairs[2])\n",
    "        tested += 1\n",
    "    \n",
    "    except KeyError:\n",
    "        # if w1 not in w2v_model.wv.vocab.keys():\n",
    "        #     w1_units = sp.EncodeAsPieces(w1)[1:]\n",
    "        #     w1_vectors = np.array([w2v_model.wv[unit] for unit in w1_units])\n",
    "        #     w1_vector = w1_vectors.mean(axis=0)\n",
    "        # else:\n",
    "        #     w1_vector = w2v_model.wv[w1]\n",
    "        # if w2 not in w2v_model.wv.vocab.keys():\n",
    "        #     w2_units = sp.EncodeAsPieces(w2)[1:]\n",
    "        #     w2_vectors = np.array([w2v_model.wv[unit] for unit in w2_units])\n",
    "        #     w2_vector = w2_vectors.mean(axis=0)\n",
    "        # else:\n",
    "        #     w2_vector = w2v_model.wv[w2]\n",
    "\n",
    "        # pred = cosine_similarity(w1_vector.reshape(1, -1), w2_vector.reshape(1, -1))\n",
    "        oov += 1\n",
    "    \n",
    "    preds_all.append(pred)\n",
    "    gold_all.append(pairs[2])\n",
    "        \n",
    "\n",
    "print(spearmanr(preds_vocab, gold_vocab)[0], f\", tested {tested}/{len(simlex_scores)} pairs\")\n",
    "print(spearmanr(preds_all, gold_all)[0], f\", including OOV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05325e31680d4f9aac652dd1fb33d09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39m# Add all similar words\u001b[39;00m\n\u001b[0;32m     27\u001b[0m tqdm_iterator\u001b[39m.\u001b[39mset_postfix({\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m : \u001b[39m\"\u001b[39m\u001b[39madding all words\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[1;32m---> 28\u001b[0m \u001b[39mfor\u001b[39;00m similar_word, score \u001b[39min\u001b[39;00m w2v_model\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mmost_similar(word, topn\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m):\n\u001b[0;32m     29\u001b[0m     tqdm_iterator\u001b[39m.\u001b[39mset_postfix({\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m : \u001b[39m\"\u001b[39m\u001b[39mchecked 50 words\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[0;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m score \u001b[39m>\u001b[39m threshold:\n",
      "File \u001b[1;32mc:\\Users\\mj115gl\\work_dir\\thesis\\audio-semantics\\venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:849\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[39mif\u001b[39;00m indexer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(topn, \u001b[39mint\u001b[39m):\n\u001b[0;32m    847\u001b[0m     \u001b[39mreturn\u001b[39;00m indexer\u001b[39m.\u001b[39mmost_similar(mean, topn)\n\u001b[1;32m--> 849\u001b[0m dists \u001b[39m=\u001b[39m dot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectors[clip_start:clip_end], mean) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorms[clip_start:clip_end]\n\u001b[0;32m    850\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m topn:\n\u001b[0;32m    851\u001b[0m     \u001b[39mreturn\u001b[39;00m dists\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_clusters = []\n",
    "words = list(w2v_model.wv.key_to_index.keys())\n",
    "\n",
    "tqdm_iterator = tqdm(range(0, 99, 1))\n",
    "\n",
    "for threshold in tqdm_iterator:\n",
    "    threshold = threshold / 100\n",
    "    word_to_cluster = dict()  # Stores map from word to cluster\n",
    "    cluster_to_words = dict()  # Stores map from cluster to words\n",
    "    cluster_idx = 0  # Counter\n",
    "\n",
    "    for word in words:\n",
    "        tqdm_iterator.set_postfix({\"Word\": word})\n",
    "        # Check if word has already been clustered\n",
    "        if word not in word_to_cluster.keys():\n",
    "            # Create new cluster\n",
    "            cluster_idx += 1\n",
    "            # cluster_key = chr(0x0020 + cluster_idx)\n",
    "            cluster_key = cluster_idx\n",
    "\n",
    "            # Add new word to cluster\n",
    "            tqdm_iterator.set_postfix({\"status\" : \"adding new word to cluster\"})\n",
    "            cluster_to_words[cluster_key] = [word]\n",
    "            word_to_cluster[word] = cluster_key\n",
    "            \n",
    "            # Add all similar words\n",
    "            tqdm_iterator.set_postfix({\"status\" : \"adding all words\"})\n",
    "            for similar_word, score in w2v_model.wv.most_similar(word, topn=50):\n",
    "                tqdm_iterator.set_postfix({\"status\" : \"checked 50 words\"})\n",
    "                if score > threshold:\n",
    "                    cluster_to_words[cluster_key].append(similar_word)\n",
    "                    word_to_cluster[similar_word] = cluster_key\n",
    "    \n",
    "    n_clusters.append(len(cluster_to_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "681566"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = {\n",
    "    i: key for i, key in enumerate(ascii_letters)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_key = {}\n",
    "key_to_word = {}\n",
    "\n",
    "with open(\"data/quantized/dev-gold.csv\", \"r\") as key_file:\n",
    "    for line in key_file.readlines()[1:]:\n",
    "        dataset, key, _, word = line.strip().split(\",\")\n",
    "        if word not in word_to_key:\n",
    "            word_to_key[word] = {\n",
    "                'librispeech': [],\n",
    "                'synthetic': []\n",
    "            }\n",
    "        word_to_key[word][dataset].append(key)\n",
    "        key_to_word[key] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = {}\n",
    "\n",
    "for dataset in [\"librispeech\", \"synthetic\"]:\n",
    "    with open(f\"data/quantized/semantic/dev/{dataset}/quantized_outputs.txt\", \"r\") as utterance_file:\n",
    "        for line in utterance_file.readlines():\n",
    "            key, seq = line.strip().split(\"\\t\")\n",
    "            utterance = seq.split(\",\")[1:]\n",
    "\n",
    "            key = \"ls_\" + key_to_word[key] if dataset == \"librispeech\" else \"sy_\" + key_to_word[key]\n",
    "            if key not in utterances:\n",
    "                utterances[key] = []\n",
    "\n",
    "            utterances[key].append(\n",
    "                \"\".join(\n",
    "                    [letters[int(v)] for i, v in enumerate(utterance)]\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/level_wise/level0/utterances_original.txt\", \"w+\", encoding=\"utf-8\") as ufp:\n",
    "    for word in utterances:\n",
    "        for utterance in utterances[word]:\n",
    "            ufp.write(word + \"\\t\" + utterance + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_pairs = []\n",
    "rel_pairs = []\n",
    "\n",
    "with open(\"data/quantized/dev-pairs.csv\", \"r\") as pairs_file:\n",
    "    for line in pairs_file.readlines()[1:]:\n",
    "        dataset, _, w1, w2, sim, rel = line.strip().split(\",\")\n",
    "        if sim:\n",
    "            sim_pairs.append((dataset, w1, w2, float(sim)))\n",
    "        if rel:\n",
    "            rel_pairs.append((dataset, w1, w2, float(rel)))\n",
    "\n",
    "with open(\"data/level_wise/level0/pairs.txt\", \"w+\", encoding=\"utf-8\") as pairs_fp:\n",
    "    for pair in sim_pairs:\n",
    "        dataset, w1, w2, score = pair\n",
    "        pairs_fp.write(\n",
    "            (\"ls_\" + w1 if dataset == \"librispeech\" else \"sy_\" + w1) + \",\" +\n",
    "            (\"ls_\" + w2 if dataset == \"librispeech\" else \"sy_\" + w2) + \",\" +\n",
    "            str(score) + \",\" + \"\\n\"\n",
    "        )\n",
    "    for pair in rel_pairs:\n",
    "        dataset, w1, w2, score = pair\n",
    "        pairs_fp.write(\n",
    "            (\"ls_\" + w1 if dataset == \"librispeech\" else \"sy_\" + w1) + \",\" +\n",
    "            (\"ls_\" + w2 if dataset == \"librispeech\" else \"sy_\" + w2) + \",\" +\n",
    "            \",\" + str(score) + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_model = spm.SentencePieceProcessor()\n",
    "\n",
    "sp_model.Load(\"models/original_60k_250x1/level1/unigram_vs60000_lw.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_corpus = []\n",
    "\n",
    "with open(\"data/level_wise/level0/dev_corpus_original.txt\", \"r\", encoding=\"utf-8\") as ocfp:\n",
    "    for line in ocfp.readlines():\n",
    "        original_corpus.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/original_60k_250x1/level1/dev_corpus_ft.txt\", \"w+\", encoding=\"utf-8\") as ncfp:\n",
    "    for line in original_corpus:\n",
    "        pieces = list(\n",
    "            filter(\n",
    "                lambda x: x != \"▁\",\n",
    "                sp_model.EncodeAsPieces(line)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "        \n",
    "        ncfp.write(\" \".join(units) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = fasttext.train_unsupervised(\n",
    "    \"data/original_60k_250x1/level1/corpus_ft.txt\",\n",
    "    \"cbow\",\n",
    "    dim=250,\n",
    "    thread=4,\n",
    "    epoch=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fasttext.FastText._FastText"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.save_model(\"models/fasttext_cbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_model = fasttext.load_model(\"models/fasttext_cbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from levelwise_model.test_bench import LSTestBench\n",
    "from levelwise_model.utterances import WordToUtteranceMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_mapping = WordToUtteranceMapping(map_file=\"data/level_wise/level0/utterances_original.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bench = LSTestBench(scores_file=\"data/level_wise/level0/pairs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bench.ft_score_and_save(ft_model=ft_model, utterances=utterance_mapping, results_file=\"results/ft_cbow\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\mj115gl\\work_dir\\thesis\\audio-semantics\\models\\db_final. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\mj115gl\\work_dir\\thesis\\audio-semantics\\models\\db_final were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('C:\\\\Users\\\\mj115gl\\\\work_dir\\\\thesis\\\\audio-semantics\\\\models\\\\db_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.Load(\"models/comp_60k_250x1/level1/unigram_vs60000_lw.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PlQQQQ oobbbbbIkk QorrrAAAff lllQQo oVVVpppjj UQ zzzzOOOOOOOOOOO\n"
     ]
    }
   ],
   "source": [
    "input_text = \"PlQQQQoobbbbbIkkQorrrAAAfflllQQooVVVpppjjUQzzzzOOOOOOOOOOO\"\n",
    "\n",
    "pieces = list(\n",
    "    filter(\n",
    "        lambda x: x != \"▁\",\n",
    "        sp_model.EncodeAsPieces(input_text)\n",
    "    )\n",
    ")\n",
    "\n",
    "units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "print(\" \".join(units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our sentences we like to encode\n",
    "sentences = ['UUUQ obbbbbIpkk QQ rrAAffff QQQ oVVVppppjj rrrKKKKK OOOOOOOOOOMMMMMM',\n",
    "    'lQ oobbbbIII kkkkQQQ orrAAff llQQ oVVVVpppjjj KKKKKKzOO VV',\n",
    "    \"PQQ obbbbbbkkkQ rrrrrfff QlQQQ oVVVpppjjjj rrrKKKKKK zzzO HHH\",\n",
    "    \"pDjj CCCCCCobbbbII pkkkQQQ rrrrAAff ffllll QQooVVV pppjjjj E KKKKKKKKKO KKKKOOOOOOOOOO\",\n",
    "    \"PlQQQQ oobbbbbIkk QorrrAAAff lllQQo oVVVpppjj UQ zzzzOOOOOOOOOOO\"\n",
    "]\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.2528386 , -1.2860142 ,  0.8338024 , ..., -0.7757539 ,\n",
       "         0.14618087, -1.0259092 ],\n",
       "       [ 1.0756059 , -1.3480805 ,  1.1199704 , ..., -0.3672155 ,\n",
       "         0.57059145, -1.5578766 ],\n",
       "       [ 0.7610003 , -1.17496   ,  1.1490151 , ..., -0.3124733 ,\n",
       "         0.29432905, -1.3124402 ],\n",
       "       [ 0.6083768 , -1.2210584 ,  1.4475336 , ..., -0.35673165,\n",
       "         0.3092123 , -1.241608  ],\n",
       "       [ 1.0120195 , -1.1010975 ,  1.4700158 , ..., -0.53832185,\n",
       "         0.29323524, -1.0156256 ]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000002 , 0.9521097 , 0.93304825, 0.9465009 , 0.9377239 ],\n",
       "       [0.9521097 , 0.9999995 , 0.93354774, 0.9546412 , 0.9466411 ],\n",
       "       [0.93304825, 0.93354774, 0.99999964, 0.9444392 , 0.9428807 ],\n",
       "       [0.9465009 , 0.9546412 , 0.9444392 , 0.9999999 , 0.9522452 ],\n",
       "       [0.9377239 , 0.9466411 , 0.9428807 , 0.9522452 , 1.0000002 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = cosine_similarity(embeddings, dense_output=True)\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9443777"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(similarities[np.where(np.tril(similarities, -1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from levelwise_model.utterances import WordToUtteranceMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = WordToUtteranceMapping(\"data/level_wise/level0/utterances.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_utterances = {}\n",
    "\n",
    "for word in utterances.utterances:\n",
    "    updated_utterances[word] = []\n",
    "    for utt in utterances.utterances[word]:\n",
    "        pieces = list(\n",
    "            filter(\n",
    "                lambda x: x != \"▁\",\n",
    "                sp_model.EncodeAsPieces(utt)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "        updated_utterances[word].append(\n",
    "            \" \".join(units)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode(\"test\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg positive score (LS) = 0.9355412933498762 ([0.9478882, 0.9341124, 0.9349109, 0.92906487, 0.93097824, 0.919134, 0.95544744, 0.92582834, 0.93838286, 0.92994934, 0.93962383, 0.9301245, 0.9431162, 0.94423187, 0.94626683, 0.9413644, 0.930119, 0.9322844, 0.93690395, 0.93456966, 0.9336634, 0.93571436, 0.9314061, 0.93531317, 0.94441867, 0.9393776, 0.9473236, 0.9428841, 0.9286855, 0.9383246, 0.94521725, 0.9484435, 0.9259017, 0.93798345, 0.93186796, 0.92320585, 0.94726497, 0.92739666, 0.93498486, 0.9584384, 0.9345622, 0.93492544, 0.934797, 0.93858486, 0.9498227, 0.93013906, 0.9253137, 0.9444783, 0.91060156, 0.9337509, 0.9257393, 0.93742037, 0.9397783, 0.9404488, 0.9348502, 0.931848, 0.9314847, 0.93038434, 0.95055205, 0.93702954, 0.9408466, 0.95822686, 0.9228851, 0.9493038, 0.9305943, 0.9484994, 0.9359726, 0.9392512, 0.94092494, 0.93935937, 0.9496282, 0.9377645, 0.9383466, 0.9265093, 0.93705744, 0.9304095, 0.9282099, 0.9382057, 0.9431409, 0.9332338, 0.9283988, 0.927216, 0.9556505, 0.9328858, 0.929038, 0.93341285, 0.94099605, 0.93750554, 0.93163073, 0.94969976, 0.9485182, 0.92675835, 0.9414111, 0.92657614, 0.92310274, 0.94627213, 0.9176958, 0.9289311, 0.9394741, 0.92834145, 0.92669827, 0.9419103, 0.9282761, 0.93180203, 0.9404125, 0.9470179, 0.95926434, 0.9436666, 0.92489606, 0.9305266, 0.9495324, 0.9313967, 0.9323461, 0.93591684, 0.93446714, 0.9475562, 0.93476, 0.9562388, 0.9390476, 0.9189472, 0.9509532, 0.9297228, 0.93015295, 0.94409716, 0.93298286, 0.9570097, 0.93881476, 0.95296335, 0.9375011, 0.93064195, 0.93924826, 0.92996144, 0.93563163, 0.9293801, 0.92730516, 0.9311047, 0.92934173, 0.94526225, 0.93996936, 0.93072724, 0.9427483, 0.9517347, 0.9324837, 0.9365136, 0.92582035, 0.9367058, 0.92885387, 0.9448242, 0.9214201, 0.9453705, 0.9284371, 0.94707227, 0.92920846, 0.94317067, 0.9268554, 0.9200409, 0.94584084, 0.96522725, 0.9389452, 0.93606323, 0.9479491, 0.93016386, 0.93865985, 0.9318522, 0.9530666, 0.9386586, 0.9426801, 0.9381007, 0.9213573, 0.9497077, 0.9242072, 0.8513242, 0.9387448, 0.9425435, 0.9389641, 0.92660266, 0.93222934, 0.933224, 0.93821305, 0.92638105, 0.927044, 0.93731785, 0.93454564, 0.9303766, 0.93768054, 0.92774755, 0.9329214, 0.9350598, 0.92584354, 0.92813903, 0.9346239, 0.94348997, 0.93851674, 0.9250948, 0.9282032, 0.9320695, 0.9303248, 0.93808705, 0.95331115, 0.9351597, 0.94990474, 0.951473, 0.91922635, 0.9377992, 0.9263392, 0.9393084, 0.92811805, 0.95318586, 0.9451589, 0.934337, 0.9300971, 0.93197286, 0.92998457, 0.9302464, 0.9379508, 0.93993187, 0.9259309, 0.9527869, 0.9303497, 0.9427193, 0.9415123, 0.9358638, 0.92885166, 0.9373384, 0.9386459, 0.9267571, 0.91873616, 0.9419708, 0.9380289, 0.9430942, 0.9550393, 0.9328886, 0.9375131, 0.94440055, 0.93578166, 0.9368903, 0.9524012, 0.9309484, 0.94557035, 0.9456247, 0.9320608, 0.95368993, 0.93900245, 0.927421, 0.9321163, 0.92745954, 0.9415351, 0.92892, 0.9405554, 0.94177395, 0.93416226, 0.93430126, 0.92456937, 0.9336459, 0.9373516, 0.939388, 0.91985965, 0.9359642, 0.94008744, 0.9609712, 0.9319258, 0.932348, 0.9463091, 0.923872, 0.92661744, 0.94882184, 0.9218476, 0.9032246, 0.92180204, 0.92929614, 0.93864447, 0.93857837, 0.92097026, 0.92041343, 0.9380362, 0.92909914, 0.94830686, 0.9337763, 0.9279669, 0.9412919, 0.9210854, 0.9526133, 0.9346986, 0.94914556, 0.92301166, 0.9227858, 0.941201, 0.95028687, 0.9161561, 0.94868034, 0.93743527, 0.93758214, 0.9301553, 0.94504374, 0.93807805, 0.93370056, 0.9471472, 0.93502045, 0.93059206, 0.9293677, 0.9268019, 0.92730415, 0.92778, 0.93133473, 0.93506765, 0.9392285, 0.9254727, 0.92747813, 0.94838417, 0.93462247, 0.9335527, 0.9330389, 0.9343563, 0.9393753, 0.9331364, 0.94346684, 0.92988896, 0.9301295, 0.934737, 0.9439031, 0.9326489, 0.93788844, 0.93207693, 0.9376185, 0.9356296, 0.912852, 0.9209969, 0.93931204, 0.92937946, 0.92740935, 0.95211715, 0.9313183, 0.9363906, 0.9377624, 0.9338485, 0.9318519, 0.95063883, 0.9411603, 0.9288541, 0.95903957, 0.9317289, 0.93615335, 0.9492567, 0.94842756, 0.92878264, 0.92967784, 0.9359183, 0.93158376, 0.94361717, 0.94860613, 0.93288547, 0.9360018, 0.93023586, 0.93585503, 0.9279039, 0.93036616, 0.9313274, 0.9436764, 0.9340763, 0.9368685, 0.94388527, 0.95404977, 0.9402434, 0.9418556, 0.9445602, 0.9327821, 0.93920857, 0.93561196, 0.9435494, 0.9217393, 0.92459995, 0.9402375, 0.9136423, 0.9380072, 0.93431395, 0.9242842, 0.92871267, 0.93543565, 0.93290085, 0.9483955, 0.93215704, 0.9561126, 0.9315668, 0.93357474, 0.9356171, 0.95792216, 0.9286095, 0.9330447, 0.92909896, 0.9435349, 0.9300337, 0.9268694, 0.9317165, 0.9338725, 0.9250268, 0.9359837, 0.9300211, 0.92925787, 0.93808484, 0.9129828, 0.9358495, 0.92093825, 0.9275675, 0.9507275, 0.94078696, 0.9341656, 0.9253488, 0.9273997, 0.94390815, 0.93722475, 0.92101526, 0.9438839, 0.92560804, 0.9296476, 0.9081702, 0.93582976, 0.93255424, 0.9389034, 0.9322705, 0.93815184, 0.9238279, 0.9388037] words).\n",
      "Avg positive score (SY) = 0.9486159888641977 ([0.9560749, 0.95487, 0.9512922, 0.9395144, 0.9611303, 0.9483146, 0.95585966, 0.94239116, 0.9472931, 0.9449115, 0.9443371, 0.9514308, 0.948503, 0.9382165, 0.9523523, 0.9398484, 0.94768816, 0.9509273, 0.9397885, 0.95142287, 0.9333658, 0.95111686, 0.9397672, 0.95006275, 0.94318247, 0.94475937, 0.9519506, 0.9508505, 0.94996613, 0.9418673, 0.95743555, 0.9492326, 0.95126134, 0.96191186, 0.9419663, 0.94352984, 0.96236235, 0.94971925, 0.95041376, 0.9532833, 0.95061666, 0.95790094, 0.9182329, 0.9456338, 0.954428, 0.9495843, 0.9424224, 0.9460082, 0.94880265, 0.9641271, 0.9559266, 0.9442632, 0.9411326, 0.9379006, 0.9554494, 0.94899446, 0.9377954, 0.96318334, 0.9561593, 0.94286156, 0.95003027, 0.9464898, 0.94551826, 0.9450624, 0.93009406, 0.91057307, 0.94497806, 0.9446406, 0.9357087, 0.9548519, 0.95128804, 0.9526839, 0.952874, 0.92965335, 0.95535976, 0.9534152, 0.9434901, 0.92726785, 0.95909977, 0.9444036, 0.9392044, 0.94146514, 0.94506, 0.96192354, 0.96308905, 0.95552725, 0.9445364, 0.9461193, 0.9632947, 0.9538718, 0.96730804, 0.9507449, 0.9509857, 0.9434924, 0.9522095, 0.9432822, 0.95157194, 0.9434471, 0.9517927, 0.92344457, 0.95499414, 0.94354224, 0.9417173, 0.9377279, 0.9495751, 0.9481872, 0.9507508, 0.92753404, 0.95345956, 0.9424744, 0.94806904, 0.95062417, 0.9579255, 0.9440333, 0.94586915, 0.95292974, 0.95188206, 0.9369957, 0.94501114, 0.94260186, 0.96227974, 0.94599265, 0.93578595, 0.95779437, 0.94865686, 0.95924634, 0.9471442, 0.95020056, 0.9542296, 0.95191383, 0.9245195, 0.95188683, 0.9578262, 0.9412536, 0.94983035, 0.9405653, 0.95166445, 0.96093553, 0.9510267, 0.9511616, 0.9608373, 0.94936305, 0.9641606, 0.94447494, 0.95674604, 0.9410229, 0.95453453, 0.9632557, 0.96301323, 0.9509267, 0.92942, 0.9487267, 0.9566858, 0.94116443, 0.95315737, 0.95712805, 0.94752145, 0.9517956, 0.94989175, 0.94075364, 0.94961286, 0.961528, 0.9431041, 0.94532555, 0.9318285, 0.9321645, 0.94394445, 0.949683, 0.94864994, 0.9542565, 0.9616619, 0.95083565, 0.94172764, 0.9453013, 0.9380272, 0.9510619, 0.9606912, 0.94838744, 0.94646233, 0.9481864, 0.9377895, 0.9507567, 0.962328, 0.93638897, 0.9446134, 0.95256215, 0.9438146, 0.94936675, 0.94807696, 0.94580984, 0.94481033, 0.95469826, 0.9252024, 0.96094036, 0.9399465, 0.9516209, 0.94517297, 0.9573359, 0.95258856, 0.960787, 0.95768356, 0.92437893, 0.9474167, 0.93735605, 0.95259625, 0.94296503, 0.95368433, 0.95562935, 0.9626947, 0.9443834, 0.9524104, 0.93381524, 0.9631262, 0.9404996, 0.9568686, 0.9616575, 0.9410603, 0.9420679, 0.95693946, 0.9442716, 0.94034696, 0.9357743, 0.9507146, 0.9582226, 0.9400239, 0.9444952, 0.9468923, 0.9423878, 0.94848317, 0.9318061, 0.9401857, 0.9532349, 0.9456714, 0.95467037, 0.9591749, 0.9548216, 0.938523, 0.94214743, 0.94906855, 0.95302635, 0.9416322, 0.9380874, 0.9500308, 0.9504774, 0.9548771, 0.94499445, 0.9542367, 0.9507168, 0.93158674, 0.9623785, 0.9534686, 0.9508768, 0.9523358, 0.95230526, 0.9434034, 0.95471567, 0.94598144, 0.9493485, 0.95009893, 0.9575421, 0.95467234, 0.93844485, 0.9457743, 0.9350272, 0.9516354, 0.9478106, 0.9537332, 0.9441169, 0.95886487, 0.95599407, 0.9442087, 0.9512291, 0.95682937, 0.95937514, 0.9496619, 0.9459255, 0.9487746, 0.9501495, 0.93682665, 0.94574744, 0.9536324, 0.9522156, 0.954202, 0.9512213, 0.95211464, 0.94727254, 0.93204737, 0.9519952, 0.9451938, 0.93220687, 0.9521589, 0.95515496, 0.94622344, 0.9463379, 0.93648237, 0.95347124, 0.9474916, 0.95850724, 0.94803125, 0.94111013, 0.9427332, 0.9584144, 0.948662, 0.9632543, 0.9600811, 0.95668405, 0.946579, 0.950579, 0.946003, 0.9497032, 0.94384986, 0.93411946, 0.9426923, 0.95052534, 0.9467122, 0.94016844, 0.94101745, 0.9352644, 0.93851775, 0.9417611, 0.9312024, 0.9605357, 0.9661915, 0.93281436, 0.95960236, 0.94255257, 0.95774037, 0.9543254, 0.94001764, 0.94573087, 0.9546216, 0.95234615, 0.9548516, 0.9474321, 0.9507987, 0.94640034, 0.9473164, 0.95425457, 0.9604232, 0.9492809, 0.9441027, 0.9413664, 0.9470206, 0.95461065, 0.9447262, 0.96198314, 0.95035154, 0.94176745, 0.942178, 0.9523925, 0.94775724, 0.96207124, 0.9563305, 0.93947864, 0.94860196, 0.9158426, 0.9513736, 0.96298146, 0.9542365, 0.95242023, 0.9538813, 0.9533487, 0.94377595, 0.95514655, 0.9451373, 0.9443881, 0.95305616, 0.9269298, 0.94898266, 0.95312333, 0.94434196, 0.95378095, 0.9593973, 0.96053106, 0.9607041, 0.9490972, 0.94735223, 0.9451359, 0.9554923, 0.9488041, 0.95494556, 0.95248073, 0.9457424, 0.9502487, 0.9496949, 0.9508002, 0.9532142, 0.9506641, 0.9348455, 0.9479397, 0.95404077, 0.9455057, 0.9451277, 0.9419391, 0.9416416, 0.95627767, 0.9520878, 0.95642465, 0.94762945, 0.96076375, 0.9399864, 0.9445173, 0.9462672, 0.9511699, 0.9452042, 0.95195657, 0.93727285, 0.94997185, 0.95277405, 0.9247069, 0.9217592, 0.9467299, 0.9456103, 0.9538336, 0.9507684, 0.9577691, 0.9503611, 0.96601135, 0.95689416, 0.95963985, 0.9546837, 0.93348056, 0.9455436, 0.95585966, 0.9589224, 0.9364357, 0.93827754, 0.94958067, 0.95700353, 0.9667309, 0.9439531, 0.9226539, 0.9678728, 0.9520564, 0.9319512, 0.94946855, 0.94641066, 0.95740825, 0.948345, 0.9471651, 0.91893905, 0.9592256, 0.95635104, 0.94199187, 0.94976115, 0.94620544, 0.9438074, 0.94378376, 0.9495774, 0.95420474, 0.9434676, 0.94416934, 0.9475443, 0.9500012, 0.9559784, 0.9430647, 0.9434614, 0.9545622, 0.95410854, 0.93224543, 0.94998163, 0.9491098, 0.95264643, 0.95267004, 0.96347934, 0.946547, 0.9508579, 0.96400577, 0.9465216, 0.9405043, 0.9602998, 0.9551801, 0.9462748, 0.9426796, 0.9572704, 0.9572833, 0.9483741, 0.954937, 0.9453223, 0.9576476, 0.9509699, 0.9589071, 0.9458919, 0.9468129, 0.96195954, 0.95205504, 0.9584079, 0.9539445, 0.9494415, 0.9624761, 0.95663244, 0.9473013, 0.952545, 0.9437862, 0.958652, 0.9332545, 0.9483712, 0.95064396, 0.93583375, 0.9613576, 0.96189255, 0.93444157, 0.95501846, 0.9460867, 0.94382787, 0.96702504, 0.95623356, 0.9417806, 0.94715214, 0.94080836, 0.9380252, 0.9558248, 0.9548478, 0.9477504, 0.94778633, 0.9532214, 0.94674397, 0.95293516, 0.9401166, 0.9507715, 0.96299666, 0.942823, 0.9535038, 0.9409297, 0.95291144, 0.9513121, 0.94475085, 0.9321417, 0.9547019, 0.9528752, 0.9524657, 0.94273454, 0.93399066, 0.94326925, 0.94660395, 0.94710356, 0.9475906, 0.9398479, 0.9557647, 0.9527891, 0.94817376, 0.9546918, 0.95350605, 0.95006627, 0.9395569, 0.9458742, 0.95335555, 0.94150263, 0.9580198, 0.9441395, 0.9504192, 0.95184714, 0.95242935, 0.94498414, 0.9546943, 0.9499671, 0.9503378, 0.95066905, 0.96715575, 0.9507108, 0.9492466, 0.94094324, 0.9508152, 0.9424922, 0.9431649, 0.9461792, 0.94036317, 0.9493203, 0.9541562, 0.9419617, 0.9475923, 0.9431452, 0.92698485, 0.93669254, 0.9382729, 0.946974, 0.955952, 0.94678277, 0.953845, 0.9410963, 0.9489729, 0.9572618, 0.95804214, 0.9427846, 0.9470641, 0.9443527, 0.9620221, 0.9558532, 0.9535177, 0.9456046, 0.9513169, 0.9487574, 0.9510145, 0.94915754, 0.9610216, 0.93156284, 0.9496973, 0.9545153, 0.9503047, 0.963408, 0.94760877, 0.94491285, 0.9509895, 0.95423555, 0.924674, 0.948122, 0.9459877, 0.94594616, 0.96408635, 0.95452476, 0.9569338, 0.9501298, 0.9552589, 0.9571326, 0.9572633, 0.9487253, 0.9457056, 0.94189, 0.947829, 0.95324206, 0.9477947, 0.9440913, 0.951238, 0.9568739, 0.9425473, 0.9590443, 0.95613796, 0.9575002, 0.95226073, 0.9415024, 0.9386359, 0.94262475, 0.9519119, 0.9507324, 0.96855235, 0.94734067, 0.9544394, 0.94848984, 0.9455522, 0.9466669, 0.9594891, 0.9533747, 0.95133114, 0.94300556, 0.94977766, 0.9613816, 0.95592546, 0.96396905, 0.9657326, 0.95396227, 0.9411123, 0.94408494, 0.95496416, 0.95374745, 0.9471776, 0.96337885, 0.956651, 0.937193, 0.95040935, 0.9303839, 0.94801885, 0.9563775, 0.95271254, 0.9220901, 0.94076633, 0.94355017, 0.9545682, 0.9310097, 0.9499915, 0.9497679, 0.9563256, 0.9524102, 0.944709, 0.9503661, 0.9549782, 0.9385231, 0.9367587, 0.94568086, 0.95096874, 0.96594626, 0.9454274, 0.95192236, 0.9482415, 0.960906, 0.9518221, 0.9304425, 0.9544888, 0.9448376, 0.93635005, 0.950875, 0.9511165, 0.9434708, 0.9401497, 0.9418084, 0.9505612, 0.9492503, 0.9449234, 0.9684771, 0.95139694, 0.9475736, 0.94445133, 0.9447934, 0.9714813, 0.95595986, 0.9573341, 0.96613216, 0.9515786, 0.95383316, 0.9481402, 0.9521566, 0.95079046, 0.9422107, 0.915587, 0.92930746, 0.95669585, 0.94311744, 0.9465682, 0.94285935, 0.94251394, 0.95128345, 0.9496991, 0.9432361, 0.94311714, 0.9450574, 0.95275563, 0.9384227, 0.94959515, 0.93215626, 0.9659133, 0.951829, 0.94022846, 0.94320315, 0.9500568, 0.9508815, 0.9508154, 0.9625605, 0.95981103, 0.94565946, 0.9515707, 0.9527934, 0.9522422, 0.9567931, 0.94412714, 0.9299233, 0.9325121, 0.94911593, 0.94214743, 0.92871, 0.9504614, 0.9519453, 0.9405568, 0.9524653, 0.94571733, 0.96105796, 0.9508276, 0.9517684, 0.953633, 0.94031787, 0.94570565, 0.9224331, 0.9439826, 0.9422157, 0.95761126, 0.9375108, 0.9497966, 0.92769474, 0.9485743, 0.95370895, 0.95466536, 0.9541283, 0.95098, 0.94156027, 0.9495528, 0.9520604, 0.9485554, 0.9501994, 0.9567657, 0.94399375, 0.9489465, 0.95471287, 0.94910526, 0.9368089, 0.9536724, 0.95683867, 0.9527993, 0.95565253, 0.95886564, 0.9557263, 0.94939613, 0.9472432, 0.9504654, 0.94144946, 0.96682996, 0.9639354, 0.951963, 0.94506645, 0.9428389, 0.94446874, 0.95043826, 0.94475317, 0.9533052, 0.9460304, 0.95802116, 0.95652837, 0.9682498, 0.95413, 0.9409632, 0.9257929, 0.9544461, 0.9363876, 0.9563973, 0.95203215, 0.9458596, 0.9375364, 0.94752145, 0.9562376, 0.9290631, 0.9382732, 0.94908184, 0.95214003, 0.9565931, 0.94678646, 0.96306807, 0.95996636, 0.9542298, 0.95250624, 0.9442756, 0.96434134, 0.9450445, 0.95428634, 0.9446113, 0.94787294, 0.942599, 0.9505129, 0.9574371, 0.9249782, 0.9540107, 0.9531409, 0.94111925, 0.9438327, 0.952275, 0.9525406, 0.95136875, 0.95896596, 0.9435634, 0.9456372, 0.9567917, 0.9522047, 0.9390598, 0.9499827, 0.95602006, 0.9520478, 0.95827466, 0.9583923, 0.96121913, 0.9562697, 0.9588754, 0.95225257, 0.94022346, 0.945271, 0.95424455, 0.9504129, 0.9591727, 0.9346872, 0.92415553, 0.9617679, 0.9536881, 0.95375544, 0.95024544, 0.9562039, 0.94331163, 0.93545955, 0.9472165, 0.9581309, 0.9569788, 0.9442942, 0.94787496, 0.94818276, 0.9533298, 0.9405331, 0.9289759, 0.9328036, 0.9525408, 0.9484351, 0.9376375, 0.944596, 0.96035594, 0.95784634, 0.9585505, 0.94123787, 0.9504829, 0.95445484, 0.9499137, 0.92003495, 0.95127743, 0.9466715, 0.96099335, 0.9535961, 0.95023745, 0.9550967, 0.9441862, 0.94807833, 0.9468376, 0.9512921, 0.9561805, 0.9620309, 0.9433942, 0.9526384, 0.95189786, 0.9353574, 0.96171284, 0.9515886, 0.9435315, 0.9540685, 0.9406915, 0.9553712, 0.9455634, 0.9380038, 0.9609666, 0.9434399, 0.9481285, 0.95716125, 0.94942874, 0.9539802, 0.951528, 0.95563006, 0.9524324, 0.9369395, 0.94372433, 0.9326132, 0.94633776, 0.94361645, 0.9516833, 0.9420256, 0.95306355, 0.9486845, 0.9368358, 0.94664377, 0.9490699, 0.93841547, 0.9515787, 0.933779, 0.95314497, 0.9482549, 0.9444923, 0.95487165, 0.94934535, 0.9647183, 0.9506917, 0.95467263, 0.9463523, 0.95668715, 0.9573786, 0.9381983, 0.94477326, 0.95625347, 0.93240637, 0.95189977, 0.94989204, 0.9631806, 0.9605705, 0.9522231, 0.9577229, 0.9562101, 0.9511285, 0.9504164, 0.9613042, 0.945781, 0.94723064, 0.9451763, 0.9489066, 0.94605905, 0.9395835, 0.9568594, 0.94920594, 0.950953, 0.9567921, 0.9447384, 0.944879, 0.93039185, 0.93730015, 0.92789406, 0.94433683, 0.95506114, 0.9474101, 0.9484152, 0.940763, 0.92759746, 0.93396264, 0.9623768, 0.9196755, 0.9552045, 0.9495032, 0.9523739, 0.94115067, 0.93867606, 0.956507, 0.9446947, 0.95430416, 0.9456334, 0.9452295, 0.9306968, 0.92673904, 0.9572199, 0.9503822, 0.94201976, 0.9529445, 0.94205713, 0.95216656, 0.94388133, 0.95298845, 0.9331403, 0.9420035, 0.937711, 0.962465, 0.94330835, 0.9480424, 0.9533778, 0.9324873, 0.95068294, 0.9537471, 0.95419425, 0.94394463, 0.9329858, 0.9494837, 0.9365449, 0.9450037, 0.9588693, 0.96279734, 0.9554042, 0.9367694, 0.94698817, 0.96089035, 0.95689946] words).\n"
     ]
    }
   ],
   "source": [
    "# Test positive\n",
    "ls_sims_pos = []\n",
    "sy_sims_pos = []\n",
    "tested = 0\n",
    "\n",
    "for word in updated_utterances:\n",
    "    if len(updated_utterances[word]) > 1:\n",
    "        tested += 1\n",
    "        utterances_list = [\n",
    "            \" \".join(\n",
    "                [\n",
    "                    piece.replace(\"▁\", \"\")\n",
    "                    for piece in list(\n",
    "                        filter(\n",
    "                            lambda x: x != \"▁\",\n",
    "                            sp_model.EncodeAsPieces(utterance)\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            ) for utterance in updated_utterances[word]\n",
    "        ]\n",
    "\n",
    "        embeddings = model.encode(utterances_list)\n",
    "        similarities = cosine_similarity(embeddings)\n",
    "\n",
    "        if word.startswith(\"ls_\"):\n",
    "            ls_sims_pos.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "        if word.startswith(\"sy_\"):\n",
    "            sy_sims_pos.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "\n",
    "print(f\"Avg positive score (LS) = {sum(ls_sims_pos)/len(ls_sims_pos)} ({len(ls_sims_pos)} words).\")\n",
    "print(f\"Avg positive score (SY) = {sum(sy_sims_pos)/len(sy_sims_pos)} ({len(sy_sims_pos)} words).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53003300330033\n"
     ]
    }
   ],
   "source": [
    "n_utts = []\n",
    "\n",
    "for word in updated_utterances:\n",
    "    n_utts.append(len(updated_utterances[word]))\n",
    "\n",
    "print(sum(n_utts)/len(n_utts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested 1515/1515 words.\n",
      "Avg negative score = 0.9371212101218724.\n"
     ]
    }
   ],
   "source": [
    "# Test negative\n",
    "ls_sims_neg = []\n",
    "sy_sims_neg = []\n",
    "tested = 0\n",
    "n_negative_samples = 5\n",
    "\n",
    "ls_word_list = set([word for word in updated_utterances.keys() if word.startswith(\"ls_\")])\n",
    "sy_word_list = set([word for word in updated_utterances.keys() if word.startswith(\"sy_\")])\n",
    "\n",
    "for word in updated_utterances:\n",
    "    tested += 1\n",
    "\n",
    "    negative_samples = []\n",
    "    if word.startswith(\"ls_\"):\n",
    "        use_list = ls_word_list\n",
    "    else:\n",
    "        use_list = sy_word_list\n",
    "    sample = np.random.choice(list(use_list - {word}), size=(n_negative_samples))\n",
    "    for s in sample:\n",
    "        negative_samples.append(\n",
    "            np.random.choice(updated_utterances[s])\n",
    "        )\n",
    "\n",
    "    utterances_list = [\n",
    "        \" \".join(\n",
    "            [\n",
    "                piece.replace(\"▁\", \"\")\n",
    "                for piece in list(\n",
    "                    filter(\n",
    "                        lambda x: x != \"▁\",\n",
    "                        sp_model.EncodeAsPieces(utterance)\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        ) for utterance in negative_samples\n",
    "    ]\n",
    "    \n",
    "    embeddings = model.encode(utterances_list)\n",
    "    similarities = cosine_similarity(embeddings)\n",
    "\n",
    "    if word.startswith(\"ls_\"):\n",
    "        ls_sims_neg.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "    if word.startswith(\"sy_\"):\n",
    "        sy_sims_neg.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "\n",
    "print(f\"Avg negative score (LS) = {sum(ls_sims_neg)/len(ls_sims_neg)} ({len(ls_sims_neg)} words).\")\n",
    "print(f\"Avg negative score (SY) = {sum(sy_sims_neg)/len(sy_sims_neg)} ({len(sy_sims_neg)} words).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9440270919979656"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sy_sims)/len(sy_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_pairs = []\n",
    "rel_pairs = []\n",
    "\n",
    "with open(\"data/level_wise/level0/pairs.txt\", \"r\") as pairs_file:\n",
    "    for line in pairs_file.readlines()[1:]:\n",
    "        w1, w2, sim, rel = line.strip().split(\",\")\n",
    "        if sim:\n",
    "            sim_pairs.append((w1, w2, float(sim)))\n",
    "        if rel:\n",
    "            rel_pairs.append((w1, w2, float(rel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_vectors(word):\n",
    "    return model.encode(updated_utterances[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': {'librispeech': {'min': 7.805751624454922, 'avg': 8.578354383032712, 'max': 7.286344671214396}, 'synthetic': {'min': -1.3427713587253782, 'avg': -1.174001161944971, 'max': -3.791671346703998}}, 'errors': 0, 'trials': 1013}\n"
     ]
    }
   ],
   "source": [
    "scores = {\n",
    "    test_set: {\n",
    "        method: [] for method in [\"min\", \"max\", \"avg\", \"all\"]\n",
    "    } for test_set in [\"librispeech\", \"synthetic\"]\n",
    "}\n",
    "gold_standard = {\n",
    "    \"librispeech\": [],\n",
    "    \"synthetic\": []\n",
    "}\n",
    "trials = 0\n",
    "errors = 0\n",
    "\n",
    "for pair in rel_pairs:\n",
    "    try:\n",
    "        w1, w2, rel = pair\n",
    "\n",
    "        test_set = \"librispeech\" \\\n",
    "            if w1.startswith(\"ls_\") \\\n",
    "            else \"synthetic\"\n",
    "        w1.replace(\"ls_\", \"\").replace(\"sy_\", \"\")\n",
    "        w2.replace(\"ls_\", \"\").replace(\"sy_\", \"\")\n",
    "\n",
    "        w1_vectors = get_model_vectors(\n",
    "            w1\n",
    "        )\n",
    "        w2_vectors = get_model_vectors(\n",
    "            w2\n",
    "        )\n",
    "\n",
    "        similarities = [\n",
    "            cosine_similarity(i.reshape(1, -1), j.reshape(1, -1))\n",
    "            for i in w1_vectors\n",
    "            for j in w2_vectors\n",
    "        ]\n",
    "\n",
    "        scores[test_set][\"min\"].append(np.min(similarities))\n",
    "        scores[test_set][\"avg\"].append(np.mean(similarities))\n",
    "        scores[test_set][\"max\"].append(np.max(similarities))\n",
    "\n",
    "        gold_standard[test_set].append(rel)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        errors += 1\n",
    "    trials += 1\n",
    "\n",
    "print({\n",
    "    'score': {\n",
    "        test_set: {\n",
    "            var: pearsonr(\n",
    "                scores[test_set][var],\n",
    "                gold_standard[test_set]\n",
    "            )[0] * 100\n",
    "            for var in ['min', 'avg', 'max']\n",
    "        }\n",
    "        for test_set in ['librispeech', 'synthetic']\n",
    "    },\n",
    "    'errors': errors,\n",
    "    'trials': trials\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(\"models/comp_60k_250x5/level1/w2v_vs250_w5_lw.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ls_individual': []}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg positive score (LS) = 0.3204226875545289 (422 words).\n",
      "Avg positive score (SY) = 0.6264196041416614 (1034 words).\n"
     ]
    }
   ],
   "source": [
    "# Test positive\n",
    "ls_sims = []\n",
    "sy_sims = []\n",
    "tested = 0\n",
    "\n",
    "for word in updated_utterances:\n",
    "    if len(updated_utterances[word]) > 1:\n",
    "        tested += 1\n",
    "        embeddings = utterances.get_vectors_from_word(\n",
    "            word, sp_model, w2v_model\n",
    "        )[:, 0, :]\n",
    "\n",
    "        similarities = cosine_similarity(embeddings)\n",
    "\n",
    "        if word.startswith(\"ls_\"):\n",
    "            ls_sims.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "        if word.startswith(\"sy_\"):\n",
    "            sy_sims.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "\n",
    "print(f\"Avg positive score (LS) = {sum(ls_sims)/len(ls_sims)} ({len(ls_sims)} words).\")\n",
    "print(f\"Avg positive score (SY) = {sum(sy_sims)/len(sy_sims)} ({len(sy_sims)} words).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_from_utterance(\n",
    "            utterance,\n",
    "            sp_model: spm.SentencePieceProcessor,\n",
    "            w2v_model: Word2Vec\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Gets the embeddings of the given utterance.\n",
    "        \"\"\"\n",
    "        if utterance in w2v_model.wv.key_to_index.keys():\n",
    "            return w2v_model.wv[utterance].reshape(1, -1)\n",
    "        else:\n",
    "            pieces = list(\n",
    "                filter(\n",
    "                    lambda x: x != \"▁\",\n",
    "                    sp_model.EncodeAsPieces(utterance)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "\n",
    "            vectors = np.array([w2v_model.wv[unit] for unit in units])\n",
    "            return vectors.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 24\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m sample:\n\u001b[0;32m     20\u001b[0m     negative_samples\u001b[39m.\u001b[39mappend(\n\u001b[0;32m     21\u001b[0m         np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(updated_utterances[s])\n\u001b[0;32m     22\u001b[0m     )\n\u001b[1;32m---> 24\u001b[0m embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\n\u001b[0;32m     25\u001b[0m     get_vector_from_utterance(utterance, sp_model, w2v_model) \u001b[39mfor\u001b[39;00m utterance \u001b[39min\u001b[39;00m negative_samples\n\u001b[0;32m     26\u001b[0m ])\n\u001b[0;32m     28\u001b[0m similarities \u001b[39m=\u001b[39m cosine_similarity(embeddings)\n\u001b[0;32m     30\u001b[0m \u001b[39mif\u001b[39;00m word\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mls_\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Test negative\n",
    "ls_sims = []\n",
    "sy_sims = []\n",
    "tested = 0\n",
    "n_negative_samples = 5\n",
    "\n",
    "ls_word_list = set([word for word in updated_utterances.keys() if word.startswith(\"ls_\")])\n",
    "sy_word_list = set([word for word in updated_utterances.keys() if word.startswith(\"sy_\")])\n",
    "\n",
    "for word in updated_utterances:\n",
    "    tested += 1\n",
    "\n",
    "    negative_samples = []\n",
    "    if word.startswith(\"ls_\"):\n",
    "        use_list = ls_word_list\n",
    "    else:\n",
    "        use_list = sy_word_list\n",
    "    sample = np.random.choice(list(use_list - {word}), size=(n_negative_samples))\n",
    "    for s in sample:\n",
    "        negative_samples.append(\n",
    "            np.random.choice(updated_utterances[s])\n",
    "        )\n",
    "\n",
    "    embeddings = np.array([\n",
    "        get_vector_from_utterance(utterance, sp_model, w2v_model) for utterance in negative_samples\n",
    "    ])\n",
    "    \n",
    "    similarities = cosine_similarity(embeddings)\n",
    "\n",
    "    if word.startswith(\"ls_\"):\n",
    "        ls_sims.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "    if word.startswith(\"sy_\"):\n",
    "        sy_sims.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "\n",
    "print(f\"Avg negative score (LS) = {sum(ls_sims)/len(ls_sims)} ({len(ls_sims)} words).\")\n",
    "print(f\"Avg negative score (SY) = {sum(sy_sims)/len(sy_sims)} ({len(sy_sims)} words).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 1, 250)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 250)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
