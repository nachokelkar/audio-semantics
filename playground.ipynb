{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuBERT test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, HubertModel\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/hubert-large-ls960-ft\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = HubertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"C:\\\\Users\\\\mj115gl\\\\work_dir\\\\thesis\\\\audio-semantics\\\\data\\\\LibriSpeech\\\\dev-clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = processor(ds[\"train\"][2][\"audio\"][\"array\"], return_tensors=\"pt\").input_values  # Batch size 1\n",
    "logits = model(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.decode(predicted_ids[0])\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train\"][1][\"audio\"][\"array\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.last_hidden_state.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec, LineSentence\n",
    "# from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_fp = open(\"data/gtbrg_i.txt\", \"r\", encoding=\"utf-16\")\n",
    "sentences = LineSentence(line_fp)\n",
    "# line_fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_fp.seek(0)\n",
    "line_fp.readline()\n",
    "# line_fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 100\n",
    "window = 5\n",
    "w2v_model_tag = \"TEST\"\n",
    "W2V_MODEL_PATH = f\"models/w2v_vs{vector_size}_w{window}_{w2v_model_tag}.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    sentences,\n",
    "    window=window,\n",
    "    vector_size=vector_size,\n",
    "    min_count=0,\n",
    "    workers=4,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(W2V_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(w2v_model.wv.key_to_index.keys())[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsim_scores = []\n",
    "\n",
    "with open(\"data/wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\") as wordsim_fp:\n",
    "    for line in wordsim_fp.readlines():\n",
    "        scores = line.split(\"\\t\")\n",
    "        w1, w2 = scores[0], scores[1]\n",
    "        gold_score = float(scores[2])\n",
    "        wordsim_scores.append([w1, w2, gold_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_vocab = []\n",
    "gold_all = []\n",
    "preds_vocab = []\n",
    "preds_all = []\n",
    "tested = 0\n",
    "oov = 0\n",
    "\n",
    "for pairs in wordsim_scores:\n",
    "    w1, w2 = pairs[0].lower(), pairs[1].lower()\n",
    "    \n",
    "    try:\n",
    "        pred = w2v_model.wv.similarity(w1, w2)\n",
    "        preds_vocab.append(pred)\n",
    "        gold_vocab.append(pairs[2])\n",
    "        tested += 1\n",
    "    \n",
    "    except KeyError:\n",
    "        # if w1 not in w2v_model.wv.vocab.keys():\n",
    "        #     w1_units = sp.EncodeAsPieces(w1)[1:]\n",
    "        #     w1_vectors = np.array([w2v_model.wv[unit] for unit in w1_units])\n",
    "        #     w1_vector = w1_vectors.mean(axis=0)\n",
    "        # else:\n",
    "        #     w1_vector = w2v_model.wv[w1]\n",
    "        # if w2 not in w2v_model.wv.vocab.keys():\n",
    "        #     w2_units = sp.EncodeAsPieces(w2)[1:]\n",
    "        #     w2_vectors = np.array([w2v_model.wv[unit] for unit in w2_units])\n",
    "        #     w2_vector = w2_vectors.mean(axis=0)\n",
    "        # else:\n",
    "        #     w2_vector = w2v_model.wv[w2]\n",
    "\n",
    "        # pred = cosine_similarity(w1_vector.reshape(1, -1), w2_vector.reshape(1, -1))\n",
    "        oov += 1\n",
    "    \n",
    "    preds_all.append(pred)\n",
    "    gold_all.append(pairs[2])\n",
    "        \n",
    "\n",
    "print(spearmanr(preds_vocab, gold_vocab)[0], f\", tested {tested}/{len(wordsim_scores)} pairs\")\n",
    "print(spearmanr(preds_all, gold_all)[0], f\", including OOV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simlex_scores = []\n",
    "\n",
    "with open(\"data/SimLex-999/SimLex-999.txt\") as simlex_fp:\n",
    "    for line in simlex_fp.readlines()[1:]:\n",
    "        scores = line.split(\"\\t\")\n",
    "        w1, w2 = scores[0], scores[1]\n",
    "        gold_score = float(scores[3])\n",
    "        simlex_scores.append([w1, w2, gold_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_vocab = []\n",
    "gold_all = []\n",
    "preds_vocab = []\n",
    "preds_all = []\n",
    "tested = 0\n",
    "oov = 0\n",
    "\n",
    "for pairs in simlex_scores:\n",
    "    w1, w2 = pairs[0].lower(), pairs[1].lower()\n",
    "    \n",
    "    try:\n",
    "        pred = w2v_model.wv.similarity(w1, w2)\n",
    "        preds_vocab.append(pred)\n",
    "        gold_vocab.append(pairs[2])\n",
    "        tested += 1\n",
    "    \n",
    "    except KeyError:\n",
    "        # if w1 not in w2v_model.wv.vocab.keys():\n",
    "        #     w1_units = sp.EncodeAsPieces(w1)[1:]\n",
    "        #     w1_vectors = np.array([w2v_model.wv[unit] for unit in w1_units])\n",
    "        #     w1_vector = w1_vectors.mean(axis=0)\n",
    "        # else:\n",
    "        #     w1_vector = w2v_model.wv[w1]\n",
    "        # if w2 not in w2v_model.wv.vocab.keys():\n",
    "        #     w2_units = sp.EncodeAsPieces(w2)[1:]\n",
    "        #     w2_vectors = np.array([w2v_model.wv[unit] for unit in w2_units])\n",
    "        #     w2_vector = w2_vectors.mean(axis=0)\n",
    "        # else:\n",
    "        #     w2_vector = w2v_model.wv[w2]\n",
    "\n",
    "        # pred = cosine_similarity(w1_vector.reshape(1, -1), w2_vector.reshape(1, -1))\n",
    "        oov += 1\n",
    "    \n",
    "    preds_all.append(pred)\n",
    "    gold_all.append(pairs[2])\n",
    "        \n",
    "\n",
    "print(spearmanr(preds_vocab, gold_vocab)[0], f\", tested {tested}/{len(simlex_scores)} pairs\")\n",
    "print(spearmanr(preds_all, gold_all)[0], f\", including OOV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = []\n",
    "words = list(w2v_model.wv.key_to_index.keys())\n",
    "\n",
    "tqdm_iterator = tqdm(range(0, 99, 1))\n",
    "\n",
    "for threshold in tqdm_iterator:\n",
    "    threshold = threshold / 100\n",
    "    word_to_cluster = dict()  # Stores map from word to cluster\n",
    "    cluster_to_words = dict()  # Stores map from cluster to words\n",
    "    cluster_idx = 0  # Counter\n",
    "\n",
    "    for word in words:\n",
    "        tqdm_iterator.set_postfix({\"Word\": word})\n",
    "        # Check if word has already been clustered\n",
    "        if word not in word_to_cluster.keys():\n",
    "            # Create new cluster\n",
    "            cluster_idx += 1\n",
    "            # cluster_key = chr(0x0020 + cluster_idx)\n",
    "            cluster_key = cluster_idx\n",
    "\n",
    "            # Add new word to cluster\n",
    "            tqdm_iterator.set_postfix({\"status\" : \"adding new word to cluster\"})\n",
    "            cluster_to_words[cluster_key] = [word]\n",
    "            word_to_cluster[word] = cluster_key\n",
    "            \n",
    "            # Add all similar words\n",
    "            tqdm_iterator.set_postfix({\"status\" : \"adding all words\"})\n",
    "            for similar_word, score in w2v_model.wv.most_similar(word, topn=50):\n",
    "                tqdm_iterator.set_postfix({\"status\" : \"checked 50 words\"})\n",
    "                if score > threshold:\n",
    "                    cluster_to_words[cluster_key].append(similar_word)\n",
    "                    word_to_cluster[similar_word] = cluster_key\n",
    "    \n",
    "    n_clusters.append(len(cluster_to_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = {\n",
    "    i: key for i, key in enumerate(ascii_letters)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_key = {}\n",
    "key_to_word = {}\n",
    "\n",
    "with open(\"data/quantized/dev-gold.csv\", \"r\") as key_file:\n",
    "    for line in key_file.readlines()[1:]:\n",
    "        dataset, key, _, word = line.strip().split(\",\")\n",
    "        if word not in word_to_key:\n",
    "            word_to_key[word] = {\n",
    "                'librispeech': [],\n",
    "                'synthetic': []\n",
    "            }\n",
    "        word_to_key[word][dataset].append(key)\n",
    "        key_to_word[key] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = {}\n",
    "\n",
    "for dataset in [\"librispeech\", \"synthetic\"]:\n",
    "    with open(f\"data/quantized/semantic/dev/{dataset}/quantized_outputs.txt\", \"r\") as utterance_file:\n",
    "        for line in utterance_file.readlines():\n",
    "            key, seq = line.strip().split(\"\\t\")\n",
    "            utterance = seq.split(\",\")[1:]\n",
    "\n",
    "            key = \"ls_\" + key_to_word[key] if dataset == \"librispeech\" else \"sy_\" + key_to_word[key]\n",
    "            if key not in utterances:\n",
    "                utterances[key] = []\n",
    "\n",
    "            utterances[key].append(\n",
    "                \"\".join(\n",
    "                    [letters[int(v)] for i, v in enumerate(utterance)]\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/level_wise/level0/utterances_original.txt\", \"w+\", encoding=\"utf-8\") as ufp:\n",
    "    for word in utterances:\n",
    "        for utterance in utterances[word]:\n",
    "            ufp.write(word + \"\\t\" + utterance + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_pairs = []\n",
    "rel_pairs = []\n",
    "\n",
    "with open(\"data/quantized/dev-pairs.csv\", \"r\") as pairs_file:\n",
    "    for line in pairs_file.readlines()[1:]:\n",
    "        dataset, _, w1, w2, sim, rel = line.strip().split(\",\")\n",
    "        if sim:\n",
    "            sim_pairs.append((dataset, w1, w2, float(sim)))\n",
    "        if rel:\n",
    "            rel_pairs.append((dataset, w1, w2, float(rel)))\n",
    "\n",
    "with open(\"data/level_wise/level0/pairs.txt\", \"w+\", encoding=\"utf-8\") as pairs_fp:\n",
    "    for pair in sim_pairs:\n",
    "        dataset, w1, w2, score = pair\n",
    "        pairs_fp.write(\n",
    "            (\"ls_\" + w1 if dataset == \"librispeech\" else \"sy_\" + w1) + \",\" +\n",
    "            (\"ls_\" + w2 if dataset == \"librispeech\" else \"sy_\" + w2) + \",\" +\n",
    "            str(score) + \",\" + \"\\n\"\n",
    "        )\n",
    "    for pair in rel_pairs:\n",
    "        dataset, w1, w2, score = pair\n",
    "        pairs_fp.write(\n",
    "            (\"ls_\" + w1 if dataset == \"librispeech\" else \"sy_\" + w1) + \",\" +\n",
    "            (\"ls_\" + w2 if dataset == \"librispeech\" else \"sy_\" + w2) + \",\" +\n",
    "            \",\" + str(score) + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_model = spm.SentencePieceProcessor()\n",
    "\n",
    "sp_model.Load(\"models/original_60k_250x1/level1/unigram_vs60000_lw.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_corpus = []\n",
    "\n",
    "with open(\"data/level_wise/level0/dev_corpus_original.txt\", \"r\", encoding=\"utf-8\") as ocfp:\n",
    "    for line in ocfp.readlines():\n",
    "        original_corpus.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/original_60k_250x1/level1/dev_corpus_ft.txt\", \"w+\", encoding=\"utf-8\") as ncfp:\n",
    "    for line in original_corpus:\n",
    "        pieces = list(\n",
    "            filter(\n",
    "                lambda x: x != \"▁\",\n",
    "                sp_model.EncodeAsPieces(line)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "        \n",
    "        ncfp.write(\" \".join(units) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = fasttext.train_unsupervised(\n",
    "    \"data/original_60k_250x1/level1/corpus_ft.txt\",\n",
    "    \"cbow\",\n",
    "    dim=250,\n",
    "    thread=4,\n",
    "    epoch=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.save_model(\"models/fasttext_cbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_model = fasttext.load_model(\"models/fasttext_cbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from levelwise_model.test_bench import LSTestBench\n",
    "from levelwise_model.utterances import WordToUtteranceMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_mapping = WordToUtteranceMapping(map_file=\"data/level_wise/level0/utterances_original.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bench = LSTestBench(scores_file=\"data/level_wise/level0/pairs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bench.ft_score_and_save(ft_model=ft_model, utterances=utterance_mapping, results_file=\"results/ft_cbow\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\mj115gl\\work_dir\\thesis\\audio-semantics\\models\\db_final. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\mj115gl\\work_dir\\thesis\\audio-semantics\\models\\db_final were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('C:\\\\Users\\\\mj115gl\\\\work_dir\\\\thesis\\\\audio-semantics\\\\models\\\\db_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.Load(\"models/original_60k_250x1/level1/unigram_vs60000_lw.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from levelwise_model.utterances import WordToUtteranceMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = WordToUtteranceMapping(utterances=\"data/level_wise/level0/utterances_original.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_utterances = {}\n",
    "\n",
    "for word in utterances.utterances:\n",
    "    updated_utterances[word] = []\n",
    "    for utt in utterances.utterances[word]:\n",
    "        pieces = list(\n",
    "            filter(\n",
    "                lambda x: x != \"▁\",\n",
    "                sp_model.EncodeAsPieces(utt)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "        updated_utterances[word].append(\n",
    "            \" \".join(units)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_utterances = WordToUtteranceMapping(utterances=updated_utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from levelwise_model.test_bench import LSTestBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bench = LSTestBench(\"data/level_wise/level0/pairs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_word_vec_fn(utt):\n",
    "    pieces = list(\n",
    "        filter(\n",
    "            lambda x: x != \"▁\",\n",
    "            sp_model.EncodeAsPieces(utt)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "    return model.encode(\" \".join(units)).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'similarities': {'score': {'librispeech': 8.253180987037307,\n",
       "   'synthetic': -0.004998110381584656},\n",
       "  'errors': 0,\n",
       "  'trials': 1013},\n",
       " 'xw-abx-test': {'ABX Result': {'librispeech': 0.581140350877193,\n",
       "   'synthetic': 0.49278846153846156,\n",
       "   'combined': 0.5240683229813664}},\n",
       " 'sw-abx-test': {'Same-word ABX Result': {'librispeech': 0.66,\n",
       "   'synthetic': 0.71,\n",
       "   'mixed': 0.6457142857142857}}}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bench.run_suite(\n",
    "    utterances=updated_utterances,\n",
    "    word_vec_fn=db_word_vec_fn,\n",
    "    results_file=\"results/db-50.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test positive\n",
    "# ls_sims_pos = []\n",
    "# sy_sims_pos = []\n",
    "# tested = 0\n",
    "\n",
    "# for word in updated_utterances:\n",
    "#     if len(updated_utterances[word]) > 1:\n",
    "#         tested += 1\n",
    "#         utterances_list = [\n",
    "#             \" \".join(\n",
    "#                 [\n",
    "#                     piece.replace(\"▁\", \"\")\n",
    "#                     for piece in list(\n",
    "#                         filter(\n",
    "#                             lambda x: x != \"▁\",\n",
    "#                             sp_model.EncodeAsPieces(utterance)\n",
    "#                         )\n",
    "#                     )\n",
    "#                 ]\n",
    "#             ) for utterance in updated_utterances[word]\n",
    "#         ]\n",
    "\n",
    "#         embeddings = model.encode(utterances_list)\n",
    "#         similarities = cosine_similarity(embeddings)\n",
    "\n",
    "#         if word.startswith(\"ls_\"):\n",
    "#             ls_sims_pos.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "#         if word.startswith(\"sy_\"):\n",
    "#             sy_sims_pos.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "\n",
    "# print(f\"Avg positive score (LS) = {sum(ls_sims_pos)/len(ls_sims_pos)} ({len(ls_sims_pos)} words).\")\n",
    "# print(f\"Avg positive score (SY) = {sum(sy_sims_pos)/len(sy_sims_pos)} ({len(sy_sims_pos)} words).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_utts = []\n",
    "\n",
    "# for word in updated_utterances:\n",
    "#     n_utts.append(len(updated_utterances[word]))\n",
    "\n",
    "# print(sum(n_utts)/len(n_utts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test negative\n",
    "# ls_sims_neg = []\n",
    "# sy_sims_neg = []\n",
    "# tested = 0\n",
    "# n_negative_samples = 5\n",
    "\n",
    "# ls_word_list = set([word for word in updated_utterances.keys() if word.startswith(\"ls_\")])\n",
    "# sy_word_list = set([word for word in updated_utterances.keys() if word.startswith(\"sy_\")])\n",
    "\n",
    "# for word in updated_utterances:\n",
    "#     tested += 1\n",
    "\n",
    "#     negative_samples = []\n",
    "#     if word.startswith(\"ls_\"):\n",
    "#         use_list = ls_word_list\n",
    "#     else:\n",
    "#         use_list = sy_word_list\n",
    "#     sample = np.random.choice(list(use_list - {word}), size=(n_negative_samples))\n",
    "#     for s in sample:\n",
    "#         negative_samples.append(\n",
    "#             np.random.choice(updated_utterances[s])\n",
    "#         )\n",
    "\n",
    "#     utterances_list = [\n",
    "#         \" \".join(\n",
    "#             [\n",
    "#                 piece.replace(\"▁\", \"\")\n",
    "#                 for piece in list(\n",
    "#                     filter(\n",
    "#                         lambda x: x != \"▁\",\n",
    "#                         sp_model.EncodeAsPieces(utterance)\n",
    "#                     )\n",
    "#                 )\n",
    "#             ]\n",
    "#         ) for utterance in negative_samples\n",
    "#     ]\n",
    "    \n",
    "#     embeddings = model.encode(utterances_list)\n",
    "#     similarities = cosine_similarity(embeddings)\n",
    "\n",
    "#     if word.startswith(\"ls_\"):\n",
    "#         ls_sims_neg.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "#     if word.startswith(\"sy_\"):\n",
    "#         sy_sims_neg.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "\n",
    "# print(f\"Avg negative score (LS) = {sum(ls_sims_neg)/len(ls_sims_neg)} ({len(ls_sims_neg)} words).\")\n",
    "# print(f\"Avg negative score (SY) = {sum(sy_sims_neg)/len(sy_sims_neg)} ({len(sy_sims_neg)} words).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(sy_sims)/len(sy_sims)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABX Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(\"models/original_60k_250x1/level1/w2v_vs250_w1_lw.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wv_word_vec_fn(utt):\n",
    "    if utt in w2v_model.wv.key_to_index.keys():\n",
    "        return w2v_model.wv[utt].reshape(1, -1)\n",
    "    else:\n",
    "        pieces = list(\n",
    "            filter(\n",
    "                lambda x: x != \"▁\",\n",
    "                sp_model.EncodeAsPieces(utt)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "\n",
    "        vectors = np.array([w2v_model.wv[unit] for unit in units])\n",
    "        return vectors.mean(axis=0).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'similarities': {'score': {'librispeech': 9.01030729942617,\n",
       "   'synthetic': 1.028987880071217},\n",
       "  'errors': 0,\n",
       "  'trials': 1013},\n",
       " 'xw-abx-test': {'ABX Result': {'librispeech': 0.49780701754385964,\n",
       "   'synthetic': 0.46033653846153844,\n",
       "   'combined': 0.47360248447204967}},\n",
       " 'sw-abx-test': {'Same-word ABX Result': {'librispeech': 0.9057142857142857,\n",
       "   'synthetic': 0.9571428571428572,\n",
       "   'mixed': 0.8442857142857143}}}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bench.run_suite(\n",
    "    utterances=utterances,\n",
    "    word_vec_fn=wv_word_vec_fn,\n",
    "    results_file=\"results/w2v.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = 0\n",
    "# total = 0\n",
    "# use_noise_for_x = True\n",
    "\n",
    "# def get_correct_word(pair, word):\n",
    "#     if pair[0] == word:\n",
    "#         return pair[1]\n",
    "#     return pair[1]\n",
    "\n",
    "# for word_a in updated_utterances:\n",
    "#     # Sort words by similarity\n",
    "#     similar_words = sorted(\n",
    "#         # Get only pairs containing word A\n",
    "#         filter(\n",
    "#             lambda x: x[0] == word_a or x[1] == word_a,\n",
    "#             rel_pairs\n",
    "#         ),\n",
    "#         key=lambda x: x[2],\n",
    "#         reverse=True\n",
    "#     )\n",
    "\n",
    "#     similar_words = list(\n",
    "#         map(\n",
    "#             lambda x: get_correct_word(x, word_a),\n",
    "#             similar_words\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     for utt_a in updated_utterances[word_a]:\n",
    "#         word_b = similar_words[0]\n",
    "#         utt_b = np.random.choice(\n",
    "#             updated_utterances[word_b]\n",
    "#         )\n",
    "\n",
    "#         if not use_noise_for_x:\n",
    "#             word_x = similar_words[-1]\n",
    "#             utt_x = np.random.choice(\n",
    "#                 updated_utterances[word_x]\n",
    "#             )\n",
    "\n",
    "#         else:\n",
    "#             utt_x = np.random.choice(\n",
    "#                 list(ascii_letters),\n",
    "#                 len(utt_a),\n",
    "#                 replace=True\n",
    "#             )\n",
    "#             utt_x = \"\".join(utt_x)\n",
    "\n",
    "#         v_a = word_vec_fn(utt_a).reshape(1, -1)\n",
    "#         v_b = word_vec_fn(utt_b).reshape(1, -1)\n",
    "#         v_x = word_vec_fn(utt_x).reshape(1, -1)\n",
    "\n",
    "#         if cosine_similarity(v_a, v_b) > cosine_similarity(v_a, v_x):\n",
    "#             preds += 1\n",
    "#         total += 1\n",
    "\n",
    "# print({\"ABX Result\": preds/total})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ABX Result': 0.5062111801242236}\n"
     ]
    }
   ],
   "source": [
    "# preds = 0\n",
    "# total = 0\n",
    "# use_noise_for_x = False\n",
    "\n",
    "# def get_correct_word(pair, word):\n",
    "#     if pair[0] == word:\n",
    "#         return pair[1]\n",
    "#     return pair[1]\n",
    "\n",
    "# with open(\"results/abx_test_db.txt\", \"w+\") as abx_results_file:\n",
    "#     abx_results_file.write(\"A,B,X,sim(AB),sim(AX),chosen\\n\")\n",
    "#     for word_a in updated_utterances:\n",
    "#         # Sort words by similarity\n",
    "#         similar_words = sorted(\n",
    "#             # Get only pairs containing word A\n",
    "#             filter(\n",
    "#                 lambda x: x[0] == word_a or x[1] == word_a,\n",
    "#                 rel_pairs\n",
    "#             ),\n",
    "#             key=lambda x: x[2],\n",
    "#             reverse=True\n",
    "#         )\n",
    "\n",
    "#         similar_words = list(\n",
    "#             map(\n",
    "#                 lambda x: get_correct_word(x, word_a),\n",
    "#                 similar_words\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#         for utt_a in updated_utterances[word_a]:\n",
    "#             word_b = similar_words[0]\n",
    "#             utt_b = np.random.choice(\n",
    "#                 updated_utterances[word_b]\n",
    "#             )\n",
    "\n",
    "#             if not use_noise_for_x:\n",
    "#                 word_x = similar_words[-1]\n",
    "#                 utt_x = np.random.choice(\n",
    "#                     updated_utterances[word_x]\n",
    "#                 )\n",
    "\n",
    "#             else:\n",
    "#                 word_x = \"noise\"\n",
    "#                 utt_x = np.random.choice(\n",
    "#                     list(ascii_letters),\n",
    "#                     len(utt_a),\n",
    "#                     replace=True\n",
    "#                 )\n",
    "#                 utt_x = \"\".join(utt_x)\n",
    "\n",
    "#             if len(set([word_a, word_b, word_x])) == 3:\n",
    "#                 v_a = word_vec_fn(utt_a).reshape(1, -1)\n",
    "#                 v_b = word_vec_fn(utt_b).reshape(1, -1)\n",
    "#                 v_x = word_vec_fn(utt_x).reshape(1, -1)\n",
    "\n",
    "#                 sim_ab = cosine_similarity(v_a, v_b)\n",
    "#                 sim_ax = cosine_similarity(v_a, v_x)\n",
    "\n",
    "#                 chosen = \"X\"\n",
    "#                 if sim_ab > sim_ax:\n",
    "#                     chosen = \"B\"\n",
    "#                     preds += 1\n",
    "#                 total += 1\n",
    "\n",
    "#                 abx_results_file.write(\",\".join([word_a, word_b, word_x, str(sim_ab), str(sim_ax), chosen]) + \"\\n\")\n",
    "\n",
    "# print({\"ABX Result\": preds/total})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(set([word_a, word_b, word_x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_pairs = []\n",
    "# rel_pairs = []\n",
    "\n",
    "# with open(\"data/level_wise/level0/pairs.txt\", \"r\") as pairs_file:\n",
    "#     for line in pairs_file.readlines()[1:]:\n",
    "#         w1, w2, sim, rel = line.strip().split(\",\")\n",
    "#         if sim:\n",
    "#             sim_pairs.append((w1, w2, float(sim)))\n",
    "#         if rel:\n",
    "#             rel_pairs.append((w1, w2, float(rel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse vector baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.Load(\"models/original_60k_250x1/level1/unigram_vs60000_lw.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sv_word_vec_fn(word):\n",
    "    vector = np.zeros(60000)\n",
    "    tokens = sp_model.EncodeAsIds(word)\n",
    "    vector[tokens] = 1\n",
    "    return vector.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from levelwise_model.test_bench import LSTestBench\n",
    "from levelwise_model.utterances import WordToUtteranceMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bench = LSTestBench(\"data/level_wise/level0/pairs.txt\")\n",
    "utterances = WordToUtteranceMapping(utterances=\"data/level_wise/level0/utterances_original.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'similarities': {'score': {'librispeech': -6.103752173089786,\n",
       "   'synthetic': 0.2691688253264706},\n",
       "  'errors': 0,\n",
       "  'trials': 1013},\n",
       " 'xw-abx-test': {'ABX Result': {'librispeech': 0.40131578947368424,\n",
       "   'synthetic': 0.18389423076923078,\n",
       "   'combined': 0.2608695652173913}},\n",
       " 'sw-abx-test': {'Same-word ABX Result': {'librispeech': 0.564,\n",
       "   'synthetic': 0.454,\n",
       "   'mixed': 0.448}}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bench.run_suite(\n",
    "    utterances=utterances,\n",
    "    word_vec_fn=sv_word_vec_fn,\n",
    "    results_file=\"results/baseline_sparse_vectors.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate ABX test pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from levelwise_model.utterances import WordToUtteranceMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = WordToUtteranceMapping(map_file=\"data/level_wise/level0/utterances_original.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(a, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_utterances = []\n",
    "for word in utterances.utterances:\n",
    "    for utt in utterances.utterances[word]:\n",
    "        all_utterances.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/level_wise/level0/abx_sw_original.txt\", \"w+\", encoding=\"utf-8\") as abx_file:\n",
    "    for word_a in utterances.utterances:\n",
    "        for utt_a in utterances.utterances[word_a]:\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
