{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuBERT test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, HubertModel\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/hubert-large-ls960-ft\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = HubertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"C:\\\\Users\\\\mj115gl\\\\work_dir\\\\thesis\\\\audio-semantics\\\\data\\\\LibriSpeech\\\\dev-clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = processor(ds[\"train\"][2][\"audio\"][\"array\"], return_tensors=\"pt\").input_values  # Batch size 1\n",
    "logits = model(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.decode(predicted_ids[0])\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train\"][1][\"audio\"][\"array\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.last_hidden_state.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec, LineSentence\n",
    "# from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_fp = open(\"data/gtbrg_i.txt\", \"r\", encoding=\"utf-16\")\n",
    "sentences = LineSentence(line_fp)\n",
    "# line_fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_fp.seek(0)\n",
    "line_fp.readline()\n",
    "# line_fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 100\n",
    "window = 5\n",
    "w2v_model_tag = \"TEST\"\n",
    "W2V_MODEL_PATH = f\"models/w2v_vs{vector_size}_w{window}_{w2v_model_tag}.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    sentences,\n",
    "    window=window,\n",
    "    vector_size=vector_size,\n",
    "    min_count=0,\n",
    "    workers=4,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(W2V_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(w2v_model.wv.key_to_index.keys())[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsim_scores = []\n",
    "\n",
    "with open(\"data/wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\") as wordsim_fp:\n",
    "    for line in wordsim_fp.readlines():\n",
    "        scores = line.split(\"\\t\")\n",
    "        w1, w2 = scores[0], scores[1]\n",
    "        gold_score = float(scores[2])\n",
    "        wordsim_scores.append([w1, w2, gold_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_vocab = []\n",
    "gold_all = []\n",
    "preds_vocab = []\n",
    "preds_all = []\n",
    "tested = 0\n",
    "oov = 0\n",
    "\n",
    "for pairs in wordsim_scores:\n",
    "    w1, w2 = pairs[0].lower(), pairs[1].lower()\n",
    "    \n",
    "    try:\n",
    "        pred = w2v_model.wv.similarity(w1, w2)\n",
    "        preds_vocab.append(pred)\n",
    "        gold_vocab.append(pairs[2])\n",
    "        tested += 1\n",
    "    \n",
    "    except KeyError:\n",
    "        # if w1 not in w2v_model.wv.vocab.keys():\n",
    "        #     w1_units = sp.EncodeAsPieces(w1)[1:]\n",
    "        #     w1_vectors = np.array([w2v_model.wv[unit] for unit in w1_units])\n",
    "        #     w1_vector = w1_vectors.mean(axis=0)\n",
    "        # else:\n",
    "        #     w1_vector = w2v_model.wv[w1]\n",
    "        # if w2 not in w2v_model.wv.vocab.keys():\n",
    "        #     w2_units = sp.EncodeAsPieces(w2)[1:]\n",
    "        #     w2_vectors = np.array([w2v_model.wv[unit] for unit in w2_units])\n",
    "        #     w2_vector = w2_vectors.mean(axis=0)\n",
    "        # else:\n",
    "        #     w2_vector = w2v_model.wv[w2]\n",
    "\n",
    "        # pred = cosine_similarity(w1_vector.reshape(1, -1), w2_vector.reshape(1, -1))\n",
    "        oov += 1\n",
    "    \n",
    "    preds_all.append(pred)\n",
    "    gold_all.append(pairs[2])\n",
    "        \n",
    "\n",
    "print(spearmanr(preds_vocab, gold_vocab)[0], f\", tested {tested}/{len(wordsim_scores)} pairs\")\n",
    "print(spearmanr(preds_all, gold_all)[0], f\", including OOV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simlex_scores = []\n",
    "\n",
    "with open(\"data/SimLex-999/SimLex-999.txt\") as simlex_fp:\n",
    "    for line in simlex_fp.readlines()[1:]:\n",
    "        scores = line.split(\"\\t\")\n",
    "        w1, w2 = scores[0], scores[1]\n",
    "        gold_score = float(scores[3])\n",
    "        simlex_scores.append([w1, w2, gold_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_vocab = []\n",
    "gold_all = []\n",
    "preds_vocab = []\n",
    "preds_all = []\n",
    "tested = 0\n",
    "oov = 0\n",
    "\n",
    "for pairs in simlex_scores:\n",
    "    w1, w2 = pairs[0].lower(), pairs[1].lower()\n",
    "    \n",
    "    try:\n",
    "        pred = w2v_model.wv.similarity(w1, w2)\n",
    "        preds_vocab.append(pred)\n",
    "        gold_vocab.append(pairs[2])\n",
    "        tested += 1\n",
    "    \n",
    "    except KeyError:\n",
    "        # if w1 not in w2v_model.wv.vocab.keys():\n",
    "        #     w1_units = sp.EncodeAsPieces(w1)[1:]\n",
    "        #     w1_vectors = np.array([w2v_model.wv[unit] for unit in w1_units])\n",
    "        #     w1_vector = w1_vectors.mean(axis=0)\n",
    "        # else:\n",
    "        #     w1_vector = w2v_model.wv[w1]\n",
    "        # if w2 not in w2v_model.wv.vocab.keys():\n",
    "        #     w2_units = sp.EncodeAsPieces(w2)[1:]\n",
    "        #     w2_vectors = np.array([w2v_model.wv[unit] for unit in w2_units])\n",
    "        #     w2_vector = w2_vectors.mean(axis=0)\n",
    "        # else:\n",
    "        #     w2_vector = w2v_model.wv[w2]\n",
    "\n",
    "        # pred = cosine_similarity(w1_vector.reshape(1, -1), w2_vector.reshape(1, -1))\n",
    "        oov += 1\n",
    "    \n",
    "    preds_all.append(pred)\n",
    "    gold_all.append(pairs[2])\n",
    "        \n",
    "\n",
    "print(spearmanr(preds_vocab, gold_vocab)[0], f\", tested {tested}/{len(simlex_scores)} pairs\")\n",
    "print(spearmanr(preds_all, gold_all)[0], f\", including OOV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = []\n",
    "words = list(w2v_model.wv.key_to_index.keys())\n",
    "\n",
    "tqdm_iterator = tqdm(range(0, 99, 1))\n",
    "\n",
    "for threshold in tqdm_iterator:\n",
    "    threshold = threshold / 100\n",
    "    word_to_cluster = dict()  # Stores map from word to cluster\n",
    "    cluster_to_words = dict()  # Stores map from cluster to words\n",
    "    cluster_idx = 0  # Counter\n",
    "\n",
    "    for word in words:\n",
    "        tqdm_iterator.set_postfix({\"Word\": word})\n",
    "        # Check if word has already been clustered\n",
    "        if word not in word_to_cluster.keys():\n",
    "            # Create new cluster\n",
    "            cluster_idx += 1\n",
    "            # cluster_key = chr(0x0020 + cluster_idx)\n",
    "            cluster_key = cluster_idx\n",
    "\n",
    "            # Add new word to cluster\n",
    "            tqdm_iterator.set_postfix({\"status\" : \"adding new word to cluster\"})\n",
    "            cluster_to_words[cluster_key] = [word]\n",
    "            word_to_cluster[word] = cluster_key\n",
    "            \n",
    "            # Add all similar words\n",
    "            tqdm_iterator.set_postfix({\"status\" : \"adding all words\"})\n",
    "            for similar_word, score in w2v_model.wv.most_similar(word, topn=50):\n",
    "                tqdm_iterator.set_postfix({\"status\" : \"checked 50 words\"})\n",
    "                if score > threshold:\n",
    "                    cluster_to_words[cluster_key].append(similar_word)\n",
    "                    word_to_cluster[similar_word] = cluster_key\n",
    "    \n",
    "    n_clusters.append(len(cluster_to_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = {\n",
    "    i: key for i, key in enumerate(ascii_letters)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_key = {}\n",
    "key_to_word = {}\n",
    "\n",
    "with open(\"data/quantized/dev-gold.csv\", \"r\") as key_file:\n",
    "    for line in key_file.readlines()[1:]:\n",
    "        dataset, key, _, word = line.strip().split(\",\")\n",
    "        if word not in word_to_key:\n",
    "            word_to_key[word] = {\n",
    "                'librispeech': [],\n",
    "                'synthetic': []\n",
    "            }\n",
    "        word_to_key[word][dataset].append(key)\n",
    "        key_to_word[key] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = {}\n",
    "\n",
    "for dataset in [\"librispeech\", \"synthetic\"]:\n",
    "    with open(f\"data/quantized/semantic/dev/{dataset}/quantized_outputs.txt\", \"r\") as utterance_file:\n",
    "        for line in utterance_file.readlines():\n",
    "            key, seq = line.strip().split(\"\\t\")\n",
    "            utterance = seq.split(\",\")[1:]\n",
    "\n",
    "            key = \"ls_\" + key_to_word[key] if dataset == \"librispeech\" else \"sy_\" + key_to_word[key]\n",
    "            if key not in utterances:\n",
    "                utterances[key] = []\n",
    "\n",
    "            utterances[key].append(\n",
    "                \"\".join(\n",
    "                    [letters[int(v)] for i, v in enumerate(utterance)]\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/level_wise/level0/utterances_original.txt\", \"w+\", encoding=\"utf-8\") as ufp:\n",
    "    for word in utterances:\n",
    "        for utterance in utterances[word]:\n",
    "            ufp.write(word + \"\\t\" + utterance + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_pairs = []\n",
    "rel_pairs = []\n",
    "\n",
    "with open(\"data/quantized/dev-pairs.csv\", \"r\") as pairs_file:\n",
    "    for line in pairs_file.readlines()[1:]:\n",
    "        dataset, _, w1, w2, sim, rel = line.strip().split(\",\")\n",
    "        if sim:\n",
    "            sim_pairs.append((dataset, w1, w2, float(sim)))\n",
    "        if rel:\n",
    "            rel_pairs.append((dataset, w1, w2, float(rel)))\n",
    "\n",
    "with open(\"data/level_wise/level0/pairs.txt\", \"w+\", encoding=\"utf-8\") as pairs_fp:\n",
    "    for pair in sim_pairs:\n",
    "        dataset, w1, w2, score = pair\n",
    "        pairs_fp.write(\n",
    "            (\"ls_\" + w1 if dataset == \"librispeech\" else \"sy_\" + w1) + \",\" +\n",
    "            (\"ls_\" + w2 if dataset == \"librispeech\" else \"sy_\" + w2) + \",\" +\n",
    "            str(score) + \",\" + \"\\n\"\n",
    "        )\n",
    "    for pair in rel_pairs:\n",
    "        dataset, w1, w2, score = pair\n",
    "        pairs_fp.write(\n",
    "            (\"ls_\" + w1 if dataset == \"librispeech\" else \"sy_\" + w1) + \",\" +\n",
    "            (\"ls_\" + w2 if dataset == \"librispeech\" else \"sy_\" + w2) + \",\" +\n",
    "            \",\" + str(score) + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model = spm.SentencePieceProcessor()\n",
    "\n",
    "sp_model.Load(\"models/original_60k_250x1/level1/unigram_vs60000_lw.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_corpus = []\n",
    "\n",
    "with open(\"data/level_wise/level0/dev_corpus_original.txt\", \"r\", encoding=\"utf-8\") as ocfp:\n",
    "    for line in ocfp.readlines():\n",
    "        original_corpus.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/original_60k_250x1/level1/dev_corpus_ft.txt\", \"w+\", encoding=\"utf-8\") as ncfp:\n",
    "    for line in original_corpus:\n",
    "        pieces = list(\n",
    "            filter(\n",
    "                lambda x: x != \"▁\",\n",
    "                sp_model.EncodeAsPieces(line)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "        \n",
    "        ncfp.write(\" \".join(units) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = fasttext.train_unsupervised(\n",
    "    \"data/original_60k_250x1/level1/corpus_ft.txt\",\n",
    "    \"cbow\",\n",
    "    dim=250,\n",
    "    thread=4,\n",
    "    epoch=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.save_model(\"models/fasttext_cbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_model = fasttext.load_model(\"models/fasttext_cbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from levelwise_model.test_bench import LSTestBench\n",
    "from levelwise_model.utterances import WordToUtteranceMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_mapping = WordToUtteranceMapping(map_file=\"data/level_wise/level0/utterances_original.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bench = LSTestBench(scores_file=\"data/level_wise/level0/pairs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bench.ft_score_and_save(ft_model=ft_model, utterances=utterance_mapping, results_file=\"results/ft_cbow\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('C:\\\\Users\\\\mj115gl\\\\work_dir\\\\thesis\\\\audio-semantics\\\\models\\\\db_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.Load(\"models/comp_60k_250x1/level1/unigram_vs60000_lw.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"PlQQQQoobbbbbIkkQorrrAAAfflllQQooVVVpppjjUQzzzzOOOOOOOOOOO\"\n",
    "\n",
    "pieces = list(\n",
    "    filter(\n",
    "        lambda x: x != \"▁\",\n",
    "        sp_model.EncodeAsPieces(input_text)\n",
    "    )\n",
    ")\n",
    "\n",
    "units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "print(\" \".join(units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(embeddings, dense_output=True)\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(similarities[np.where(np.tril(similarities, -1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from levelwise_model.utterances import WordToUtteranceMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = WordToUtteranceMapping(\"data/level_wise/level0/utterances.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_utterances = {}\n",
    "\n",
    "for word in utterances.utterances:\n",
    "    updated_utterances[word] = []\n",
    "    for utt in utterances.utterances[word]:\n",
    "        pieces = list(\n",
    "            filter(\n",
    "                lambda x: x != \"▁\",\n",
    "                sp_model.EncodeAsPieces(utt)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "        updated_utterances[word].append(\n",
    "            \" \".join(units)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encode(\"test\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test positive\n",
    "ls_sims_pos = []\n",
    "sy_sims_pos = []\n",
    "tested = 0\n",
    "\n",
    "for word in updated_utterances:\n",
    "    if len(updated_utterances[word]) > 1:\n",
    "        tested += 1\n",
    "        utterances_list = [\n",
    "            \" \".join(\n",
    "                [\n",
    "                    piece.replace(\"▁\", \"\")\n",
    "                    for piece in list(\n",
    "                        filter(\n",
    "                            lambda x: x != \"▁\",\n",
    "                            sp_model.EncodeAsPieces(utterance)\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            ) for utterance in updated_utterances[word]\n",
    "        ]\n",
    "\n",
    "        embeddings = model.encode(utterances_list)\n",
    "        similarities = cosine_similarity(embeddings)\n",
    "\n",
    "        if word.startswith(\"ls_\"):\n",
    "            ls_sims_pos.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "        if word.startswith(\"sy_\"):\n",
    "            sy_sims_pos.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "\n",
    "print(f\"Avg positive score (LS) = {sum(ls_sims_pos)/len(ls_sims_pos)} ({len(ls_sims_pos)} words).\")\n",
    "print(f\"Avg positive score (SY) = {sum(sy_sims_pos)/len(sy_sims_pos)} ({len(sy_sims_pos)} words).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_utts = []\n",
    "\n",
    "for word in updated_utterances:\n",
    "    n_utts.append(len(updated_utterances[word]))\n",
    "\n",
    "print(sum(n_utts)/len(n_utts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test negative\n",
    "ls_sims_neg = []\n",
    "sy_sims_neg = []\n",
    "tested = 0\n",
    "n_negative_samples = 5\n",
    "\n",
    "ls_word_list = set([word for word in updated_utterances.keys() if word.startswith(\"ls_\")])\n",
    "sy_word_list = set([word for word in updated_utterances.keys() if word.startswith(\"sy_\")])\n",
    "\n",
    "for word in updated_utterances:\n",
    "    tested += 1\n",
    "\n",
    "    negative_samples = []\n",
    "    if word.startswith(\"ls_\"):\n",
    "        use_list = ls_word_list\n",
    "    else:\n",
    "        use_list = sy_word_list\n",
    "    sample = np.random.choice(list(use_list - {word}), size=(n_negative_samples))\n",
    "    for s in sample:\n",
    "        negative_samples.append(\n",
    "            np.random.choice(updated_utterances[s])\n",
    "        )\n",
    "\n",
    "    utterances_list = [\n",
    "        \" \".join(\n",
    "            [\n",
    "                piece.replace(\"▁\", \"\")\n",
    "                for piece in list(\n",
    "                    filter(\n",
    "                        lambda x: x != \"▁\",\n",
    "                        sp_model.EncodeAsPieces(utterance)\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        ) for utterance in negative_samples\n",
    "    ]\n",
    "    \n",
    "    embeddings = model.encode(utterances_list)\n",
    "    similarities = cosine_similarity(embeddings)\n",
    "\n",
    "    if word.startswith(\"ls_\"):\n",
    "        ls_sims_neg.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "    if word.startswith(\"sy_\"):\n",
    "        sy_sims_neg.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "\n",
    "print(f\"Avg negative score (LS) = {sum(ls_sims_neg)/len(ls_sims_neg)} ({len(ls_sims_neg)} words).\")\n",
    "print(f\"Avg negative score (SY) = {sum(sy_sims_neg)/len(sy_sims_neg)} ({len(sy_sims_neg)} words).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sy_sims)/len(sy_sims)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABX Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vec_fn(utt):\n",
    "    pieces = list(\n",
    "        filter(\n",
    "            lambda x: x != \"▁\",\n",
    "            sp_model.EncodeAsPieces(utt)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "    return model.encode(\" \".join(units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = 0\n",
    "total = 0\n",
    "use_noise_for_x = True\n",
    "\n",
    "def get_correct_word(pair, word):\n",
    "    if pair[0] == word:\n",
    "        return pair[1]\n",
    "    return pair[1]\n",
    "\n",
    "for word_a in updated_utterances:\n",
    "    # Sort words by similarity\n",
    "    similar_words = sorted(\n",
    "        # Get only pairs containing word A\n",
    "        filter(\n",
    "            lambda x: x[0] == word_a or x[1] == word_a,\n",
    "            rel_pairs\n",
    "        ),\n",
    "        key=lambda x: x[2],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    similar_words = list(\n",
    "        map(\n",
    "            lambda x: get_correct_word(x, word_a),\n",
    "            similar_words\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for utt_a in updated_utterances[word_a]:\n",
    "        word_b = similar_words[0]\n",
    "        utt_b = np.random.choice(\n",
    "            updated_utterances[word_b]\n",
    "        )\n",
    "\n",
    "        if not use_noise_for_x:\n",
    "            word_x = similar_words[-1]\n",
    "            utt_x = np.random.choice(\n",
    "                updated_utterances[word_x]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            utt_x = np.random.choice(\n",
    "                list(ascii_letters),\n",
    "                len(utt_a),\n",
    "                replace=True\n",
    "            )\n",
    "            utt_x = \"\".join(utt_x)\n",
    "\n",
    "        v_a = word_vec_fn(utt_a).reshape(1, -1)\n",
    "        v_b = word_vec_fn(utt_b).reshape(1, -1)\n",
    "        v_x = word_vec_fn(utt_x).reshape(1, -1)\n",
    "\n",
    "        if cosine_similarity(v_a, v_b) > cosine_similarity(v_a, v_x):\n",
    "            preds += 1\n",
    "        total += 1\n",
    "\n",
    "print({\"ABX Result\": preds/total})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ABX Result': 0.5062111801242236}\n"
     ]
    }
   ],
   "source": [
    "preds = 0\n",
    "total = 0\n",
    "use_noise_for_x = False\n",
    "\n",
    "def get_correct_word(pair, word):\n",
    "    if pair[0] == word:\n",
    "        return pair[1]\n",
    "    return pair[1]\n",
    "\n",
    "with open(\"results/abx_test_db.txt\", \"w+\") as abx_results_file:\n",
    "    abx_results_file.write(\"A,B,X,sim(AB),sim(AX),chosen\\n\")\n",
    "    for word_a in updated_utterances:\n",
    "        # Sort words by similarity\n",
    "        similar_words = sorted(\n",
    "            # Get only pairs containing word A\n",
    "            filter(\n",
    "                lambda x: x[0] == word_a or x[1] == word_a,\n",
    "                rel_pairs\n",
    "            ),\n",
    "            key=lambda x: x[2],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        similar_words = list(\n",
    "            map(\n",
    "                lambda x: get_correct_word(x, word_a),\n",
    "                similar_words\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for utt_a in updated_utterances[word_a]:\n",
    "            word_b = similar_words[0]\n",
    "            utt_b = np.random.choice(\n",
    "                updated_utterances[word_b]\n",
    "            )\n",
    "\n",
    "            if not use_noise_for_x:\n",
    "                word_x = similar_words[-1]\n",
    "                utt_x = np.random.choice(\n",
    "                    updated_utterances[word_x]\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                word_x = \"noise\"\n",
    "                utt_x = np.random.choice(\n",
    "                    list(ascii_letters),\n",
    "                    len(utt_a),\n",
    "                    replace=True\n",
    "                )\n",
    "                utt_x = \"\".join(utt_x)\n",
    "\n",
    "            if len(set([word_a, word_b, word_x])) == 3:\n",
    "                v_a = word_vec_fn(utt_a).reshape(1, -1)\n",
    "                v_b = word_vec_fn(utt_b).reshape(1, -1)\n",
    "                v_x = word_vec_fn(utt_x).reshape(1, -1)\n",
    "\n",
    "                sim_ab = cosine_similarity(v_a, v_b)\n",
    "                sim_ax = cosine_similarity(v_a, v_x)\n",
    "\n",
    "                chosen = \"X\"\n",
    "                if sim_ab > sim_ax:\n",
    "                    chosen = \"B\"\n",
    "                    preds += 1\n",
    "                total += 1\n",
    "\n",
    "                abx_results_file.write(\",\".join([word_a, word_b, word_x, str(sim_ab), str(sim_ax), chosen]) + \"\\n\")\n",
    "\n",
    "print({\"ABX Result\": preds/total})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([word_a, word_b, word_x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_pairs = []\n",
    "rel_pairs = []\n",
    "\n",
    "with open(\"data/level_wise/level0/pairs.txt\", \"r\") as pairs_file:\n",
    "    for line in pairs_file.readlines()[1:]:\n",
    "        w1, w2, sim, rel = line.strip().split(\",\")\n",
    "        if sim:\n",
    "            sim_pairs.append((w1, w2, float(sim)))\n",
    "        if rel:\n",
    "            rel_pairs.append((w1, w2, float(rel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_vectors(word):\n",
    "    return model.encode(updated_utterances[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    test_set: {\n",
    "        method: [] for method in [\"min\", \"max\", \"avg\", \"all\"]\n",
    "    } for test_set in [\"librispeech\", \"synthetic\"]\n",
    "}\n",
    "gold_standard = {\n",
    "    \"librispeech\": [],\n",
    "    \"synthetic\": []\n",
    "}\n",
    "trials = 0\n",
    "errors = 0\n",
    "\n",
    "for pair in rel_pairs:\n",
    "    try:\n",
    "        w1, w2, rel = pair\n",
    "\n",
    "        test_set = \"librispeech\" \\\n",
    "            if w1.startswith(\"ls_\") \\\n",
    "            else \"synthetic\"\n",
    "        w1.replace(\"ls_\", \"\").replace(\"sy_\", \"\")\n",
    "        w2.replace(\"ls_\", \"\").replace(\"sy_\", \"\")\n",
    "\n",
    "        w1_vectors = get_model_vectors(\n",
    "            w1\n",
    "        )\n",
    "        w2_vectors = get_model_vectors(\n",
    "            w2\n",
    "        )\n",
    "\n",
    "        similarities = [\n",
    "            cosine_similarity(i.reshape(1, -1), j.reshape(1, -1))\n",
    "            for i in w1_vectors\n",
    "            for j in w2_vectors\n",
    "        ]\n",
    "\n",
    "        scores[test_set][\"min\"].append(np.min(similarities))\n",
    "        scores[test_set][\"avg\"].append(np.mean(similarities))\n",
    "        scores[test_set][\"max\"].append(np.max(similarities))\n",
    "\n",
    "        gold_standard[test_set].append(rel)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        errors += 1\n",
    "    trials += 1\n",
    "\n",
    "print({\n",
    "    'score': {\n",
    "        test_set: {\n",
    "            var: pearsonr(\n",
    "                scores[test_set][var],\n",
    "                gold_standard[test_set]\n",
    "            )[0] * 100\n",
    "            for var in ['min', 'avg', 'max']\n",
    "        }\n",
    "        for test_set in ['librispeech', 'synthetic']\n",
    "    },\n",
    "    'errors': errors,\n",
    "    'trials': trials\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(\"models/original_60k_250x1/level1/w2v_vs250_w1_lw.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test positive\n",
    "ls_sims = []\n",
    "sy_sims = []\n",
    "tested = 0\n",
    "\n",
    "for word in updated_utterances:\n",
    "    if len(updated_utterances[word]) > 1:\n",
    "        tested += 1\n",
    "        embeddings = utterances.get_vectors_from_word(\n",
    "            word, sp_model, w2v_model\n",
    "        )[:, 0, :]\n",
    "\n",
    "        similarities = cosine_similarity(embeddings)\n",
    "\n",
    "        if word.startswith(\"ls_\"):\n",
    "            ls_sims.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "        if word.startswith(\"sy_\"):\n",
    "            sy_sims.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "\n",
    "print(f\"Avg positive score (LS) = {sum(ls_sims)/len(ls_sims)} ({len(ls_sims)} words).\")\n",
    "print(f\"Avg positive score (SY) = {sum(sy_sims)/len(sy_sims)} ({len(sy_sims)} words).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_from_utterance(\n",
    "            utterance,\n",
    "            sp_model: spm.SentencePieceProcessor,\n",
    "            w2v_model: Word2Vec\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Gets the embeddings of the given utterance.\n",
    "        \"\"\"\n",
    "        if utterance in w2v_model.wv.key_to_index.keys():\n",
    "            return w2v_model.wv[utterance].reshape(1, -1)\n",
    "        else:\n",
    "            pieces = list(\n",
    "                filter(\n",
    "                    lambda x: x != \"▁\",\n",
    "                    sp_model.EncodeAsPieces(utterance)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "\n",
    "            vectors = np.array([w2v_model.wv[unit] for unit in units])\n",
    "            return vectors.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test negative\n",
    "ls_sims = []\n",
    "sy_sims = []\n",
    "tested = 0\n",
    "n_negative_samples = 5\n",
    "\n",
    "ls_word_list = set([word for word in updated_utterances.keys() if word.startswith(\"ls_\")])\n",
    "sy_word_list = set([word for word in updated_utterances.keys() if word.startswith(\"sy_\")])\n",
    "\n",
    "for word in updated_utterances:\n",
    "    tested += 1\n",
    "\n",
    "    negative_samples = []\n",
    "    if word.startswith(\"ls_\"):\n",
    "        use_list = ls_word_list\n",
    "    else:\n",
    "        use_list = sy_word_list\n",
    "    sample = np.random.choice(list(use_list - {word}), size=(n_negative_samples))\n",
    "    for s in sample:\n",
    "        negative_samples.append(\n",
    "            np.random.choice(updated_utterances[s])\n",
    "        )\n",
    "\n",
    "    embeddings = np.array([\n",
    "        get_vector_from_utterance(utterance, sp_model, w2v_model) for utterance in negative_samples\n",
    "    ])\n",
    "    \n",
    "    similarities = cosine_similarity(embeddings)\n",
    "\n",
    "    if word.startswith(\"ls_\"):\n",
    "        ls_sims.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "    if word.startswith(\"sy_\"):\n",
    "        sy_sims.append(np.mean(similarities[np.where(np.tril(similarities, -1))]))\n",
    "\n",
    "print(f\"Avg negative score (LS) = {sum(ls_sims)/len(ls_sims)} ({len(ls_sims)} words).\")\n",
    "print(f\"Avg negative score (SY) = {sum(sy_sims)/len(sy_sims)} ({len(sy_sims)} words).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_model.Load(\"models/original_60k_250x1/level1/unigram_vs60000_lw.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vec_fn(utterance):\n",
    "    if utterance in w2v_model.wv.key_to_index.keys():\n",
    "        return w2v_model.wv[utterance].reshape(1, -1)\n",
    "    else:\n",
    "        pieces = list(\n",
    "            filter(\n",
    "                lambda x: x != \"▁\",\n",
    "                sp_model.EncodeAsPieces(utterance)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        units = [piece.replace(\"▁\", \"\") for piece in pieces]\n",
    "\n",
    "        vectors = np.array([w2v_model.wv[unit] for unit in units])\n",
    "        return vectors.mean(axis=0).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ABX Result': 0.4767080745341615}\n"
     ]
    }
   ],
   "source": [
    "preds = 0\n",
    "total = 0\n",
    "use_noise_for_x = False\n",
    "\n",
    "def get_correct_word(pair, word):\n",
    "    if pair[0] == word:\n",
    "        return pair[1]\n",
    "    return pair[1]\n",
    "\n",
    "with open(\"results/abx_test.txt\", \"w+\") as abx_results_file:\n",
    "    abx_results_file.write(\"A,B,X,sim(AB),sim(AX),chosen\\n\")\n",
    "    for word_a in updated_utterances:\n",
    "        # Sort words by similarity\n",
    "        similar_words = sorted(\n",
    "            # Get only pairs containing word A\n",
    "            filter(\n",
    "                lambda x: x[0] == word_a or x[1] == word_a,\n",
    "                rel_pairs\n",
    "            ),\n",
    "            key=lambda x: x[2],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        similar_words = list(\n",
    "            map(\n",
    "                lambda x: get_correct_word(x, word_a),\n",
    "                similar_words\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for utt_a in updated_utterances[word_a]:\n",
    "            word_b = similar_words[0]\n",
    "            utt_b = np.random.choice(\n",
    "                updated_utterances[word_b]\n",
    "            )\n",
    "\n",
    "            if not use_noise_for_x:\n",
    "                word_x = similar_words[-1]\n",
    "                utt_x = np.random.choice(\n",
    "                    updated_utterances[word_x]\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                word_x = \"noise\"\n",
    "                utt_x = np.random.choice(\n",
    "                    list(ascii_letters),\n",
    "                    len(utt_a),\n",
    "                    replace=True\n",
    "                )\n",
    "                utt_x = \"\".join(utt_x)\n",
    "\n",
    "            if len(set([word_a, word_b, word_x])) == 3:\n",
    "                v_a = word_vec_fn(utt_a).reshape(1, -1)\n",
    "                v_b = word_vec_fn(utt_b).reshape(1, -1)\n",
    "                v_x = word_vec_fn(utt_x).reshape(1, -1)\n",
    "\n",
    "                sim_ab = cosine_similarity(v_a, v_b)\n",
    "                sim_ax = cosine_similarity(v_a, v_x)\n",
    "\n",
    "                chosen = \"X\"\n",
    "                if sim_ab > sim_ax:\n",
    "                    chosen = \"B\"\n",
    "                    preds += 1\n",
    "                total += 1\n",
    "\n",
    "                abx_results_file.write(\",\".join([word_a, word_b, word_x, str(sim_ab), str(sim_ax), chosen]) + \"\\n\")\n",
    "\n",
    "print({\"ABX Result\": preds/total})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
